{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-09-16T16:45:36.689035Z",
     "iopub.status.busy": "2021-09-16T16:45:36.688632Z",
     "iopub.status.idle": "2021-09-16T16:45:37.837657Z",
     "shell.execute_reply": "2021-09-16T16:45:37.836879Z",
     "shell.execute_reply.started": "2021-09-16T16:45:36.688934Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import glob\n",
    "import os\n",
    "import gc\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from sklearn import preprocessing, model_selection\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import numpy.matlib\n",
    "\n",
    "\n",
    "path_submissions = '/'\n",
    "\n",
    "target_name = 'target'\n",
    "scores_folds = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.2.4'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-16T16:45:37.839532Z",
     "iopub.status.busy": "2021-09-16T16:45:37.839055Z",
     "iopub.status.idle": "2021-09-16T16:45:37.884933Z",
     "shell.execute_reply": "2021-09-16T16:45:37.883981Z",
     "shell.execute_reply.started": "2021-09-16T16:45:37.839499Z"
    }
   },
   "outputs": [],
   "source": [
    "# data directory\n",
    "data_dir = '../data/'\n",
    "\n",
    "# Function to calculate first WAP\n",
    "def calc_wap1(df):\n",
    "    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) / (df['bid_size1'] + df['ask_size1'])\n",
    "    return wap\n",
    "\n",
    "# Function to calculate second WAP\n",
    "def calc_wap2(df):\n",
    "    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) / (df['bid_size2'] + df['ask_size2'])\n",
    "    return wap\n",
    "\n",
    "def calc_wap3(df):\n",
    "    wap = (df['bid_price1'] * df['bid_size1'] + df['ask_price1'] * df['ask_size1']) / (df['bid_size1'] + df['ask_size1'])\n",
    "    return wap\n",
    "\n",
    "def calc_wap4(df):\n",
    "    wap = (df['bid_price2'] * df['bid_size2'] + df['ask_price2'] * df['ask_size2']) / (df['bid_size2'] + df['ask_size2'])\n",
    "    return wap\n",
    "\n",
    "# Function to calculate the log of the return\n",
    "# Remember that logb(x / y) = logb(x) - logb(y)\n",
    "def log_return(series):\n",
    "    return np.log(series).diff()\n",
    "\n",
    "# Calculate the realized volatility\n",
    "def realized_volatility(series):\n",
    "    return np.sqrt(np.sum(series**2))\n",
    "\n",
    "# Function to count unique elements of a series\n",
    "def count_unique(series):\n",
    "    return len(np.unique(series))\n",
    "\n",
    "# Function to read our base train and test set\n",
    "def read_train_test():\n",
    "#     train = pd.read_csv(data_dir+'train.csv')\n",
    "    test = pd.read_csv(data_dir+'test.csv')\n",
    "    # Create a key to merge with book and trade data\n",
    "#     train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n",
    "    test['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n",
    "    #print(f'Our training set has {train.shape[0]} rows')\n",
    "    return test\n",
    "\n",
    "# Function to preprocess book data (for each stock id)\n",
    "def book_preprocessor(file_path):\n",
    "    df = pd.read_parquet(file_path)\n",
    "    # Calculate Wap\n",
    "    df['wap1'] = calc_wap1(df)\n",
    "    df['wap2'] = calc_wap2(df)\n",
    "    df['wap3'] = calc_wap3(df)\n",
    "    df['wap4'] = calc_wap4(df)\n",
    "    # Calculate log returns\n",
    "    df['log_return1'] = df.groupby(['time_id'])['wap1'].apply(log_return)\n",
    "    df['log_return2'] = df.groupby(['time_id'])['wap2'].apply(log_return)\n",
    "    df['log_return3'] = df.groupby(['time_id'])['wap3'].apply(log_return)\n",
    "    df['log_return4'] = df.groupby(['time_id'])['wap4'].apply(log_return)\n",
    "    # Calculate wap balance\n",
    "    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n",
    "    # Calculate spread\n",
    "    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) / ((df['ask_price1'] + df['bid_price1']) / 2)\n",
    "    df['price_spread2'] = (df['ask_price2'] - df['bid_price2']) / ((df['ask_price2'] + df['bid_price2']) / 2)\n",
    "    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n",
    "    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n",
    "    df[\"bid_ask_spread\"] = abs(df['bid_spread'] - df['ask_spread'])\n",
    "    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n",
    "    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n",
    "    \n",
    "    # Dict for aggregations\n",
    "    create_feature_dict = {\n",
    "        'wap1': [np.sum, np.std ],\n",
    "        'wap2': [np.sum, np.std ],\n",
    "        'wap3': [np.sum, np.std ],\n",
    "        'wap4': [np.sum, np.std ],\n",
    "        'log_return1': [realized_volatility],\n",
    "        'log_return2': [realized_volatility],\n",
    "        'log_return3': [realized_volatility],\n",
    "        'log_return4': [realized_volatility],\n",
    "        'wap_balance': [np.sum, np.max],\n",
    "        'price_spread':[np.sum, np.max],\n",
    "        'price_spread2':[np.sum, np.max],\n",
    "        'bid_spread':[np.sum, np.max],\n",
    "        'ask_spread':[np.sum, np.max],\n",
    "        'total_volume':[np.sum, np.max],\n",
    "        'volume_imbalance':[np.sum, np.max],\n",
    "        \"bid_ask_spread\":[np.sum,  np.max],\n",
    "    }\n",
    "    create_feature_dict_time = {\n",
    "        'log_return1': [realized_volatility],\n",
    "        'log_return2': [realized_volatility],\n",
    "        'log_return3': [realized_volatility],\n",
    "        'log_return4': [realized_volatility],\n",
    "    }\n",
    "    \n",
    "    # Function to get group stats for different windows (seconds in bucket)\n",
    "    def get_stats_window(fe_dict,seconds_in_bucket, add_suffix = False):\n",
    "        # Group by the window\n",
    "        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(fe_dict).reset_index()\n",
    "        # Rename columns joining suffix\n",
    "        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n",
    "        # Add a suffix to differentiate windows\n",
    "        if add_suffix:\n",
    "            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n",
    "        return df_feature\n",
    "    \n",
    "    # Get the stats for different windows\n",
    "    df_feature = get_stats_window(create_feature_dict,seconds_in_bucket = 0, add_suffix = False)\n",
    "    df_feature_500 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 500, add_suffix = True)\n",
    "    df_feature_400 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 400, add_suffix = True)\n",
    "    df_feature_300 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 300, add_suffix = True)\n",
    "    df_feature_200 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 200, add_suffix = True)\n",
    "    df_feature_100 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 100, add_suffix = True)\n",
    "\n",
    "    # Merge all\n",
    "    df_feature = df_feature.merge(df_feature_500, how = 'left', left_on = 'time_id_', right_on = 'time_id__500')\n",
    "    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n",
    "    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n",
    "    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n",
    "    df_feature = df_feature.merge(df_feature_100, how = 'left', left_on = 'time_id_', right_on = 'time_id__100')\n",
    "    # Drop unnecesary time_ids\n",
    "    df_feature.drop(['time_id__500','time_id__400', 'time_id__300', 'time_id__200','time_id__100'], axis = 1, inplace = True)\n",
    "    \n",
    "    \n",
    "    # Create row_id so we can merge\n",
    "    stock_id = file_path.split('=')[1]\n",
    "    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x: f'{stock_id}-{x}')\n",
    "    df_feature.drop(['time_id_'], axis = 1, inplace = True)\n",
    "    return df_feature\n",
    "\n",
    "# Function to preprocess trade data (for each stock id)\n",
    "def trade_preprocessor(file_path):\n",
    "    df = pd.read_parquet(file_path)\n",
    "    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n",
    "    df['amount']=df['price']*df['size']\n",
    "    # Dict for aggregations\n",
    "    create_feature_dict = {\n",
    "        'log_return':[realized_volatility],\n",
    "        'seconds_in_bucket':[count_unique],\n",
    "        'size':[np.sum, np.max, np.min],\n",
    "        'order_count':[np.sum,np.max],\n",
    "        'amount':[np.sum,np.max,np.min],\n",
    "    }\n",
    "    create_feature_dict_time = {\n",
    "        'log_return':[realized_volatility],\n",
    "        'seconds_in_bucket':[count_unique],\n",
    "        'size':[np.sum],\n",
    "        'order_count':[np.sum],\n",
    "    }\n",
    "    # Function to get group stats for different windows (seconds in bucket)\n",
    "    def get_stats_window(fe_dict,seconds_in_bucket, add_suffix = False):\n",
    "        # Group by the window\n",
    "        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(fe_dict).reset_index()\n",
    "        # Rename columns joining suffix\n",
    "        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n",
    "        # Add a suffix to differentiate windows\n",
    "        if add_suffix:\n",
    "            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n",
    "        return df_feature\n",
    "    \n",
    "\n",
    "    # Get the stats for different windows\n",
    "    df_feature = get_stats_window(create_feature_dict,seconds_in_bucket = 0, add_suffix = False)\n",
    "    df_feature_500 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 500, add_suffix = True)\n",
    "    df_feature_400 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 400, add_suffix = True)\n",
    "    df_feature_300 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 300, add_suffix = True)\n",
    "    df_feature_200 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 200, add_suffix = True)\n",
    "    df_feature_100 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 100, add_suffix = True)\n",
    "    \n",
    "    def tendency(price, vol):    \n",
    "        df_diff = np.diff(price)\n",
    "        val = (df_diff/price[1:])*100\n",
    "        power = np.sum(val*vol[1:])\n",
    "        return(power)\n",
    "    \n",
    "    lis = []\n",
    "    for n_time_id in df['time_id'].unique():\n",
    "        df_id = df[df['time_id'] == n_time_id]        \n",
    "        tendencyV = tendency(df_id['price'].values, df_id['size'].values)      \n",
    "        f_max = np.sum(df_id['price'].values > np.mean(df_id['price'].values))\n",
    "        f_min = np.sum(df_id['price'].values < np.mean(df_id['price'].values))\n",
    "        df_max =  np.sum(np.diff(df_id['price'].values) > 0)\n",
    "        df_min =  np.sum(np.diff(df_id['price'].values) < 0)\n",
    "        # new\n",
    "        abs_diff = np.median(np.abs( df_id['price'].values - np.mean(df_id['price'].values)))        \n",
    "        energy = np.mean(df_id['price'].values**2)\n",
    "        iqr_p = np.percentile(df_id['price'].values,75) - np.percentile(df_id['price'].values,25)\n",
    "#         iqr_p2 = np.percentile(df_id['price'].values,95) - np.percentile(df_id['price'].values,5)\n",
    "        # vol vars\n",
    "        \n",
    "        abs_diff_v = np.median(np.abs( df_id['size'].values - np.mean(df_id['size'].values)))        \n",
    "        energy_v = np.sum(df_id['size'].values**2)\n",
    "        iqr_p_v = np.percentile(df_id['size'].values,75) - np.percentile(df_id['size'].values,25)\n",
    "#         iqr_p2_v = np.percentile(df_id['size'].values,95) - np.percentile(df_id['size'].values,5)\n",
    "        \n",
    "        lis.append({'time_id':n_time_id,'tendency':tendencyV,'f_max':f_max,'f_min':f_min,'df_max':df_max,'df_min':df_min,\n",
    "                   'abs_diff':abs_diff,'energy':energy,'iqr_p':iqr_p,'abs_diff_v':abs_diff_v,'energy_v':energy_v,'iqr_p_v':iqr_p_v})\n",
    "    \n",
    "    df_lr = pd.DataFrame(lis)\n",
    "        \n",
    "   \n",
    "    df_feature = df_feature.merge(df_lr, how = 'left', left_on = 'time_id_', right_on = 'time_id')\n",
    "    \n",
    "    # Merge all\n",
    "    df_feature = df_feature.merge(df_feature_500, how = 'left', left_on = 'time_id_', right_on = 'time_id__500')\n",
    "    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n",
    "    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n",
    "    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n",
    "    df_feature = df_feature.merge(df_feature_100, how = 'left', left_on = 'time_id_', right_on = 'time_id__100')\n",
    "    # Drop unnecesary time_ids\n",
    "    df_feature.drop(['time_id__500','time_id__400', 'time_id__300', 'time_id__200','time_id','time_id__100'], axis = 1, inplace = True)\n",
    "    \n",
    "    \n",
    "    df_feature = df_feature.add_prefix('trade_')\n",
    "    stock_id = file_path.split('=')[1]\n",
    "    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n",
    "    df_feature.drop(['trade_time_id_'], axis = 1, inplace = True)\n",
    "    return df_feature\n",
    "\n",
    "# Function to get group stats for the stock_id and time_id\n",
    "def get_time_stock(df):\n",
    "    vol_cols = ['log_return1_realized_volatility', 'log_return2_realized_volatility', 'log_return1_realized_volatility_400', 'log_return2_realized_volatility_400', \n",
    "                'log_return1_realized_volatility_300', 'log_return2_realized_volatility_300', 'log_return1_realized_volatility_200', 'log_return2_realized_volatility_200', \n",
    "                'trade_log_return_realized_volatility', 'trade_log_return_realized_volatility_400', 'trade_log_return_realized_volatility_300', 'trade_log_return_realized_volatility_200']\n",
    "\n",
    "\n",
    "    # Group by the stock id\n",
    "    df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n",
    "    # Rename columns joining suffix\n",
    "    df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n",
    "    df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n",
    "\n",
    "    # Group by the stock id\n",
    "    df_time_id = df.groupby(['time_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n",
    "    # Rename columns joining suffix\n",
    "    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n",
    "    df_time_id = df_time_id.add_suffix('_' + 'time')\n",
    "    \n",
    "    # Merge with original dataframe\n",
    "    df = df.merge(df_stock_id, how = 'left', left_on = ['stock_id'], right_on = ['stock_id__stock'])\n",
    "    df = df.merge(df_time_id, how = 'left', left_on = ['time_id'], right_on = ['time_id__time'])\n",
    "    df.drop(['stock_id__stock', 'time_id__time'], axis = 1, inplace = True)\n",
    "    return df\n",
    "    \n",
    "# Funtion to make preprocessing function in parallel (for each stock id)\n",
    "def preprocessor(list_stock_ids, is_train = True):\n",
    "    \n",
    "    # Parrallel for loop\n",
    "    def for_joblib(stock_id):\n",
    "        # Train\n",
    "        if is_train:\n",
    "            file_path_book = data_dir + \"book_train.parquet/stock_id=\" + str(stock_id)\n",
    "            file_path_trade = data_dir + \"trade_train.parquet/stock_id=\" + str(stock_id)\n",
    "        # Test\n",
    "        else:\n",
    "            file_path_book = data_dir + \"book_test.parquet/stock_id=\" + str(stock_id)\n",
    "            file_path_trade = data_dir + \"trade_test.parquet/stock_id=\" + str(stock_id)\n",
    "    \n",
    "        # Preprocess book and trade data and merge them\n",
    "        df_tmp = pd.merge(book_preprocessor(file_path_book), trade_preprocessor(file_path_trade), on = 'row_id', how = 'left')\n",
    "        \n",
    "        # Return the merge dataframe\n",
    "        return df_tmp\n",
    "    \n",
    "    # Use parallel api to call paralle for loop\n",
    "    df = Parallel(n_jobs = -1, verbose = 1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\n",
    "    # Concatenate all the dataframes that return from Parallel\n",
    "    df = pd.concat(df, ignore_index = True)\n",
    "    return df\n",
    "\n",
    "# Function to calculate the root mean squared percentage error\n",
    "def rmspe(y_true, y_pred):\n",
    "    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
    "\n",
    "# Function to early stop with root mean squared percentage error\n",
    "def feval_rmspe(y_pred, lgb_train):\n",
    "    y_true = lgb_train.get_label()\n",
    "    return 'RMSPE', rmspe(y_true, y_pred), False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-16T16:45:37.887083Z",
     "iopub.status.busy": "2021-09-16T16:45:37.886696Z",
     "iopub.status.idle": "2021-09-16T16:45:43.686363Z",
     "shell.execute_reply": "2021-09-16T16:45:43.685332Z",
     "shell.execute_reply.started": "2021-09-16T16:45:37.887040Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    2.2s finished\n"
     ]
    }
   ],
   "source": [
    "# Read train and test\n",
    "train =pd.read_pickle(\"train_cu.pkl\")\n",
    "test = read_train_test()\n",
    "\n",
    "# Get unique stock ids \n",
    "# train_stock_ids = train['stock_id'].unique()\n",
    "# Preprocess them using Parallel and our single stock id functions\n",
    "# train_ = preprocessor(train_stock_ids, is_train = True)\n",
    "# train = train.merge(train_, on = ['row_id'], how = 'left')\n",
    "\n",
    "# Get unique stock ids \n",
    "test_stock_ids = test['stock_id'].unique()\n",
    "# Preprocess them using Parallel and our single stock id functions\n",
    "test_ = preprocessor(test_stock_ids, is_train = False)\n",
    "test = test.merge(test_, on = ['row_id'], how = 'left')\n",
    "\n",
    "# Get group stats of time_id and stock_id\n",
    "# train = get_time_stock(train)\n",
    "test = get_time_stock(test)\n",
    "\n",
    "train1=train\n",
    "test1=test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.loc[train.time_id == 11,'log_return1_realized_volatility']\n",
    "def preprocessor2(list_stock_ids, is_train = True):\n",
    "    \n",
    "    # Parrallel for loop\n",
    "    def for_joblib(stock_id):\n",
    "        # Train\n",
    "        if is_train:\n",
    "            file_path_book = data_dir + \"book_train.parquet/stock_id=\" + str(stock_id)\n",
    "            file_path_trade = data_dir + \"trade_train.parquet/stock_id=\" + str(stock_id)\n",
    "        # Test\n",
    "        else:\n",
    "            file_path_book = data_dir + \"book_test.parquet/stock_id=\" + str(stock_id)\n",
    "            file_path_trade = data_dir + \"trade_test.parquet/stock_id=\" + str(stock_id)\n",
    "    \n",
    "        # Preprocess book and trade data and merge them\n",
    "        df_tmp = pd.merge(book_preprocessor2(file_path_book,stock_id), trade_preprocessor2(file_path_trade,stock_id), on = 'row_id', how = 'left')\n",
    "        \n",
    "        # Return the merge dataframe\n",
    "        return df_tmp\n",
    "    \n",
    "    # Use parallel api to call paralle for loop\n",
    "    df = Parallel(n_jobs = -1, verbose = 1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\n",
    "#     df = [for_joblib(stock_id) for stock_id in list_stock_ids]\n",
    "    # Concatenate all the dataframes that return from Parallel\n",
    "    df = pd.concat(df, ignore_index = True)\n",
    "    return df\n",
    "def trade_preprocessor2(file_path,stock_id):\n",
    "    df = pd.read_parquet(file_path)\n",
    "    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n",
    "    lis = []\n",
    "    for n_time_id in df['time_id'].unique():\n",
    "        df_id = df[df['time_id'] == n_time_id]        \n",
    "        iqr_p2 = np.percentile(df_id['price'].values,95) - np.percentile(df_id['price'].values,5)\n",
    "        mean_return = np.nanmean(df_id['log_return'])\n",
    "\n",
    "        lis.append({'time_id':n_time_id,'iqr_p2':iqr_p2,'mean_return': mean_return})\n",
    "    df_lr = pd.DataFrame(lis)\n",
    "    df_lr['row_id'] = df_lr['time_id'].apply(lambda x: f'{stock_id}-{x}')\n",
    "    \n",
    "#     df_feature = df.merge(df_lr, how = 'left', left_on = 'time_id', right_on = 'time_id')\n",
    "\n",
    "    return df_lr\n",
    "def book_preprocessor2(file_path,stock_id):\n",
    "    df = pd.read_parquet(file_path)\n",
    "    # Calculate Wap\n",
    "    df['wap1'] = calc_wap1(df)\n",
    "    df['wap2'] = calc_wap2(df)\n",
    "    df['wap3'] = calc_wap3(df)\n",
    "    df['wap4'] = calc_wap4(df)\n",
    "    # Calculate log returns\n",
    "    df['log_return1'] = df.groupby(['time_id'])['wap1'].apply(log_return).fillna(0)\n",
    "    df['log_return2'] = df.groupby(['time_id'])['wap2'].apply(log_return).fillna(0)\n",
    "    df['log_return3'] = df.groupby(['time_id'])['wap3'].apply(log_return).fillna(0)\n",
    "    df['log_return4'] = df.groupby(['time_id'])['wap4'].apply(log_return).fillna(0)\n",
    "    #\n",
    "    lis = []\n",
    "    for n_time_id in df['time_id'].unique():\n",
    "        df_id = df[df['time_id'] == n_time_id]        \n",
    "        iqr_p21 = np.percentile(df_id['log_return1'].values,95) - np.percentile(df_id['log_return1'].values,5)\n",
    "        mean_return1 = np.nanmean(df_id['log_return1'])\n",
    "        iqr_p22 = np.percentile(df_id['log_return2'].values,95) - np.percentile(df_id['log_return2'].values,5)\n",
    "        mean_return2 = np.nanmean(df_id['log_return2'])\n",
    "        iqr_p23 = np.percentile(df_id['log_return3'].values,95) - np.percentile(df_id['log_return3'].values,5)\n",
    "        mean_return3 = np.nanmean(df_id['log_return3'])\n",
    "        iqr_p24 = np.percentile(df_id['log_return4'].values,95) - np.percentile(df_id['log_return4'].values,5)\n",
    "        mean_return4 = np.nanmean(df_id['log_return4'])\n",
    "\n",
    "        lis.append({'time_id':n_time_id,'iqr_p21':iqr_p21,'iqr_p22':iqr_p22,'iqr_p23':iqr_p23,'iqr_p24':iqr_p24,\n",
    "                    'mean_return1': mean_return1,'mean_return2': mean_return2,'mean_return3': mean_return3,\n",
    "                    'mean_return4': mean_return4})\n",
    "    df_lr = pd.DataFrame(lis)\n",
    "    df_lr['row_id'] = df_lr['time_id'].apply(lambda x: f'{stock_id}-{x}')\n",
    "\n",
    "    return df_lr\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.4s finished\n"
     ]
    }
   ],
   "source": [
    "# stock_id = 0\n",
    "# train_stock_ids = train['stock_id'].unique()\n",
    "# Preprocess them using Parallel and our single stock id functions\n",
    "# temp = preprocessor2(train_stock_ids, is_train = True)\n",
    "# file_path_book_0 = data_dir + \"book_train.parquet/stock_id=\" + str(stock_id)\n",
    "# file_path_trade_0 = data_dir + \"trade_train.parquet/stock_id=\" + str(stock_id)\n",
    "test_stock_ids = test['stock_id'].unique()\n",
    "temp_test = preprocessor2(test_stock_ids, is_train = False)\n",
    "temp2_test = temp_test.merge(test, on= 'row_id')\n",
    "temp2_test.drop(['time_id_y','time_id_x'],axis = 1,inplace = True )\n",
    "# book_preprocessor2(file_path0)\n",
    "# file_path0\n",
    "# temp = pd.merge(book_preprocessor2(file_path_book_0), trade_preprocessor2(file_path_trade_0), on = ['row_id','time_id'], how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amakr\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\seaborn\\distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='target', ylabel='Density'>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.distplot(((train.loc[train.stock_id == 0,'target'].sort_values(ascending = True)).reset_index(drop = True)))\n",
    "# plt.plot(((train.loc[train.stock_id == 1,'target'].sort_values(ascending = True)).reset_index(drop = True)))\n",
    "# plt.plot(((train.loc[train.stock_id == 2,'target'].sort_values(ascending = True)).reset_index(drop = True)))\n",
    "# plt.plot(((train.loc[train.stock_id == 3,'target'].sort_values(ascending = True)).reset_index(drop = True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_sid0 = train.loc[train.stock_id == 0]\n",
    "# train = temp2\n",
    "# test = temp2_test\n",
    "# test.loc[test.stock_id == 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corrr = temp2.corr()\n",
    "# abs(corrr['target']).sort_values(ascending= False)\n",
    "# train.columns.difference(test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (temp2['time_id'] != temp2['time_id_y'] ).sum()\n",
    "# temp2.loc[(temp2['time_id'] != temp2['time_id_y'] )][['time_id_y','time_id','row_id','time_id_x']]\n",
    "# temp2.drop(['time_id_y','time_id_x'],axis = 1,inplace = True )\n",
    "# train_cust['time_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp2.isna().any().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corrr = train_sid0.merge(temp, on = ['row_id']).corr()\n",
    "# corrr['target'].sort_values(ascending= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# from tqdm import tqdm\n",
    "# # sns.distplot(train['wap1_mean'])\n",
    "# # train.loc[train.time_id == 11,'wap1_mean'].mean()\n",
    "# def sharp_ratio(df):\n",
    "#     df1 = df.copy()\n",
    "#     df1['sharpe_ratio1'] = 0\n",
    "#     df1['sharpe_ratio2'] = 0\n",
    "#     df1['sharpe_ratio3'] = 0\n",
    "#     df1['sharpe_ratio4'] = 0\n",
    "#     time_ids = df.time_id.unique().tolist()\n",
    "#     for t_id in tqdm(time_ids):\n",
    "#         m1 = df1.loc[df1.time_id == t_id,'log_return1_mean'].mean()\n",
    "#         m2 = df1.loc[df1.time_id == t_id,'log_return2_mean'].mean()\n",
    "#         m3 = df1.loc[df1.time_id == t_id,'log_return3_mean'].mean()\n",
    "#         m4 = df1.loc[df1.time_id == t_id,'log_return4_mean'].mean()\n",
    "#         df1.loc[df1.time_id == t_id,'sharpe_ratio1'] = (df1.loc[df1.time_id == t_id,'log_return1_mean']-m1)/df1.loc[df1.time_id == t_id,'log_return1_realized_volatility']\n",
    "#         df1.loc[df1.time_id == t_id,'sharpe_ratio2'] = (df1.loc[df1.time_id == t_id,'log_return2_mean']-m2)/df1.loc[df1.time_id == t_id,'log_return2_realized_volatility']\n",
    "#         df1.loc[df1.time_id == t_id,'sharpe_ratio3'] = (df1.loc[df1.time_id == t_id,'log_return3_mean']-m3)/df1.loc[df1.time_id == t_id,'log_return3_realized_volatility']\n",
    "#         df1.loc[df1.time_id == t_id,'sharpe_ratio4'] = (df1.loc[df1.time_id == t_id,'log_return4_mean']-m4)/df1.loc[df1.time_id == t_id,'log_return4_realized_volatility']\n",
    "#     return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_sr = sharp_ratio(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corrr= train_sr.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-16T16:45:47.272003Z",
     "iopub.status.busy": "2021-09-16T16:45:47.271649Z",
     "iopub.status.idle": "2021-09-16T16:45:47.302235Z",
     "shell.execute_reply": "2021-09-16T16:45:47.301225Z",
     "shell.execute_reply.started": "2021-09-16T16:45:47.271967Z"
    }
   },
   "outputs": [],
   "source": [
    "# replace by order sum (tau)\n",
    "train['size_tau'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique'] )\n",
    "test['size_tau'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique'] )\n",
    "#train['size_tau_450'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_450'] )\n",
    "#test['size_tau_450'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_450'] )\n",
    "train['size_tau_400'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_400'] )\n",
    "test['size_tau_400'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_400'] )\n",
    "train['size_tau_300'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_300'] )\n",
    "test['size_tau_300'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_300'] )\n",
    "#train['size_tau_150'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_150'] )\n",
    "#test['size_tau_150'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_150'] )\n",
    "train['size_tau_200'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_200'] )\n",
    "test['size_tau_200'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_200'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-16T16:45:48.100981Z",
     "iopub.status.busy": "2021-09-16T16:45:48.100634Z",
     "iopub.status.idle": "2021-09-16T16:45:48.133332Z",
     "shell.execute_reply": "2021-09-16T16:45:48.132589Z",
     "shell.execute_reply.started": "2021-09-16T16:45:48.100944Z"
    }
   },
   "outputs": [],
   "source": [
    "train['size_tau2'] = np.sqrt( 1/ train['trade_order_count_sum'] )\n",
    "test['size_tau2'] = np.sqrt( 1/ test['trade_order_count_sum'] )\n",
    "#train['size_tau2_450'] = np.sqrt( 0.25/ train['trade_order_count_sum'] )\n",
    "#test['size_tau2_450'] = np.sqrt( 0.25/ test['trade_order_count_sum'] )\n",
    "train['size_tau2_400'] = np.sqrt( 0.33/ train['trade_order_count_sum'] )\n",
    "test['size_tau2_400'] = np.sqrt( 0.33/ test['trade_order_count_sum'] )\n",
    "train['size_tau2_300'] = np.sqrt( 0.5/ train['trade_order_count_sum'] )\n",
    "test['size_tau2_300'] = np.sqrt( 0.5/ test['trade_order_count_sum'] )\n",
    "#train['size_tau2_150'] = np.sqrt( 0.75/ train['trade_order_count_sum'] )\n",
    "#test['size_tau2_150'] = np.sqrt( 0.75/ test['trade_order_count_sum'] )\n",
    "train['size_tau2_200'] = np.sqrt( 0.66/ train['trade_order_count_sum'] )\n",
    "test['size_tau2_200'] = np.sqrt( 0.66/ test['trade_order_count_sum'] )\n",
    "\n",
    "# delta tau\n",
    "train['size_tau2_d'] = train['size_tau2_400'] - train['size_tau2']\n",
    "test['size_tau2_d'] = test['size_tau2_400'] - test['size_tau2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-16T16:45:49.957657Z",
     "iopub.status.busy": "2021-09-16T16:45:49.957186Z",
     "iopub.status.idle": "2021-09-16T16:45:49.967318Z",
     "shell.execute_reply": "2021-09-16T16:45:49.966423Z",
     "shell.execute_reply.started": "2021-09-16T16:45:49.957610Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "204"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colNames = [col for col in list(train.columns)\n",
    "            if col not in {\"stock_id\", \"time_id\", \"target\", \"row_id\"}]\n",
    "len(colNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-16T16:45:52.409102Z",
     "iopub.status.busy": "2021-09-16T16:45:52.408759Z",
     "iopub.status.idle": "2021-09-16T16:45:54.794253Z",
     "shell.execute_reply": "2021-09-16T16:45:54.793175Z",
     "shell.execute_reply.started": "2021-09-16T16:45:52.409066Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 4 2 1 1 2 4 6 2 1 0 4 4 1 1 1 2 4 4 4 0 1 1 3 1 1 4 3 4 3 4 4 1 3 3 4\n",
      " 3 4 1 4 1 4 4 1 0 4 4 1 0 0 3 3 3 2 0 2 4 1 4 4 1 4 1 0 3 3 0 3 0 6 5 3 3\n",
      " 0 1 2 0 3 3 3 4 1 1 0 2 3 3 1 0 1 4 4 4 4 4 1 3 1 0 1 4 1 0 1 4 1 0 4 0 4\n",
      " 0]\n",
      "[1, 11, 22, 50, 55, 56, 62, 73, 76, 78, 84, 87, 96, 101, 112, 116, 122, 124, 126]\n",
      "[0, 4, 5, 10, 15, 16, 17, 23, 26, 28, 29, 36, 42, 44, 48, 53, 66, 69, 72, 85, 94, 95, 100, 102, 109, 111, 113, 115, 118, 120]\n",
      "[3, 6, 9, 18, 61, 63, 86, 97]\n",
      "[27, 31, 33, 37, 38, 40, 58, 59, 60, 74, 75, 77, 82, 83, 88, 89, 90, 98, 99, 110]\n",
      "[2, 7, 13, 14, 19, 20, 21, 30, 32, 34, 35, 39, 41, 43, 46, 47, 51, 52, 64, 67, 68, 70, 93, 103, 104, 105, 107, 108, 114, 119, 123, 125]\n",
      "[81]\n",
      "[8, 80]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "# making agg features\n",
    "\n",
    "train_p = pd.read_csv(data_dir+'train.csv')\n",
    "train_p = train_p.pivot(index='time_id', columns='stock_id', values='target')\n",
    "\n",
    "corr = train_p.corr()\n",
    "\n",
    "ids = corr.index\n",
    "\n",
    "kmeans = KMeans(n_clusters=7, random_state=0).fit(corr.values)\n",
    "print(kmeans.labels_)\n",
    "\n",
    "l = []\n",
    "for n in range(7):\n",
    "    l.append ( [ (x-1) for x in ( (ids+1)*(kmeans.labels_ == n)) if x > 0] )\n",
    "    \n",
    "\n",
    "mat = []\n",
    "matTest = []\n",
    "\n",
    "n = 0\n",
    "for ind in l:\n",
    "    print(ind)\n",
    "    newDf = train.loc[train['stock_id'].isin(ind) ]\n",
    "    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n",
    "    newDf.loc[:,'stock_id'] = str(n)+'c1'\n",
    "    mat.append ( newDf )\n",
    "    \n",
    "    newDf = test.loc[test['stock_id'].isin(ind) ]    \n",
    "    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n",
    "    newDf.loc[:,'stock_id'] = str(n)+'c1'\n",
    "    matTest.append ( newDf )\n",
    "    \n",
    "    n+=1\n",
    "    \n",
    "mat1 = pd.concat(mat).reset_index()\n",
    "mat1.drop(columns=['target'],inplace=True)\n",
    "\n",
    "mat2 = pd.concat(matTest).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-16T16:45:54.796437Z",
     "iopub.status.busy": "2021-09-16T16:45:54.795994Z",
     "iopub.status.idle": "2021-09-16T16:45:54.962648Z",
     "shell.execute_reply": "2021-09-16T16:45:54.961753Z",
     "shell.execute_reply.started": "2021-09-16T16:45:54.796382Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of ['stock_id'] are in the columns\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-152-59019010fc55>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmat2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmat2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmat1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmat1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime_id\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmat1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmat1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpivot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'time_id'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'stock_id'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mmat1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"_\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmat1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmat1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mpivot\u001b[1;34m(self, index, columns, values)\u001b[0m\n\u001b[0;32m   6877\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpivot\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpivot\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6878\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6879\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mpivot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6880\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6881\u001b[0m     _shared_docs[\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\pivot.py\u001b[0m in \u001b[0;36mpivot\u001b[1;34m(data, index, columns, values)\u001b[0m\n\u001b[0;32m    439\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    440\u001b[0m         \u001b[0mappend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 441\u001b[1;33m         \u001b[0mindexed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcols\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mappend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    442\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    443\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mset_index\u001b[1;34m(self, keys, drop, append, inplace, verify_integrity)\u001b[0m\n\u001b[0;32m   4725\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4726\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmissing\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4727\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"None of {missing} are in the columns\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4728\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4729\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of ['stock_id'] are in the columns\""
     ]
    }
   ],
   "source": [
    "mat2 = pd.concat([mat2,mat1.loc[mat1.time_id==5]])\n",
    "mat1 = mat1.pivot(index='time_id', columns='stock_id')\n",
    "mat1.columns = [\"_\".join(x) for x in mat1.columns.ravel()]\n",
    "mat1.reset_index(inplace=True)\n",
    "\n",
    "mat2 = mat2.pivot(index='time_id', columns='stock_id')\n",
    "mat2.columns = [\"_\".join(x) for x in mat2.columns.ravel()]\n",
    "mat2.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-16T16:45:57.550031Z",
     "iopub.status.busy": "2021-09-16T16:45:57.549693Z",
     "iopub.status.idle": "2021-09-16T16:46:05.862654Z",
     "shell.execute_reply": "2021-09-16T16:46:05.861680Z",
     "shell.execute_reply.started": "2021-09-16T16:45:57.550000Z"
    }
   },
   "outputs": [],
   "source": [
    "nnn = ['time_id',\n",
    "     'log_return1_realized_volatility_0c1',\n",
    "     'log_return1_realized_volatility_1c1',     \n",
    "     'log_return1_realized_volatility_3c1',\n",
    "     'log_return1_realized_volatility_4c1',     \n",
    "     'log_return1_realized_volatility_6c1',\n",
    "     'total_volume_sum_0c1',\n",
    "     'total_volume_sum_1c1', \n",
    "     'total_volume_sum_3c1',\n",
    "     'total_volume_sum_4c1', \n",
    "     'total_volume_sum_6c1',\n",
    "     'trade_size_sum_0c1',\n",
    "     'trade_size_sum_1c1', \n",
    "     'trade_size_sum_3c1',\n",
    "     'trade_size_sum_4c1', \n",
    "     'trade_size_sum_6c1',\n",
    "     'trade_order_count_sum_0c1',\n",
    "     'trade_order_count_sum_1c1',\n",
    "     'trade_order_count_sum_3c1',\n",
    "     'trade_order_count_sum_4c1',\n",
    "     'trade_order_count_sum_6c1',      \n",
    "     'price_spread_sum_0c1',\n",
    "     'price_spread_sum_1c1',\n",
    "     'price_spread_sum_3c1',\n",
    "     'price_spread_sum_4c1',\n",
    "     'price_spread_sum_6c1',   \n",
    "     'bid_spread_sum_0c1',\n",
    "     'bid_spread_sum_1c1',\n",
    "     'bid_spread_sum_3c1',\n",
    "     'bid_spread_sum_4c1',\n",
    "     'bid_spread_sum_6c1',       \n",
    "     'ask_spread_sum_0c1',\n",
    "     'ask_spread_sum_1c1',\n",
    "     'ask_spread_sum_3c1',\n",
    "     'ask_spread_sum_4c1',\n",
    "     'ask_spread_sum_6c1',   \n",
    "     'volume_imbalance_sum_0c1',\n",
    "     'volume_imbalance_sum_1c1',\n",
    "     'volume_imbalance_sum_3c1',\n",
    "     'volume_imbalance_sum_4c1',\n",
    "     'volume_imbalance_sum_6c1',       \n",
    "     'bid_ask_spread_sum_0c1',\n",
    "     'bid_ask_spread_sum_1c1',\n",
    "     'bid_ask_spread_sum_3c1',\n",
    "     'bid_ask_spread_sum_4c1',\n",
    "     'bid_ask_spread_sum_6c1',\n",
    "     'size_tau2_0c1',\n",
    "     'size_tau2_1c1',\n",
    "     'size_tau2_3c1',\n",
    "     'size_tau2_4c1',\n",
    "     'size_tau2_6c1'] \n",
    "train = pd.merge(train,mat1[nnn],how='left',on='time_id')\n",
    "test = pd.merge(test,mat2[nnn],how='left',on='time_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-16T16:46:05.864345Z",
     "iopub.status.busy": "2021-09-16T16:46:05.864046Z",
     "iopub.status.idle": "2021-09-16T16:46:05.992180Z",
     "shell.execute_reply": "2021-09-16T16:46:05.991083Z",
     "shell.execute_reply.started": "2021-09-16T16:46:05.864317Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "277"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "del mat1,mat2\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-16T16:47:25.612936Z",
     "iopub.status.busy": "2021-09-16T16:47:25.612603Z",
     "iopub.status.idle": "2021-09-16T16:56:42.236493Z",
     "shell.execute_reply": "2021-09-16T16:56:42.235424Z",
     "shell.execute_reply.started": "2021-09-16T16:47:25.612909Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 1\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[250]\ttraining's rmse: 0.000430994\ttraining's RMSPE: 0.199326\tvalid_1's rmse: 0.000440958\tvalid_1's RMSPE: 0.204668\n",
      "[500]\ttraining's rmse: 0.000408977\ttraining's RMSPE: 0.189144\tvalid_1's rmse: 0.000425552\tvalid_1's RMSPE: 0.197517\n",
      "[750]\ttraining's rmse: 0.000395663\ttraining's RMSPE: 0.182986\tvalid_1's rmse: 0.000417597\tvalid_1's RMSPE: 0.193825\n",
      "[1000]\ttraining's rmse: 0.000386303\ttraining's RMSPE: 0.178657\tvalid_1's rmse: 0.000413598\tvalid_1's RMSPE: 0.191969\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's rmse: 0.000386303\ttraining's RMSPE: 0.178657\tvalid_1's rmse: 0.000413598\tvalid_1's RMSPE: 0.191969\n",
      "Training fold 2\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[250]\ttraining's rmse: 0.000430364\ttraining's RMSPE: 0.199382\tvalid_1's rmse: 0.000439559\tvalid_1's RMSPE: 0.202596\n",
      "[500]\ttraining's rmse: 0.000408171\ttraining's RMSPE: 0.189101\tvalid_1's rmse: 0.000423541\tvalid_1's RMSPE: 0.195213\n",
      "[750]\ttraining's rmse: 0.000394386\ttraining's RMSPE: 0.182714\tvalid_1's rmse: 0.000415446\tvalid_1's RMSPE: 0.191482\n",
      "[1000]\ttraining's rmse: 0.00038486\ttraining's RMSPE: 0.178301\tvalid_1's rmse: 0.000411058\tvalid_1's RMSPE: 0.18946\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's rmse: 0.00038486\ttraining's RMSPE: 0.178301\tvalid_1's rmse: 0.000411058\tvalid_1's RMSPE: 0.18946\n",
      "Training fold 3\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[250]\ttraining's rmse: 0.000430055\ttraining's RMSPE: 0.198885\tvalid_1's rmse: 0.000465143\tvalid_1's RMSPE: 0.215923\n",
      "[500]\ttraining's rmse: 0.000408318\ttraining's RMSPE: 0.188832\tvalid_1's rmse: 0.000449582\tvalid_1's RMSPE: 0.2087\n",
      "[750]\ttraining's rmse: 0.000394979\ttraining's RMSPE: 0.182663\tvalid_1's rmse: 0.000441528\tvalid_1's RMSPE: 0.204961\n",
      "[1000]\ttraining's rmse: 0.000385424\ttraining's RMSPE: 0.178244\tvalid_1's rmse: 0.000438051\tvalid_1's RMSPE: 0.203347\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's rmse: 0.000385424\ttraining's RMSPE: 0.178244\tvalid_1's rmse: 0.000438051\tvalid_1's RMSPE: 0.203347\n",
      "Training fold 4\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[250]\ttraining's rmse: 0.000429625\ttraining's RMSPE: 0.19904\tvalid_1's rmse: 0.00044319\tvalid_1's RMSPE: 0.204268\n",
      "[500]\ttraining's rmse: 0.000407723\ttraining's RMSPE: 0.188893\tvalid_1's rmse: 0.000427249\tvalid_1's RMSPE: 0.19692\n",
      "[750]\ttraining's rmse: 0.00039451\ttraining's RMSPE: 0.182772\tvalid_1's rmse: 0.000419613\tvalid_1's RMSPE: 0.193401\n",
      "[1000]\ttraining's rmse: 0.000385371\ttraining's RMSPE: 0.178538\tvalid_1's rmse: 0.000415944\tvalid_1's RMSPE: 0.19171\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's rmse: 0.000385371\ttraining's RMSPE: 0.178538\tvalid_1's rmse: 0.000415944\tvalid_1's RMSPE: 0.19171\n",
      "Training fold 5\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[250]\ttraining's rmse: 0.000431515\ttraining's RMSPE: 0.199594\tvalid_1's rmse: 0.000442956\tvalid_1's RMSPE: 0.205482\n",
      "[500]\ttraining's rmse: 0.000408956\ttraining's RMSPE: 0.18916\tvalid_1's rmse: 0.000427419\tvalid_1's RMSPE: 0.198275\n",
      "[750]\ttraining's rmse: 0.000395779\ttraining's RMSPE: 0.183065\tvalid_1's rmse: 0.000420484\tvalid_1's RMSPE: 0.195058\n",
      "[1000]\ttraining's rmse: 0.00038649\ttraining's RMSPE: 0.178768\tvalid_1's rmse: 0.00041685\tvalid_1's RMSPE: 0.193372\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's rmse: 0.00038649\ttraining's RMSPE: 0.178768\tvalid_1's rmse: 0.00041685\tvalid_1's RMSPE: 0.193372\n",
      "Our out of folds RMSPE is 0.194032137528383\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import lightgbm as lgb\n",
    "import joblib\n",
    "\n",
    "seed0=2021\n",
    "params0 = {\n",
    "    'objective': 'rmse',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'max_depth': -1,\n",
    "    'max_bin':100,\n",
    "    'min_data_in_leaf':500,\n",
    "    'learning_rate': 0.05,\n",
    "    'subsample': 0.72,\n",
    "    'subsample_freq': 4,\n",
    "    'feature_fraction': 0.5,\n",
    "    'lambda_l1': 0.5,\n",
    "    'lambda_l2': 1.0,\n",
    "    'categorical_column':[0],\n",
    "    'seed':seed0,\n",
    "    'feature_fraction_seed': seed0,\n",
    "    'bagging_seed': seed0,\n",
    "    'drop_seed': seed0,\n",
    "    'data_random_seed': seed0,\n",
    "    'n_jobs':-1,\n",
    "    'verbose': -1}\n",
    "seed1=42\n",
    "params1 = {\n",
    "        'learning_rate': 0.1,        \n",
    "        'lambda_l1': 2,\n",
    "        'lambda_l2': 7,\n",
    "        'num_leaves': 800,\n",
    "        'min_sum_hessian_in_leaf': 20,\n",
    "        'feature_fraction': 0.8,\n",
    "        'feature_fraction_bynode': 0.8,\n",
    "        'bagging_fraction': 0.9,\n",
    "        'bagging_freq': 42,\n",
    "        'min_data_in_leaf': 700,\n",
    "        'max_depth': 4,\n",
    "        'categorical_column':[0],\n",
    "        'seed': seed1,\n",
    "        'feature_fraction_seed': seed1,\n",
    "        'bagging_seed': seed1,\n",
    "        'drop_seed': seed1,\n",
    "        'data_random_seed': seed1,\n",
    "        'objective': 'rmse',\n",
    "        'boosting': 'gbdt',\n",
    "        'verbosity': -1,\n",
    "        'n_jobs':-1,\n",
    "    }\n",
    "# Function to early stop with root mean squared percentage error\n",
    "def rmspe(y_true, y_pred):\n",
    "    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
    "\n",
    "def feval_rmspe(y_pred, lgb_train):\n",
    "    y_true = lgb_train.get_label()\n",
    "    return 'RMSPE', rmspe(y_true, y_pred), False\n",
    "\n",
    "def train_and_evaluate_lgb(train, test, params):\n",
    "    # Hyperparammeters (just basic)\n",
    "    \n",
    "    features = [col for col in train.columns if col not in {\"time_id\", \"target\", \"row_id\"}]\n",
    "    y = train['target']\n",
    "    # Create out of folds array\n",
    "    oof_predictions = np.zeros(train.shape[0])\n",
    "    # Create test array to store predictions\n",
    "    test_predictions = np.zeros(test.shape[0])\n",
    "    # Create a KFold object\n",
    "    kfold = KFold(n_splits = 5, random_state = 2021, shuffle = True)\n",
    "    # Iterate through each fold\n",
    "    for fold, (trn_ind, val_ind) in enumerate(kfold.split(train)):\n",
    "        print(f'Training fold {fold + 1}')\n",
    "        x_train, x_val = train.iloc[trn_ind], train.iloc[val_ind]\n",
    "        y_train, y_val = y.iloc[trn_ind], y.iloc[val_ind]\n",
    "        # Root mean squared percentage error weights\n",
    "        train_weights = 1 / np.square(y_train)\n",
    "        val_weights = 1 / np.square(y_val)\n",
    "        train_dataset = lgb.Dataset(x_train[features], y_train, weight = train_weights)\n",
    "        val_dataset = lgb.Dataset(x_val[features], y_val, weight = val_weights)\n",
    "        model = lgb.train(params = params,\n",
    "                          num_boost_round=1000,\n",
    "                          train_set = train_dataset, \n",
    "                          valid_sets = [train_dataset, val_dataset], \n",
    "                          verbose_eval = 250,\n",
    "                          early_stopping_rounds=50,\n",
    "                          feval = feval_rmspe)\n",
    "        joblib.dump(model, 'lgb1_'+str(fold)+'.pkl')\n",
    "        # Add predictions to the out of folds array\n",
    "        oof_predictions[val_ind] = model.predict(x_val[features])\n",
    "        # Predict the test set\n",
    "        test_predictions += model.predict(test[features]) / 5\n",
    "    rmspe_score = rmspe(y, oof_predictions)\n",
    "    print(f'Our out of folds RMSPE is {rmspe_score}')\n",
    "    lgb.plot_importance(model,max_num_features=20)\n",
    "    # Return test predictions\n",
    "    return test_predictions\n",
    "# Traing and evaluate\n",
    "predictions_lgb1= train_and_evaluate_lgb(train, test,params0)\n",
    "#test['target'] = predictions_lgb\n",
    "#test[['row_id', 'target']].to_csv('submission.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-16T17:00:44.895839Z",
     "iopub.status.busy": "2021-09-16T17:00:44.895500Z",
     "iopub.status.idle": "2021-09-16T17:11:42.485334Z",
     "shell.execute_reply": "2021-09-16T17:11:42.484556Z",
     "shell.execute_reply.started": "2021-09-16T17:00:44.895801Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 1\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[250]\ttraining's rmse: 0.000430142\ttraining's RMSPE: 0.199146\tvalid_1's rmse: 0.000446386\tvalid_1's RMSPE: 0.206296\n",
      "[500]\ttraining's rmse: 0.000407992\ttraining's RMSPE: 0.188892\tvalid_1's rmse: 0.000430204\tvalid_1's RMSPE: 0.198818\n",
      "[750]\ttraining's rmse: 0.000394696\ttraining's RMSPE: 0.182736\tvalid_1's rmse: 0.000422615\tvalid_1's RMSPE: 0.19531\n",
      "[1000]\ttraining's rmse: 0.000385403\ttraining's RMSPE: 0.178434\tvalid_1's rmse: 0.000418792\tvalid_1's RMSPE: 0.193544\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1200]\ttraining's rmse: 0.000379188\ttraining's RMSPE: 0.175556\tvalid_1's rmse: 0.000416637\tvalid_1's RMSPE: 0.192548\n",
      "Training fold 2\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[250]\ttraining's rmse: 0.000430852\ttraining's RMSPE: 0.199268\tvalid_1's rmse: 0.000456626\tvalid_1's RMSPE: 0.211906\n",
      "[500]\ttraining's rmse: 0.000409078\ttraining's RMSPE: 0.189198\tvalid_1's rmse: 0.000441065\tvalid_1's RMSPE: 0.204684\n",
      "[750]\ttraining's rmse: 0.000395885\ttraining's RMSPE: 0.183096\tvalid_1's rmse: 0.000432935\tvalid_1's RMSPE: 0.200911\n",
      "[1000]\ttraining's rmse: 0.000386476\ttraining's RMSPE: 0.178745\tvalid_1's rmse: 0.000429147\tvalid_1's RMSPE: 0.199154\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1200]\ttraining's rmse: 0.000380291\ttraining's RMSPE: 0.175884\tvalid_1's rmse: 0.000427718\tvalid_1's RMSPE: 0.19849\n",
      "Training fold 3\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[250]\ttraining's rmse: 0.000429592\ttraining's RMSPE: 0.198911\tvalid_1's rmse: 0.000440023\tvalid_1's RMSPE: 0.203277\n",
      "[500]\ttraining's rmse: 0.000407712\ttraining's RMSPE: 0.18878\tvalid_1's rmse: 0.000423983\tvalid_1's RMSPE: 0.195867\n",
      "[750]\ttraining's rmse: 0.000394432\ttraining's RMSPE: 0.182631\tvalid_1's rmse: 0.000416228\tvalid_1's RMSPE: 0.192285\n",
      "[1000]\ttraining's rmse: 0.000385297\ttraining's RMSPE: 0.178401\tvalid_1's rmse: 0.000412282\tvalid_1's RMSPE: 0.190462\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1200]\ttraining's rmse: 0.00037912\ttraining's RMSPE: 0.175542\tvalid_1's rmse: 0.000410001\tvalid_1's RMSPE: 0.189408\n",
      "Training fold 4\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[250]\ttraining's rmse: 0.0004311\ttraining's RMSPE: 0.199268\tvalid_1's rmse: 0.000443964\tvalid_1's RMSPE: 0.206501\n",
      "[500]\ttraining's rmse: 0.000408639\ttraining's RMSPE: 0.188886\tvalid_1's rmse: 0.000428143\tvalid_1's RMSPE: 0.199142\n",
      "[750]\ttraining's rmse: 0.000395016\ttraining's RMSPE: 0.182589\tvalid_1's rmse: 0.000420436\tvalid_1's RMSPE: 0.195557\n",
      "[1000]\ttraining's rmse: 0.000385594\ttraining's RMSPE: 0.178234\tvalid_1's rmse: 0.000416837\tvalid_1's RMSPE: 0.193883\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1200]\ttraining's rmse: 0.000379446\ttraining's RMSPE: 0.175392\tvalid_1's rmse: 0.000414618\tvalid_1's RMSPE: 0.192851\n",
      "Training fold 5\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[250]\ttraining's rmse: 0.000430205\ttraining's RMSPE: 0.199327\tvalid_1's rmse: 0.000449865\tvalid_1's RMSPE: 0.207269\n",
      "[500]\ttraining's rmse: 0.000407915\ttraining's RMSPE: 0.188999\tvalid_1's rmse: 0.000434479\tvalid_1's RMSPE: 0.20018\n",
      "[750]\ttraining's rmse: 0.000394845\ttraining's RMSPE: 0.182944\tvalid_1's rmse: 0.000427691\tvalid_1's RMSPE: 0.197052\n",
      "[1000]\ttraining's rmse: 0.000385119\ttraining's RMSPE: 0.178437\tvalid_1's rmse: 0.000423499\tvalid_1's RMSPE: 0.195121\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1200]\ttraining's rmse: 0.000379178\ttraining's RMSPE: 0.175685\tvalid_1's rmse: 0.000421945\tvalid_1's RMSPE: 0.194405\n",
      "Our out of folds RMSPE is 0.19356314681930736\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import lightgbm as lgb\n",
    "\n",
    "seed0=1111\n",
    "params111 = {\n",
    "    'objective': 'rmse',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'max_depth': -1,\n",
    "    'max_bin':100,\n",
    "    'min_data_in_leaf':500,\n",
    "    'learning_rate': 0.05,\n",
    "    'subsample': 0.72,\n",
    "    'subsample_freq': 4,\n",
    "    'feature_fraction': 0.5,\n",
    "    'lambda_l1': 0.5,\n",
    "    'lambda_l2': 1.0,\n",
    "    'categorical_column':[0],\n",
    "    'seed':seed0,\n",
    "    'feature_fraction_seed': seed0,\n",
    "    'bagging_seed': seed0,\n",
    "    'drop_seed': seed0,\n",
    "    'data_random_seed': seed0,\n",
    "    'n_jobs':-1,\n",
    "    'verbose': -1}\n",
    "seed1=42\n",
    "params1 = {\n",
    "        'learning_rate': 0.1,        \n",
    "        'lambda_l1': 2,\n",
    "        'lambda_l2': 7,\n",
    "        'num_leaves': 800,\n",
    "        'min_sum_hessian_in_leaf': 20,\n",
    "        'feature_fraction': 0.8,\n",
    "        'feature_fraction_bynode': 0.8,\n",
    "        'bagging_fraction': 0.9,\n",
    "        'bagging_freq': 42,\n",
    "        'min_data_in_leaf': 700,\n",
    "        'max_depth': 4,\n",
    "        'categorical_column':[0],\n",
    "        'seed': seed1,\n",
    "        'feature_fraction_seed': seed1,\n",
    "        'bagging_seed': seed1,\n",
    "        'drop_seed': seed1,\n",
    "        'data_random_seed': seed1,\n",
    "        'objective': 'rmse',\n",
    "        'boosting': 'gbdt',\n",
    "        'verbosity': -1,\n",
    "        'n_jobs':-1,\n",
    "    }\n",
    "# Function to early stop with root mean squared percentage error\n",
    "def rmspe(y_true, y_pred):\n",
    "    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
    "\n",
    "def feval_rmspe(y_pred, lgb_train):\n",
    "    y_true = lgb_train.get_label()\n",
    "    return 'RMSPE', rmspe(y_true, y_pred), False\n",
    "\n",
    "def train_and_evaluate_lgb(train, test, params):\n",
    "    # Hyperparammeters (just basic)\n",
    "    \n",
    "    features = [col for col in train.columns if col not in {\"time_id\", \"target\", \"row_id\"}]\n",
    "    y = train['target']\n",
    "    # Create out of folds array\n",
    "    oof_predictions = np.zeros(train.shape[0])\n",
    "    # Create test array to store predictions\n",
    "    test_predictions = np.zeros(test.shape[0])\n",
    "    # Create a KFold object\n",
    "    kfold = KFold(n_splits = 5, random_state = 1111, shuffle = True)\n",
    "    # Iterate through each fold\n",
    "    for fold, (trn_ind, val_ind) in enumerate(kfold.split(train)):\n",
    "        print(f'Training fold {fold + 1}')\n",
    "        x_train, x_val = train.iloc[trn_ind], train.iloc[val_ind]\n",
    "        y_train, y_val = y.iloc[trn_ind], y.iloc[val_ind]\n",
    "        # Root mean squared percentage error weights\n",
    "        train_weights = 1 / np.square(y_train)\n",
    "        val_weights = 1 / np.square(y_val)\n",
    "        train_dataset = lgb.Dataset(x_train[features], y_train, weight = train_weights)\n",
    "        val_dataset = lgb.Dataset(x_val[features], y_val, weight = val_weights)\n",
    "        model = lgb.train(params = params,\n",
    "                          num_boost_round=1200,\n",
    "                          train_set = train_dataset, \n",
    "                          valid_sets = [train_dataset, val_dataset], \n",
    "                          verbose_eval = 250,\n",
    "                          early_stopping_rounds=50,\n",
    "                          feval = feval_rmspe)\n",
    "        # Add predictions to the out of folds array\n",
    "        joblib.dump(model, 'lgb2_'+str(fold)+'.pkl')\n",
    "        oof_predictions[val_ind] = model.predict(x_val[features])\n",
    "        # Predict the test set\n",
    "        test_predictions += model.predict(test[features]) / 5\n",
    "    rmspe_score = rmspe(y, oof_predictions)\n",
    "    print(f'Our out of folds RMSPE is {rmspe_score}')\n",
    "    lgb.plot_importance(model,max_num_features=20)\n",
    "    # Return test predictions\n",
    "    return test_predictions\n",
    "# Traing and evaluate\n",
    "predictions_lgb2= train_and_evaluate_lgb(train, test,params111)\n",
    "#test['target'] = predictions_lgb\n",
    "#test[['row_id', 'target']].to_csv('submission.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-16T17:17:38.819861Z",
     "iopub.status.busy": "2021-09-16T17:17:38.819391Z",
     "iopub.status.idle": "2021-09-16T17:17:38.827080Z",
     "shell.execute_reply": "2021-09-16T17:17:38.826227Z",
     "shell.execute_reply.started": "2021-09-16T17:17:38.819817Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.00201677]), array([0.00176633]))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_lgb2,predictions_lgb1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-16T17:17:41.559558Z",
     "iopub.status.busy": "2021-09-16T17:17:41.558994Z",
     "iopub.status.idle": "2021-09-16T17:17:41.565295Z",
     "shell.execute_reply": "2021-09-16T17:17:41.564546Z",
     "shell.execute_reply.started": "2021-09-16T17:17:41.559522Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from numpy.random import seed\n",
    "seed(42)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "def root_mean_squared_per_error(y_true, y_pred):\n",
    "         return K.sqrt(K.mean(K.square( (y_true - y_pred)/ y_true )))\n",
    "    \n",
    "es = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', patience=20, verbose=0,\n",
    "    mode='min',restore_best_weights=True)\n",
    "filepath = 'nn_model.hdf5'\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=filepath, \n",
    "                             monitor='val_loss',\n",
    "                             verbose=1, \n",
    "                             save_best_only=True,\n",
    "                             mode='min')\n",
    "plateau = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.2, patience=7, verbose=0,\n",
    "    mode='min')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-16T17:17:44.706848Z",
     "iopub.status.busy": "2021-09-16T17:17:44.706510Z",
     "iopub.status.idle": "2021-09-16T17:17:53.297248Z",
     "shell.execute_reply": "2021-09-16T17:17:53.296283Z",
     "shell.execute_reply.started": "2021-09-16T17:17:44.706817Z"
    }
   },
   "outputs": [],
   "source": [
    "# kfold based on the knn++ algorithm\n",
    "\n",
    "out_train = pd.read_csv(data_dir+'train.csv')\n",
    "out_train = out_train.pivot(index='time_id', columns='stock_id', values='target')\n",
    "\n",
    "#out_train[out_train.isna().any(axis=1)]\n",
    "out_train = out_train.fillna(out_train.mean())\n",
    "out_train.head()\n",
    "\n",
    "# code to add the just the read data after first execution\n",
    "\n",
    "# data separation based on knn ++\n",
    "nfolds = 5 # number of folds\n",
    "index = []\n",
    "totDist = []\n",
    "values = []\n",
    "# generates a matriz with the values of \n",
    "mat = out_train.values\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "mat = scaler.fit_transform(mat)\n",
    "\n",
    "nind = int(mat.shape[0]/nfolds) # number of individuals\n",
    "\n",
    "# adds index in the last column\n",
    "mat = np.c_[mat,np.arange(mat.shape[0])]\n",
    "\n",
    "\n",
    "lineNumber = np.random.choice(np.array(mat.shape[0]), size=nfolds, replace=False)\n",
    "\n",
    "lineNumber = np.sort(lineNumber)[::-1]\n",
    "\n",
    "for n in range(nfolds):\n",
    "    totDist.append(np.zeros(mat.shape[0]-nfolds))\n",
    "\n",
    "# saves index\n",
    "for n in range(nfolds):\n",
    "    \n",
    "    values.append([lineNumber[n]])    \n",
    "\n",
    "\n",
    "s=[]\n",
    "for n in range(nfolds):\n",
    "    s.append(mat[lineNumber[n],:])\n",
    "    \n",
    "    mat = np.delete(mat, obj=lineNumber[n], axis=0)\n",
    "\n",
    "for n in range(nind-1):    \n",
    "\n",
    "    luck = np.random.uniform(0,1,nfolds)\n",
    "    \n",
    "    for cycle in range(nfolds):\n",
    "         # saves the values of index           \n",
    "\n",
    "        s[cycle] = np.matlib.repmat(s[cycle], mat.shape[0], 1)\n",
    "\n",
    "        sumDist = np.sum( (mat[:,:-1] - s[cycle][:,:-1])**2 , axis=1)   \n",
    "        totDist[cycle] += sumDist        \n",
    "                \n",
    "        # probabilities\n",
    "        f = totDist[cycle]/np.sum(totDist[cycle]) # normalizing the totdist\n",
    "        j = 0\n",
    "        kn = 0\n",
    "        for val in f:\n",
    "            j += val        \n",
    "            if (j > luck[cycle]): # the column was selected\n",
    "                break\n",
    "            kn +=1\n",
    "        lineNumber[cycle] = kn\n",
    "        \n",
    "        # delete line of the value added    \n",
    "        for n_iter in range(nfolds):\n",
    "            \n",
    "            totDist[n_iter] = np.delete(totDist[n_iter],obj=lineNumber[cycle], axis=0)\n",
    "            j= 0\n",
    "        \n",
    "        s[cycle] = mat[lineNumber[cycle],:]\n",
    "        values[cycle].append(int(mat[lineNumber[cycle],-1]))\n",
    "        mat = np.delete(mat, obj=lineNumber[cycle], axis=0)\n",
    "\n",
    "\n",
    "for n_mod in range(nfolds):\n",
    "    values[n_mod] = out_train.index[values[n_mod]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-16T17:17:53.299184Z",
     "iopub.status.busy": "2021-09-16T17:17:53.298803Z",
     "iopub.status.idle": "2021-09-16T17:17:53.328842Z",
     "shell.execute_reply": "2021-09-16T17:17:53.327150Z",
     "shell.execute_reply.started": "2021-09-16T17:17:53.299143Z"
    }
   },
   "outputs": [],
   "source": [
    "#colNames.remove('row_id')\n",
    "train.replace([np.inf, -np.inf], np.nan,inplace=True)\n",
    "test.replace([np.inf, -np.inf], np.nan,inplace=True)\n",
    "qt_train = []\n",
    "train_nn=train[colNames].copy()\n",
    "test_nn=test[colNames].copy()\n",
    "for col in colNames:\n",
    "    #print(col)\n",
    "    qt = QuantileTransformer(random_state=21,n_quantiles=2000, output_distribution='normal')\n",
    "    train_nn[col] = qt.fit_transform(train_nn[[col]])\n",
    "    test_nn[col] = qt.transform(test_nn[[col]])    \n",
    "    qt_train.append(qt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-09-16T17:17:53.329794Z",
     "iopub.status.idle": "2021-09-16T17:17:53.330207Z"
    }
   },
   "outputs": [],
   "source": [
    "train_nn[['stock_id','time_id','target']]=train[['stock_id','time_id','target']]\n",
    "test_nn[['stock_id','time_id']]=test[['stock_id','time_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-09-16T17:17:53.331121Z",
     "iopub.status.idle": "2021-09-16T17:17:53.331527Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 4 2 1 1 2 4 6 2 1 0 4 4 1 1 1 2 4 4 4 0 1 1 3 1 1 4 3 4 3 4 4 1 3 3 4\n",
      " 3 4 1 4 1 4 4 1 0 4 4 1 0 0 3 3 3 2 0 2 4 1 4 4 1 4 1 0 3 3 0 3 0 6 5 3 3\n",
      " 0 1 2 0 3 3 3 4 1 1 0 2 3 3 1 0 1 4 4 4 4 4 1 3 1 0 1 4 1 0 1 4 1 0 4 0 4\n",
      " 0]\n",
      "[1, 11, 22, 50, 55, 56, 62, 73, 76, 78, 84, 87, 96, 101, 112, 116, 122, 124, 126]\n",
      "[0, 4, 5, 10, 15, 16, 17, 23, 26, 28, 29, 36, 42, 44, 48, 53, 66, 69, 72, 85, 94, 95, 100, 102, 109, 111, 113, 115, 118, 120]\n",
      "[3, 6, 9, 18, 61, 63, 86, 97]\n",
      "[27, 31, 33, 37, 38, 40, 58, 59, 60, 74, 75, 77, 82, 83, 88, 89, 90, 98, 99, 110]\n",
      "[2, 7, 13, 14, 19, 20, 21, 30, 32, 34, 35, 39, 41, 43, 46, 47, 51, 52, 64, 67, 68, 70, 93, 103, 104, 105, 107, 108, 114, 119, 123, 125]\n",
      "[81]\n",
      "[8, 80]\n"
     ]
    }
   ],
   "source": [
    "# making agg features\n",
    "from sklearn.cluster import KMeans\n",
    "train_p = pd.read_csv(data_dir+'train.csv')\n",
    "train_p = train_p.pivot(index='time_id', columns='stock_id', values='target')\n",
    "\n",
    "corr = train_p.corr()\n",
    "\n",
    "ids = corr.index\n",
    "\n",
    "kmeans = KMeans(n_clusters=7, random_state=0).fit(corr.values)\n",
    "print(kmeans.labels_)\n",
    "\n",
    "l = []\n",
    "for n in range(7):\n",
    "    l.append ( [ (x-1) for x in ( (ids+1)*(kmeans.labels_ == n)) if x > 0] )\n",
    "    \n",
    "\n",
    "mat = []\n",
    "matTest = []\n",
    "\n",
    "n = 0\n",
    "for ind in l:\n",
    "    print(ind)\n",
    "    newDf = train_nn.loc[train_nn['stock_id'].isin(ind) ]\n",
    "    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n",
    "    newDf.loc[:,'stock_id'] = str(n)+'c1'\n",
    "    mat.append ( newDf )\n",
    "    \n",
    "    newDf = test_nn.loc[test_nn['stock_id'].isin(ind) ]    \n",
    "    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n",
    "    newDf.loc[:,'stock_id'] = str(n)+'c1'\n",
    "    matTest.append ( newDf )\n",
    "    \n",
    "    n+=1\n",
    "    \n",
    "mat1 = pd.concat(mat).reset_index()\n",
    "mat1.drop(columns=['target'],inplace=True)\n",
    "\n",
    "mat2 = pd.concat(matTest).reset_index()\n",
    "mat2 = pd.concat([mat2,mat1.loc[mat1.time_id==5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-09-16T17:17:53.332514Z",
     "iopub.status.idle": "2021-09-16T17:17:53.332887Z"
    }
   },
   "outputs": [],
   "source": [
    "nnn = ['time_id',\n",
    "     'log_return1_realized_volatility_0c1',\n",
    "     'log_return1_realized_volatility_1c1',     \n",
    "     'log_return1_realized_volatility_3c1',\n",
    "     'log_return1_realized_volatility_4c1',     \n",
    "     'log_return1_realized_volatility_6c1',\n",
    "     'total_volume_sum_0c1',\n",
    "     'total_volume_sum_1c1', \n",
    "     'total_volume_sum_3c1',\n",
    "     'total_volume_sum_4c1', \n",
    "     'total_volume_sum_6c1',\n",
    "     'trade_size_sum_0c1',\n",
    "     'trade_size_sum_1c1', \n",
    "     'trade_size_sum_3c1',\n",
    "     'trade_size_sum_4c1', \n",
    "     'trade_size_sum_6c1',\n",
    "     'trade_order_count_sum_0c1',\n",
    "     'trade_order_count_sum_1c1',\n",
    "     'trade_order_count_sum_3c1',\n",
    "     'trade_order_count_sum_4c1',\n",
    "     'trade_order_count_sum_6c1',      \n",
    "     'price_spread_sum_0c1',\n",
    "     'price_spread_sum_1c1',\n",
    "     'price_spread_sum_3c1',\n",
    "     'price_spread_sum_4c1',\n",
    "     'price_spread_sum_6c1',   \n",
    "     'bid_spread_sum_0c1',\n",
    "     'bid_spread_sum_1c1',\n",
    "     'bid_spread_sum_3c1',\n",
    "     'bid_spread_sum_4c1',\n",
    "     'bid_spread_sum_6c1',       \n",
    "     'ask_spread_sum_0c1',\n",
    "     'ask_spread_sum_1c1',\n",
    "     'ask_spread_sum_3c1',\n",
    "     'ask_spread_sum_4c1',\n",
    "     'ask_spread_sum_6c1',   \n",
    "     'volume_imbalance_sum_0c1',\n",
    "     'volume_imbalance_sum_1c1',\n",
    "     'volume_imbalance_sum_3c1',\n",
    "     'volume_imbalance_sum_4c1',\n",
    "     'volume_imbalance_sum_6c1',       \n",
    "     'bid_ask_spread_sum_0c1',\n",
    "     'bid_ask_spread_sum_1c1',\n",
    "     'bid_ask_spread_sum_3c1',\n",
    "     'bid_ask_spread_sum_4c1',\n",
    "     'bid_ask_spread_sum_6c1',\n",
    "     'size_tau2_0c1',\n",
    "     'size_tau2_1c1',\n",
    "     'size_tau2_3c1',\n",
    "     'size_tau2_4c1',\n",
    "     'size_tau2_6c1'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-09-16T17:17:53.333652Z",
     "iopub.status.idle": "2021-09-16T17:17:53.334059Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amakr\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:2: FutureWarning: Index.ravel returning ndarray is deprecated; in a future version this will return a view on self.\n",
      "  \n",
      "C:\\Users\\amakr\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:6: FutureWarning: Index.ravel returning ndarray is deprecated; in a future version this will return a view on self.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "mat1 = mat1.pivot(index='time_id', columns='stock_id')\n",
    "mat1.columns = [\"_\".join(x) for x in mat1.columns.ravel()]\n",
    "mat1.reset_index(inplace=True)\n",
    "\n",
    "mat2 = mat2.pivot(index='time_id', columns='stock_id')\n",
    "mat2.columns = [\"_\".join(x) for x in mat2.columns.ravel()]\n",
    "mat2.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-09-16T17:17:53.335001Z",
     "iopub.status.idle": "2021-09-16T17:17:53.335427Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "train_nn = pd.merge(train_nn,mat1[nnn],how='left',on='time_id')\n",
    "test_nn = pd.merge(test_nn,mat2[nnn],how='left',on='time_id')\n",
    "\n",
    "train1=train_nn\n",
    "test1=test_nn\n",
    "del mat1,mat2\n",
    "del train,test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-09-16T17:17:53.336392Z",
     "iopub.status.idle": "2021-09-16T17:17:53.336761Z"
    }
   },
   "outputs": [],
   "source": [
    "#https://bignerdranch.com/blog/implementing-swish-activation-function-in-keras/\n",
    "from keras.backend import sigmoid\n",
    "def swish(x, beta = 1):\n",
    "    return (x * sigmoid(beta * x))\n",
    "\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "from keras.layers import Activation\n",
    "get_custom_objects().update({'swish': Activation(swish)})\n",
    "\n",
    "hidden_units = (128,64,32)\n",
    "stock_embedding_size = 24\n",
    "\n",
    "cat_data = train_nn['stock_id']\n",
    "\n",
    "def base_model():\n",
    "    \n",
    "    # Each instance will consist of two inputs: a single user id, and a single movie id\n",
    "    stock_id_input = keras.Input(shape=(1,), name='stock_id')\n",
    "    num_input = keras.Input(shape=(254,), name='num_data')\n",
    "\n",
    "\n",
    "    #embedding, flatenning and concatenating\n",
    "    stock_embedded = keras.layers.Embedding(max(cat_data)+1, stock_embedding_size, \n",
    "                                           input_length=1, name='stock_embedding')(stock_id_input)\n",
    "    stock_flattened = keras.layers.Flatten()(stock_embedded)\n",
    "    out = keras.layers.Concatenate()([stock_flattened, num_input])\n",
    "    \n",
    "    # Add one or more hidden layers\n",
    "    for n_hidden in hidden_units:\n",
    "\n",
    "        out = keras.layers.Dense(n_hidden, activation='swish')(out)\n",
    "        \n",
    "\n",
    "    #out = keras.layers.Concatenate()([out, num_input])\n",
    "\n",
    "    # A single output: our predicted rating\n",
    "    out = keras.layers.Dense(1, activation='linear', name='prediction')(out)\n",
    "    \n",
    "    model = keras.Model(\n",
    "    inputs = [stock_id_input, num_input],\n",
    "    outputs = out,\n",
    "    )\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-09-16T17:17:53.337713Z",
     "iopub.status.idle": "2021-09-16T17:17:53.338082Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to calculate the root mean squared percentage error\n",
    "def rmspe(y_true, y_pred):\n",
    "    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
    "\n",
    "# Function to early stop with root mean squared percentage error\n",
    "def feval_rmspe(y_pred, lgb_train):\n",
    "    y_true = lgb_train.get_label()\n",
    "    return 'RMSPE', rmspe(y_true, y_pred), False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-09-16T17:17:53.338892Z",
     "iopub.status.idle": "2021-09-16T17:17:53.339280Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV 1/5\n",
      "Epoch 1/1000\n",
      "168/168 [==============================] - ETA: 0s - loss: 7.6003\n",
      "Epoch 00001: val_loss improved from inf to 0.73565, saving model to nn_model_0.hdf5\n",
      "168/168 [==============================] - 3s 17ms/step - loss: 7.6003 - val_loss: 0.7356\n",
      "Epoch 2/1000\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.8637\n",
      "Epoch 00002: val_loss did not improve from 0.73565\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.8637 - val_loss: 0.7816\n",
      "Epoch 3/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.5980\n",
      "Epoch 00003: val_loss did not improve from 0.73565\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.6374 - val_loss: 6.7563\n",
      "Epoch 4/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 1.3323\n",
      "Epoch 00004: val_loss improved from 0.73565 to 0.58894, saving model to nn_model_0.hdf5\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 1.3298 - val_loss: 0.5889\n",
      "Epoch 5/1000\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.7127\n",
      "Epoch 00005: val_loss did not improve from 0.58894\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.7127 - val_loss: 0.9790\n",
      "Epoch 6/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 1.0121- ETA: 0s - lo\n",
      "Epoch 00006: val_loss did not improve from 0.58894\n",
      "168/168 [==============================] - 3s 18ms/step - loss: 1.0150 - val_loss: 1.5627\n",
      "Epoch 7/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.8424\n",
      "Epoch 00007: val_loss improved from 0.58894 to 0.31211, saving model to nn_model_0.hdf5\n",
      "168/168 [==============================] - 3s 17ms/step - loss: 0.8336 - val_loss: 0.3121\n",
      "Epoch 8/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2965- ETA: 0s - loss: 0.\n",
      "Epoch 00008: val_loss improved from 0.31211 to 0.30277, saving model to nn_model_0.hdf5\n",
      "168/168 [==============================] - 3s 17ms/step - loss: 0.2974 - val_loss: 0.3028\n",
      "Epoch 9/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.5161\n",
      "Epoch 00009: val_loss did not improve from 0.30277\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.5283 - val_loss: 2.9754\n",
      "Epoch 10/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 1.1011\n",
      "Epoch 00010: val_loss did not improve from 0.30277\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 1.0888 - val_loss: 0.3193\n",
      "Epoch 11/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2877- ETA: 0s - loss: 0 - ETA: 0s - loss: 0.\n",
      "Epoch 00011: val_loss did not improve from 0.30277\n",
      "168/168 [==============================] - 3s 17ms/step - loss: 0.2875 - val_loss: 0.3032\n",
      "Epoch 12/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.5474\n",
      "Epoch 00012: val_loss did not improve from 0.30277\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.5473 - val_loss: 0.3303\n",
      "Epoch 13/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2903\n",
      "Epoch 00013: val_loss improved from 0.30277 to 0.28384, saving model to nn_model_0.hdf5\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.2897 - val_loss: 0.2838\n",
      "Epoch 14/1000\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.5753\n",
      "Epoch 00014: val_loss did not improve from 0.28384\n",
      "168/168 [==============================] - 3s 17ms/step - loss: 0.5753 - val_loss: 1.1861\n",
      "Epoch 15/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.6472\n",
      "Epoch 00015: val_loss did not improve from 0.28384\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.6452 - val_loss: 0.4001\n",
      "Epoch 16/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.4426\n",
      "Epoch 00016: val_loss did not improve from 0.28384\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.4420 - val_loss: 0.4331\n",
      "Epoch 17/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2713\n",
      "Epoch 00017: val_loss improved from 0.28384 to 0.23298, saving model to nn_model_0.hdf5\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2708 - val_loss: 0.2330\n",
      "Epoch 18/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.3564\n",
      "Epoch 00018: val_loss did not improve from 0.23298\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.3576 - val_loss: 0.4247\n",
      "Epoch 19/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.4003\n",
      "Epoch 00019: val_loss did not improve from 0.23298\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.4004 - val_loss: 0.3251\n",
      "Epoch 20/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.3951\n",
      "Epoch 00020: val_loss did not improve from 0.23298\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.3952 - val_loss: 0.4203\n",
      "Epoch 21/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.3640\n",
      "Epoch 00021: val_loss did not improve from 0.23298\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.3639 - val_loss: 0.2964\n",
      "Epoch 22/1000\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.3559\n",
      "Epoch 00022: val_loss did not improve from 0.23298\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.3559 - val_loss: 0.2783\n",
      "Epoch 23/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2284\n",
      "Epoch 00023: val_loss improved from 0.23298 to 0.22187, saving model to nn_model_0.hdf5\n",
      "168/168 [==============================] - 3s 17ms/step - loss: 0.2284 - val_loss: 0.2219\n",
      "Epoch 24/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2317\n",
      "Epoch 00024: val_loss did not improve from 0.22187\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.2325 - val_loss: 0.2288\n",
      "Epoch 25/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2282\n",
      "Epoch 00025: val_loss did not improve from 0.22187\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2280 - val_loss: 0.2316\n",
      "Epoch 26/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2374\n",
      "Epoch 00026: val_loss did not improve from 0.22187\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2376 - val_loss: 0.2525\n",
      "Epoch 27/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2300\n",
      "Epoch 00027: val_loss did not improve from 0.22187\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2300 - val_loss: 0.2452\n",
      "Epoch 28/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2303\n",
      "Epoch 00028: val_loss did not improve from 0.22187\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2299 - val_loss: 0.2380\n",
      "Epoch 29/1000\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.2731\n",
      "Epoch 00029: val_loss improved from 0.22187 to 0.21925, saving model to nn_model_0.hdf5\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2731 - val_loss: 0.2193\n",
      "Epoch 30/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2401\n",
      "Epoch 00030: val_loss improved from 0.21925 to 0.21599, saving model to nn_model_0.hdf5\n",
      "168/168 [==============================] - 3s 17ms/step - loss: 0.2400 - val_loss: 0.2160\n",
      "Epoch 31/1000\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.2389\n",
      "Epoch 00031: val_loss did not improve from 0.21599\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.2389 - val_loss: 0.2275\n",
      "Epoch 32/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2388\n",
      "Epoch 00032: val_loss did not improve from 0.21599\n",
      "168/168 [==============================] - 3s 17ms/step - loss: 0.2398 - val_loss: 0.3422\n",
      "Epoch 33/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2452\n",
      "Epoch 00033: val_loss improved from 0.21599 to 0.21095, saving model to nn_model_0.hdf5\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2452 - val_loss: 0.2110\n",
      "Epoch 34/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2372\n",
      "Epoch 00034: val_loss did not improve from 0.21095\n",
      "168/168 [==============================] - 3s 18ms/step - loss: 0.2372 - val_loss: 0.2838\n",
      "Epoch 35/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2321\n",
      "Epoch 00035: val_loss did not improve from 0.21095\n",
      "168/168 [==============================] - 3s 17ms/step - loss: 0.2324 - val_loss: 0.2657\n",
      "Epoch 36/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2349\n",
      "Epoch 00036: val_loss did not improve from 0.21095\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2350 - val_loss: 0.2287\n",
      "Epoch 37/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2334\n",
      "Epoch 00037: val_loss did not improve from 0.21095\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.2334 - val_loss: 0.2206\n",
      "Epoch 38/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.2310\n",
      "Epoch 00038: val_loss did not improve from 0.21095\n",
      "168/168 [==============================] - 3s 17ms/step - loss: 0.2304 - val_loss: 0.2321\n",
      "Epoch 39/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.2543\n",
      "Epoch 00039: val_loss did not improve from 0.21095\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2535 - val_loss: 0.2222\n",
      "Epoch 40/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2330\n",
      "Epoch 00040: val_loss did not improve from 0.21095\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2330 - val_loss: 0.2362\n",
      "Epoch 41/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2078\n",
      "Epoch 00041: val_loss improved from 0.21095 to 0.20823, saving model to nn_model_0.hdf5\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2078 - val_loss: 0.2082\n",
      "Epoch 42/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2059\n",
      "Epoch 00042: val_loss did not improve from 0.20823\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2060 - val_loss: 0.2116\n",
      "Epoch 43/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2059\n",
      "Epoch 00043: val_loss did not improve from 0.20823\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2059 - val_loss: 0.2098\n",
      "Epoch 44/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2057\n",
      "Epoch 00044: val_loss did not improve from 0.20823\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2056 - val_loss: 0.2092\n",
      "Epoch 45/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2053\n",
      "Epoch 00045: val_loss improved from 0.20823 to 0.20738, saving model to nn_model_0.hdf5\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2054 - val_loss: 0.2074\n",
      "Epoch 46/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2049\n",
      "Epoch 00046: val_loss did not improve from 0.20738\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2049 - val_loss: 0.2075\n",
      "Epoch 47/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.2048\n",
      "Epoch 00047: val_loss did not improve from 0.20738\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2048 - val_loss: 0.2075\n",
      "Epoch 48/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2052\n",
      "Epoch 00048: val_loss did not improve from 0.20738\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2053 - val_loss: 0.2140\n",
      "Epoch 49/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.2061\n",
      "Epoch 00049: val_loss did not improve from 0.20738\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2061 - val_loss: 0.2080\n",
      "Epoch 50/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2049\n",
      "Epoch 00050: val_loss did not improve from 0.20738\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2050 - val_loss: 0.2093\n",
      "Epoch 51/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2058\n",
      "Epoch 00051: val_loss did not improve from 0.20738\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2057 - val_loss: 0.2083\n",
      "Epoch 52/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2068\n",
      "Epoch 00052: val_loss did not improve from 0.20738\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2068 - val_loss: 0.2088\n",
      "Epoch 53/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2025\n",
      "Epoch 00053: val_loss did not improve from 0.20738\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2025 - val_loss: 0.2083\n",
      "Epoch 54/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2021\n",
      "Epoch 00054: val_loss did not improve from 0.20738\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2021 - val_loss: 0.2074\n",
      "Epoch 55/1000\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.2020\n",
      "Epoch 00055: val_loss improved from 0.20738 to 0.20689, saving model to nn_model_0.hdf5\n",
      "168/168 [==============================] - 3s 17ms/step - loss: 0.2020 - val_loss: 0.2069\n",
      "Epoch 56/1000\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.2020\n",
      "Epoch 00056: val_loss did not improve from 0.20689\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.2020 - val_loss: 0.2075\n",
      "Epoch 57/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2019\n",
      "Epoch 00057: val_loss improved from 0.20689 to 0.20684, saving model to nn_model_0.hdf5\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.2019 - val_loss: 0.2068\n",
      "Epoch 58/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2019\n",
      "Epoch 00058: val_loss did not improve from 0.20684\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2019 - val_loss: 0.2071\n",
      "Epoch 59/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2018\n",
      "Epoch 00059: val_loss improved from 0.20684 to 0.20670, saving model to nn_model_0.hdf5\n",
      "168/168 [==============================] - 3s 17ms/step - loss: 0.2018 - val_loss: 0.2067\n",
      "Epoch 60/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2017\n",
      "Epoch 00060: val_loss did not improve from 0.20670\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.2018 - val_loss: 0.2073\n",
      "Epoch 61/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2019\n",
      "Epoch 00061: val_loss did not improve from 0.20670\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.2019 - val_loss: 0.2067\n",
      "Epoch 62/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2018\n",
      "Epoch 00062: val_loss did not improve from 0.20670\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.2018 - val_loss: 0.2072\n",
      "Epoch 63/1000\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.2019\n",
      "Epoch 00063: val_loss did not improve from 0.20670\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2019 - val_loss: 0.2073\n",
      "Epoch 64/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2018\n",
      "Epoch 00064: val_loss did not improve from 0.20670\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2018 - val_loss: 0.2077\n",
      "Epoch 65/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2020\n",
      "Epoch 00065: val_loss did not improve from 0.20670\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.2020 - val_loss: 0.2076\n",
      "Epoch 66/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.2017\n",
      "Epoch 00066: val_loss did not improve from 0.20670\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.2017 - val_loss: 0.2082\n",
      "Epoch 67/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2007\n",
      "Epoch 00067: val_loss did not improve from 0.20670\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2007 - val_loss: 0.2069\n",
      "Epoch 68/1000\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.2005\n",
      "Epoch 00068: val_loss did not improve from 0.20670\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.2005 - val_loss: 0.2070\n",
      "Epoch 69/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2005- ETA: 0s - loss: 0.2\n",
      "Epoch 00069: val_loss did not improve from 0.20670\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2005 - val_loss: 0.2070\n",
      "Epoch 70/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2005\n",
      "Epoch 00070: val_loss did not improve from 0.20670\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.2005 - val_loss: 0.2070\n",
      "Epoch 71/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2005\n",
      "Epoch 00071: val_loss did not improve from 0.20670\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2005 - val_loss: 0.2072\n",
      "Epoch 72/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167/168 [============================>.] - ETA: 0s - loss: 0.2005\n",
      "Epoch 00072: val_loss did not improve from 0.20670\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2005 - val_loss: 0.2072\n",
      "Epoch 73/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.2005\n",
      "Epoch 00073: val_loss did not improve from 0.20670\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2005 - val_loss: 0.2075\n",
      "Epoch 74/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2001\n",
      "Epoch 00074: val_loss did not improve from 0.20670\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2002 - val_loss: 0.2071\n",
      "Epoch 75/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2002\n",
      "Epoch 00075: val_loss did not improve from 0.20670\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2001 - val_loss: 0.2071\n",
      "Epoch 76/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2001\n",
      "Epoch 00076: val_loss did not improve from 0.20670\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2001 - val_loss: 0.2071\n",
      "Epoch 77/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2002- ET\n",
      "Epoch 00077: val_loss did not improve from 0.20670\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2001 - val_loss: 0.2071\n",
      "Epoch 78/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2001\n",
      "Epoch 00078: val_loss did not improve from 0.20670\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2001 - val_loss: 0.2072\n",
      "Epoch 79/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2001\n",
      "Epoch 00079: val_loss did not improve from 0.20670\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2001 - val_loss: 0.2072\n",
      "Fold 1 NN: 0.2067\n",
      "CV 2/5\n",
      "Epoch 1/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 5.7360\n",
      "Epoch 00001: val_loss improved from inf to 0.69721, saving model to nn_model_1.hdf5\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 5.7198 - val_loss: 0.6972\n",
      "Epoch 2/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.6185\n",
      "Epoch 00002: val_loss improved from 0.69721 to 0.45515, saving model to nn_model_1.hdf5\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.6196 - val_loss: 0.4552\n",
      "Epoch 3/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.5760\n",
      "Epoch 00003: val_loss did not improve from 0.45515\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.5770 - val_loss: 0.5799\n",
      "Epoch 4/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.5824\n",
      "Epoch 00004: val_loss improved from 0.45515 to 0.26495, saving model to nn_model_1.hdf5\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.5778 - val_loss: 0.2650\n",
      "Epoch 5/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.2736\n",
      "Epoch 00005: val_loss did not improve from 0.26495\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2741 - val_loss: 0.2791\n",
      "Epoch 6/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.3039\n",
      "Epoch 00006: val_loss improved from 0.26495 to 0.23936, saving model to nn_model_1.hdf5\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.3037 - val_loss: 0.2394\n",
      "Epoch 7/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.3151\n",
      "Epoch 00007: val_loss did not improve from 0.23936\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.3152 - val_loss: 0.2683\n",
      "Epoch 8/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 2.6768\n",
      "Epoch 00008: val_loss did not improve from 0.23936\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 2.6726 - val_loss: 2.3875\n",
      "Epoch 9/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.4210\n",
      "Epoch 00009: val_loss improved from 0.23936 to 0.23722, saving model to nn_model_1.hdf5\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.4194 - val_loss: 0.2372\n",
      "Epoch 10/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2395\n",
      "Epoch 00010: val_loss improved from 0.23722 to 0.23026, saving model to nn_model_1.hdf5\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2393 - val_loss: 0.2303\n",
      "Epoch 11/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2350\n",
      "Epoch 00011: val_loss did not improve from 0.23026\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2347 - val_loss: 0.2328\n",
      "Epoch 12/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2378\n",
      "Epoch 00012: val_loss did not improve from 0.23026\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2377 - val_loss: 0.2490\n",
      "Epoch 13/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2370\n",
      "Epoch 00013: val_loss did not improve from 0.23026\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2369 - val_loss: 0.2803\n",
      "Epoch 14/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2352\n",
      "Epoch 00014: val_loss improved from 0.23026 to 0.22614, saving model to nn_model_1.hdf5\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2353 - val_loss: 0.2261\n",
      "Epoch 15/1000\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.2661\n",
      "Epoch 00015: val_loss did not improve from 0.22614\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2661 - val_loss: 0.4445\n",
      "Epoch 16/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.3279- ETA: 0s - los\n",
      "Epoch 00016: val_loss did not improve from 0.22614\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.3262 - val_loss: 0.2330\n",
      "Epoch 17/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2255\n",
      "Epoch 00017: val_loss did not improve from 0.22614\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2254 - val_loss: 0.2268\n",
      "Epoch 18/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2278\n",
      "Epoch 00018: val_loss did not improve from 0.22614\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2279 - val_loss: 0.3042\n",
      "Epoch 19/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2457\n",
      "Epoch 00019: val_loss improved from 0.22614 to 0.22406, saving model to nn_model_1.hdf5\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2461 - val_loss: 0.2241\n",
      "Epoch 20/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2321\n",
      "Epoch 00020: val_loss did not improve from 0.22406\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2321 - val_loss: 0.2303\n",
      "Epoch 21/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2333\n",
      "Epoch 00021: val_loss did not improve from 0.22406\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2332 - val_loss: 0.2310\n",
      "Epoch 22/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2327\n",
      "Epoch 00022: val_loss did not improve from 0.22406\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2324 - val_loss: 0.2340\n",
      "Epoch 23/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2316\n",
      "Epoch 00023: val_loss improved from 0.22406 to 0.22013, saving model to nn_model_1.hdf5\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2315 - val_loss: 0.2201\n",
      "Epoch 24/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2334\n",
      "Epoch 00024: val_loss did not improve from 0.22013\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2333 - val_loss: 0.2226\n",
      "Epoch 25/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2318\n",
      "Epoch 00025: val_loss did not improve from 0.22013\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2318 - val_loss: 0.2300\n",
      "Epoch 26/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2299\n",
      "Epoch 00026: val_loss did not improve from 0.22013\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2299 - val_loss: 0.2210\n",
      "Epoch 27/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2352\n",
      "Epoch 00027: val_loss did not improve from 0.22013\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2348 - val_loss: 0.2416\n",
      "Epoch 28/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2500\n",
      "Epoch 00028: val_loss did not improve from 0.22013\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2499 - val_loss: 0.2273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2301\n",
      "Epoch 00029: val_loss did not improve from 0.22013\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2301 - val_loss: 0.2512\n",
      "Epoch 30/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2331\n",
      "Epoch 00030: val_loss improved from 0.22013 to 0.21932, saving model to nn_model_1.hdf5\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2332 - val_loss: 0.2193\n",
      "Epoch 31/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2390\n",
      "Epoch 00031: val_loss did not improve from 0.21932\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2389 - val_loss: 0.2453\n",
      "Epoch 32/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2408\n",
      "Epoch 00032: val_loss did not improve from 0.21932\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2407 - val_loss: 0.2222\n",
      "Epoch 33/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2248\n",
      "Epoch 00033: val_loss did not improve from 0.21932\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2251 - val_loss: 0.2256\n",
      "Epoch 34/1000\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.2918\n",
      "Epoch 00034: val_loss did not improve from 0.21932\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2918 - val_loss: 0.4582\n",
      "Epoch 35/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2412\n",
      "Epoch 00035: val_loss improved from 0.21932 to 0.21697, saving model to nn_model_1.hdf5\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2411 - val_loss: 0.2170\n",
      "Epoch 36/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2257\n",
      "Epoch 00036: val_loss did not improve from 0.21697\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2254 - val_loss: 0.2182\n",
      "Epoch 37/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2250\n",
      "Epoch 00037: val_loss improved from 0.21697 to 0.21616, saving model to nn_model_1.hdf5\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2249 - val_loss: 0.2162\n",
      "Epoch 38/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.2213- ETA: 0s - loss: 0.2\n",
      "Epoch 00038: val_loss did not improve from 0.21616\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2214 - val_loss: 0.2223\n",
      "Epoch 39/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2275\n",
      "Epoch 00039: val_loss did not improve from 0.21616\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2274 - val_loss: 0.2332\n",
      "Epoch 40/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2249\n",
      "Epoch 00040: val_loss did not improve from 0.21616\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2248 - val_loss: 0.2547\n",
      "Epoch 41/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.3039\n",
      "Epoch 00041: val_loss did not improve from 0.21616\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.3041 - val_loss: 0.2963\n",
      "Epoch 42/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.3155\n",
      "Epoch 00042: val_loss did not improve from 0.21616\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.3161 - val_loss: 0.8585\n",
      "Epoch 43/1000\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.2945\n",
      "Epoch 00043: val_loss did not improve from 0.21616\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2945 - val_loss: 0.2186\n",
      "Epoch 44/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2227\n",
      "Epoch 00044: val_loss did not improve from 0.21616\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2227 - val_loss: 0.2363\n",
      "Epoch 45/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.2075\n",
      "Epoch 00045: val_loss improved from 0.21616 to 0.21589, saving model to nn_model_1.hdf5\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2075 - val_loss: 0.2159\n",
      "Epoch 46/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2059\n",
      "Epoch 00046: val_loss did not improve from 0.21589\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2059 - val_loss: 0.2161\n",
      "Epoch 47/1000\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.2047\n",
      "Epoch 00047: val_loss improved from 0.21589 to 0.21424, saving model to nn_model_1.hdf5\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2047 - val_loss: 0.2142\n",
      "Epoch 48/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2054\n",
      "Epoch 00048: val_loss did not improve from 0.21424\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2053 - val_loss: 0.2151\n",
      "Epoch 49/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2049\n",
      "Epoch 00049: val_loss did not improve from 0.21424\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2049 - val_loss: 0.2149\n",
      "Epoch 50/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2048\n",
      "Epoch 00050: val_loss did not improve from 0.21424\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2048 - val_loss: 0.2154\n",
      "Epoch 51/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2045\n",
      "Epoch 00051: val_loss improved from 0.21424 to 0.21371, saving model to nn_model_1.hdf5\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2045 - val_loss: 0.2137\n",
      "Epoch 52/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2039\n",
      "Epoch 00052: val_loss improved from 0.21371 to 0.21273, saving model to nn_model_1.hdf5\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2039 - val_loss: 0.2127\n",
      "Epoch 53/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2053\n",
      "Epoch 00053: val_loss did not improve from 0.21273\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2053 - val_loss: 0.2177\n",
      "Epoch 54/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.2048\n",
      "Epoch 00054: val_loss did not improve from 0.21273\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2047 - val_loss: 0.2139\n",
      "Epoch 55/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2041\n",
      "Epoch 00055: val_loss did not improve from 0.21273\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2041 - val_loss: 0.2139\n",
      "Epoch 56/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2045\n",
      "Epoch 00056: val_loss did not improve from 0.21273\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2045 - val_loss: 0.2150\n",
      "Epoch 57/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.2049\n",
      "Epoch 00057: val_loss did not improve from 0.21273\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2048 - val_loss: 0.2151\n",
      "Epoch 58/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2042- ETA: 0s - loss: 0\n",
      "Epoch 00058: val_loss did not improve from 0.21273\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2041 - val_loss: 0.2171\n",
      "Epoch 59/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2040\n",
      "Epoch 00059: val_loss did not improve from 0.21273\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2040 - val_loss: 0.2131\n",
      "Epoch 60/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.2015\n",
      "Epoch 00060: val_loss did not improve from 0.21273\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2015 - val_loss: 0.2139\n",
      "Epoch 61/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2011\n",
      "Epoch 00061: val_loss did not improve from 0.21273\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2011 - val_loss: 0.2132\n",
      "Epoch 62/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2009\n",
      "Epoch 00062: val_loss did not improve from 0.21273\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2009 - val_loss: 0.2128\n",
      "Epoch 63/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2009\n",
      "Epoch 00063: val_loss did not improve from 0.21273\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2009 - val_loss: 0.2151\n",
      "Epoch 64/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.2011\n",
      "Epoch 00064: val_loss did not improve from 0.21273\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2010 - val_loss: 0.2140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2012\n",
      "Epoch 00065: val_loss improved from 0.21273 to 0.21259, saving model to nn_model_1.hdf5\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2011 - val_loss: 0.2126\n",
      "Epoch 66/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2007\n",
      "Epoch 00066: val_loss did not improve from 0.21259\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2006 - val_loss: 0.2144\n",
      "Epoch 67/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2006\n",
      "Epoch 00067: val_loss did not improve from 0.21259\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2006 - val_loss: 0.2135\n",
      "Epoch 68/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2008\n",
      "Epoch 00068: val_loss did not improve from 0.21259\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2008 - val_loss: 0.2146\n",
      "Epoch 69/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2007\n",
      "Epoch 00069: val_loss did not improve from 0.21259\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2007 - val_loss: 0.2127\n",
      "Epoch 70/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.2008\n",
      "Epoch 00070: val_loss did not improve from 0.21259\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2008 - val_loss: 0.2127\n",
      "Epoch 71/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2005\n",
      "Epoch 00071: val_loss did not improve from 0.21259\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2005 - val_loss: 0.2140\n",
      "Epoch 72/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2008- ETA: \n",
      "Epoch 00072: val_loss improved from 0.21259 to 0.21237, saving model to nn_model_1.hdf5\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2008 - val_loss: 0.2124\n",
      "Epoch 73/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2006- ETA: 0s \n",
      "Epoch 00073: val_loss did not improve from 0.21237\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2006 - val_loss: 0.2137\n",
      "Epoch 74/1000\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.2002\n",
      "Epoch 00074: val_loss did not improve from 0.21237\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2002 - val_loss: 0.2132\n",
      "Epoch 75/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2002\n",
      "Epoch 00075: val_loss did not improve from 0.21237\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2002 - val_loss: 0.2141\n",
      "Epoch 76/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2006\n",
      "Epoch 00076: val_loss did not improve from 0.21237\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2006 - val_loss: 0.2134\n",
      "Epoch 77/1000\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.2006\n",
      "Epoch 00077: val_loss did not improve from 0.21237\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2006 - val_loss: 0.2136\n",
      "Epoch 78/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.2004\n",
      "Epoch 00078: val_loss did not improve from 0.21237\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2003 - val_loss: 0.2134\n",
      "Epoch 79/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2000\n",
      "Epoch 00079: val_loss did not improve from 0.21237\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2000 - val_loss: 0.2162\n",
      "Epoch 80/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.1992\n",
      "Epoch 00080: val_loss did not improve from 0.21237\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.1992 - val_loss: 0.2129\n",
      "Epoch 81/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.1990\n",
      "Epoch 00081: val_loss did not improve from 0.21237\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.1991 - val_loss: 0.2131\n",
      "Epoch 82/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.1990\n",
      "Epoch 00082: val_loss did not improve from 0.21237\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.1990 - val_loss: 0.2143\n",
      "Epoch 83/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.1990\n",
      "Epoch 00083: val_loss did not improve from 0.21237\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.1990 - val_loss: 0.2138\n",
      "Epoch 84/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.1989\n",
      "Epoch 00084: val_loss did not improve from 0.21237\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.1989 - val_loss: 0.2131\n",
      "Epoch 85/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.1989\n",
      "Epoch 00085: val_loss did not improve from 0.21237\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.1989 - val_loss: 0.2137\n",
      "Epoch 86/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.1989\n",
      "Epoch 00086: val_loss did not improve from 0.21237\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.1989 - val_loss: 0.2140\n",
      "Epoch 87/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.1987\n",
      "Epoch 00087: val_loss did not improve from 0.21237\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.1987 - val_loss: 0.2133\n",
      "Epoch 88/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.1987\n",
      "Epoch 00088: val_loss did not improve from 0.21237\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.1987 - val_loss: 0.2132\n",
      "Epoch 89/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.1987\n",
      "Epoch 00089: val_loss did not improve from 0.21237\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.1987 - val_loss: 0.2136\n",
      "Epoch 90/1000\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.1987\n",
      "Epoch 00090: val_loss did not improve from 0.21237\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.1987 - val_loss: 0.2135\n",
      "Epoch 91/1000\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.1987\n",
      "Epoch 00091: val_loss did not improve from 0.21237\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.1987 - val_loss: 0.2136\n",
      "Epoch 92/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.1987\n",
      "Epoch 00092: val_loss did not improve from 0.21237\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.1987 - val_loss: 0.2139\n",
      "Fold 2 NN: 0.21237\n",
      "CV 3/5\n",
      "Epoch 1/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 9.0968\n",
      "Epoch 00001: val_loss improved from inf to 0.61730, saving model to nn_model_2.hdf5\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 9.0698 - val_loss: 0.6173\n",
      "Epoch 2/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.8469\n",
      "Epoch 00002: val_loss did not improve from 0.61730\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.8475 - val_loss: 0.7286\n",
      "Epoch 3/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.6616\n",
      "Epoch 00003: val_loss did not improve from 0.61730\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.6613 - val_loss: 0.7063\n",
      "Epoch 4/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.6224\n",
      "Epoch 00004: val_loss did not improve from 0.61730\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.6212 - val_loss: 0.7289\n",
      "Epoch 5/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.5994\n",
      "Epoch 00005: val_loss improved from 0.61730 to 0.57587, saving model to nn_model_2.hdf5\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.5991 - val_loss: 0.5759\n",
      "Epoch 6/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.6027- \n",
      "Epoch 00006: val_loss improved from 0.57587 to 0.56607, saving model to nn_model_2.hdf5\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.6018 - val_loss: 0.5661\n",
      "Epoch 7/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.6253\n",
      "Epoch 00007: val_loss improved from 0.56607 to 0.30712, saving model to nn_model_2.hdf5\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.6238 - val_loss: 0.3071\n",
      "Epoch 8/1000\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.2724\n",
      "Epoch 00008: val_loss improved from 0.30712 to 0.27180, saving model to nn_model_2.hdf5\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.2724 - val_loss: 0.2718\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2746\n",
      "Epoch 00009: val_loss improved from 0.27180 to 0.23480, saving model to nn_model_2.hdf5\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2750 - val_loss: 0.2348\n",
      "Epoch 10/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2957\n",
      "Epoch 00010: val_loss did not improve from 0.23480\n",
      "168/168 [==============================] - 3s 17ms/step - loss: 0.2954 - val_loss: 0.2562\n",
      "Epoch 11/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.2888\n",
      "Epoch 00011: val_loss did not improve from 0.23480\n",
      "168/168 [==============================] - 3s 17ms/step - loss: 0.2893 - val_loss: 0.2721\n",
      "Epoch 12/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2983\n",
      "Epoch 00012: val_loss did not improve from 0.23480\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2978 - val_loss: 0.2580\n",
      "Epoch 13/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 2.7009\n",
      "Epoch 00013: val_loss did not improve from 0.23480\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 2.7925 - val_loss: 16.3749\n",
      "Epoch 14/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 1.6865\n",
      "Epoch 00014: val_loss did not improve from 0.23480\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 1.6754 - val_loss: 0.6011\n",
      "Epoch 15/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.4360\n",
      "Epoch 00015: val_loss did not improve from 0.23480\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.4322 - val_loss: 0.2462\n",
      "Epoch 16/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2502\n",
      "Epoch 00016: val_loss did not improve from 0.23480\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2502 - val_loss: 0.2645\n",
      "Epoch 17/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2246\n",
      "Epoch 00017: val_loss improved from 0.23480 to 0.22117, saving model to nn_model_2.hdf5\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2246 - val_loss: 0.2212\n",
      "Epoch 18/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.2222\n",
      "Epoch 00018: val_loss did not improve from 0.22117\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2221 - val_loss: 0.2234\n",
      "Epoch 19/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2217\n",
      "Epoch 00019: val_loss did not improve from 0.22117\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2217 - val_loss: 0.2215\n",
      "Epoch 20/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.2201\n",
      "Epoch 00020: val_loss improved from 0.22117 to 0.21803, saving model to nn_model_2.hdf5\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2200 - val_loss: 0.2180\n",
      "Epoch 21/1000\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.2193\n",
      "Epoch 00021: val_loss did not improve from 0.21803\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2193 - val_loss: 0.2229\n",
      "Epoch 22/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2183\n",
      "Epoch 00022: val_loss improved from 0.21803 to 0.21790, saving model to nn_model_2.hdf5\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.2183 - val_loss: 0.2179\n",
      "Epoch 23/1000\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.2181- ETA\n",
      "Epoch 00023: val_loss improved from 0.21790 to 0.21735, saving model to nn_model_2.hdf5\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.2181 - val_loss: 0.2174\n",
      "Epoch 24/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2192\n",
      "Epoch 00024: val_loss did not improve from 0.21735\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2191 - val_loss: 0.2213\n",
      "Epoch 25/1000\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.2198\n",
      "Epoch 00025: val_loss did not improve from 0.21735\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2198 - val_loss: 0.2196\n",
      "Epoch 26/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2185\n",
      "Epoch 00026: val_loss did not improve from 0.21735\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2185 - val_loss: 0.2244\n",
      "Epoch 27/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2171\n",
      "Epoch 00027: val_loss improved from 0.21735 to 0.21610, saving model to nn_model_2.hdf5\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2172 - val_loss: 0.2161\n",
      "Epoch 28/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2177\n",
      "Epoch 00028: val_loss did not improve from 0.21610\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2177 - val_loss: 0.2185\n",
      "Epoch 29/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2185\n",
      "Epoch 00029: val_loss did not improve from 0.21610\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2185 - val_loss: 0.2249\n",
      "Epoch 30/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.2162\n",
      "Epoch 00030: val_loss did not improve from 0.21610\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2163 - val_loss: 0.2220\n",
      "Epoch 31/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2166\n",
      "Epoch 00031: val_loss did not improve from 0.21610\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2166 - val_loss: 0.2213\n",
      "Epoch 32/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2175\n",
      "Epoch 00032: val_loss did not improve from 0.21610\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2175 - val_loss: 0.2197\n",
      "Epoch 33/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2163\n",
      "Epoch 00033: val_loss improved from 0.21610 to 0.21457, saving model to nn_model_2.hdf5\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2162 - val_loss: 0.2146\n",
      "Epoch 34/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2168- ETA: 0s\n",
      "Epoch 00034: val_loss did not improve from 0.21457\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.2168 - val_loss: 0.2249\n",
      "Epoch 35/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.2171\n",
      "Epoch 00035: val_loss did not improve from 0.21457\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2172 - val_loss: 0.2219\n",
      "Epoch 36/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2163\n",
      "Epoch 00036: val_loss did not improve from 0.21457\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2163 - val_loss: 0.2155\n",
      "Epoch 37/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2174\n",
      "Epoch 00037: val_loss did not improve from 0.21457\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2174 - val_loss: 0.2184\n",
      "Epoch 38/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2180\n",
      "Epoch 00038: val_loss did not improve from 0.21457\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2180 - val_loss: 0.2201\n",
      "Epoch 39/1000\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.2185\n",
      "Epoch 00039: val_loss improved from 0.21457 to 0.21380, saving model to nn_model_2.hdf5\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2185 - val_loss: 0.2138\n",
      "Epoch 40/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2310\n",
      "Epoch 00040: val_loss did not improve from 0.21380\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2309 - val_loss: 0.2227\n",
      "Epoch 41/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.2149\n",
      "Epoch 00041: val_loss did not improve from 0.21380\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2150 - val_loss: 0.2172\n",
      "Epoch 42/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2154\n",
      "Epoch 00042: val_loss improved from 0.21380 to 0.21334, saving model to nn_model_2.hdf5\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2154 - val_loss: 0.2133\n",
      "Epoch 43/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2134\n",
      "Epoch 00043: val_loss did not improve from 0.21334\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2133 - val_loss: 0.2190\n",
      "Epoch 44/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.2165\n",
      "Epoch 00044: val_loss did not improve from 0.21334\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2163 - val_loss: 0.2145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2221\n",
      "Epoch 00045: val_loss did not improve from 0.21334\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2222 - val_loss: 0.2208\n",
      "Epoch 46/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.2147\n",
      "Epoch 00046: val_loss improved from 0.21334 to 0.21286, saving model to nn_model_2.hdf5\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2147 - val_loss: 0.2129\n",
      "Epoch 47/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.2131\n",
      "Epoch 00047: val_loss did not improve from 0.21286\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2131 - val_loss: 0.2174\n",
      "Epoch 48/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2215- ETA:  - ETA: 0s - loss: 0\n",
      "Epoch 00048: val_loss did not improve from 0.21286\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2216 - val_loss: 0.2144\n",
      "Epoch 49/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2156\n",
      "Epoch 00049: val_loss did not improve from 0.21286\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2156 - val_loss: 0.2283\n",
      "Epoch 50/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2196\n",
      "Epoch 00050: val_loss did not improve from 0.21286\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2196 - val_loss: 0.2158\n",
      "Epoch 51/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2139\n",
      "Epoch 00051: val_loss did not improve from 0.21286\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2138 - val_loss: 0.2150\n",
      "Epoch 52/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2119\n",
      "Epoch 00052: val_loss did not improve from 0.21286\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2120 - val_loss: 0.2182\n",
      "Epoch 53/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2123\n",
      "Epoch 00053: val_loss did not improve from 0.21286\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2122 - val_loss: 0.2141\n",
      "Epoch 54/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2059\n",
      "Epoch 00054: val_loss improved from 0.21286 to 0.21154, saving model to nn_model_2.hdf5\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2058 - val_loss: 0.2115\n",
      "Epoch 55/1000\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.2055\n",
      "Epoch 00055: val_loss did not improve from 0.21154\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2055 - val_loss: 0.2123\n",
      "Epoch 56/1000\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.2054\n",
      "Epoch 00056: val_loss did not improve from 0.21154\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2054 - val_loss: 0.2132\n",
      "Epoch 57/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2052\n",
      "Epoch 00057: val_loss improved from 0.21154 to 0.21107, saving model to nn_model_2.hdf5\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2051 - val_loss: 0.2111\n",
      "Epoch 58/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2053\n",
      "Epoch 00058: val_loss did not improve from 0.21107\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2053 - val_loss: 0.2111\n",
      "Epoch 59/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2049\n",
      "Epoch 00059: val_loss did not improve from 0.21107\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2050 - val_loss: 0.2113\n",
      "Epoch 60/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2050\n",
      "Epoch 00060: val_loss did not improve from 0.21107\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2051 - val_loss: 0.2123\n",
      "Epoch 61/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2051\n",
      "Epoch 00061: val_loss did not improve from 0.21107\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2051 - val_loss: 0.2133\n",
      "Epoch 62/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2046\n",
      "Epoch 00062: val_loss did not improve from 0.21107\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2047 - val_loss: 0.2118\n",
      "Epoch 63/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.2053\n",
      "Epoch 00063: val_loss did not improve from 0.21107\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2053 - val_loss: 0.2139\n",
      "Epoch 64/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.2054\n",
      "Epoch 00064: val_loss did not improve from 0.21107\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2055 - val_loss: 0.2127\n",
      "Epoch 65/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2035\n",
      "Epoch 00065: val_loss did not improve from 0.21107\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2035 - val_loss: 0.2115\n",
      "Epoch 66/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2034\n",
      "Epoch 00066: val_loss did not improve from 0.21107\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2034 - val_loss: 0.2118\n",
      "Epoch 67/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2033\n",
      "Epoch 00067: val_loss did not improve from 0.21107\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2033 - val_loss: 0.2115\n",
      "Epoch 68/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2034\n",
      "Epoch 00068: val_loss did not improve from 0.21107\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2034 - val_loss: 0.2118\n",
      "Epoch 69/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2032\n",
      "Epoch 00069: val_loss did not improve from 0.21107\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2033 - val_loss: 0.2114\n",
      "Epoch 70/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2032\n",
      "Epoch 00070: val_loss did not improve from 0.21107\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2033 - val_loss: 0.2121\n",
      "Epoch 71/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2033\n",
      "Epoch 00071: val_loss did not improve from 0.21107\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2033 - val_loss: 0.2118\n",
      "Epoch 72/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2030- ETA: 0s - lo\n",
      "Epoch 00072: val_loss did not improve from 0.21107\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2030 - val_loss: 0.2116\n",
      "Epoch 73/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2029\n",
      "Epoch 00073: val_loss did not improve from 0.21107\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2029 - val_loss: 0.2115\n",
      "Epoch 74/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2029\n",
      "Epoch 00074: val_loss did not improve from 0.21107\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2029 - val_loss: 0.2116\n",
      "Epoch 75/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2029\n",
      "Epoch 00075: val_loss did not improve from 0.21107\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2029 - val_loss: 0.2117\n",
      "Epoch 76/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2029\n",
      "Epoch 00076: val_loss did not improve from 0.21107\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2029 - val_loss: 0.2117\n",
      "Epoch 77/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2029\n",
      "Epoch 00077: val_loss did not improve from 0.21107\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2029 - val_loss: 0.2117\n",
      "Fold 3 NN: 0.21107\n",
      "CV 4/5\n",
      "Epoch 1/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 8.4299\n",
      "Epoch 00001: val_loss improved from inf to 0.73496, saving model to nn_model_3.hdf5\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 8.3143 - val_loss: 0.7350\n",
      "Epoch 2/1000\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.6686\n",
      "Epoch 00002: val_loss improved from 0.73496 to 0.55172, saving model to nn_model_3.hdf5\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.6686 - val_loss: 0.5517\n",
      "Epoch 3/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.5730\n",
      "Epoch 00003: val_loss improved from 0.55172 to 0.48515, saving model to nn_model_3.hdf5\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.5736 - val_loss: 0.4852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.5479\n",
      "Epoch 00004: val_loss did not improve from 0.48515\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.5476 - val_loss: 0.5973\n",
      "Epoch 5/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.5154\n",
      "Epoch 00005: val_loss did not improve from 0.48515\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.5150 - val_loss: 0.5780\n",
      "Epoch 6/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.5098\n",
      "Epoch 00006: val_loss did not improve from 0.48515\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.5073 - val_loss: 0.5188\n",
      "Epoch 7/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.5335\n",
      "Epoch 00007: val_loss did not improve from 0.48515\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.5296 - val_loss: 0.5038\n",
      "Epoch 8/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.4267\n",
      "Epoch 00008: val_loss improved from 0.48515 to 0.39152, saving model to nn_model_3.hdf5\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.4266 - val_loss: 0.3915\n",
      "Epoch 9/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.3791\n",
      "Epoch 00009: val_loss improved from 0.39152 to 0.39055, saving model to nn_model_3.hdf5\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.3783 - val_loss: 0.3905\n",
      "Epoch 10/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.7006\n",
      "Epoch 00010: val_loss improved from 0.39055 to 0.36198, saving model to nn_model_3.hdf5\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.6970 - val_loss: 0.3620\n",
      "Epoch 11/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2542\n",
      "Epoch 00011: val_loss improved from 0.36198 to 0.22814, saving model to nn_model_3.hdf5\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2538 - val_loss: 0.2281\n",
      "Epoch 12/1000\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.2310\n",
      "Epoch 00012: val_loss did not improve from 0.22814\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2310 - val_loss: 0.2307\n",
      "Epoch 13/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2242\n",
      "Epoch 00013: val_loss did not improve from 0.22814\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2241 - val_loss: 0.2301\n",
      "Epoch 14/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2253\n",
      "Epoch 00014: val_loss did not improve from 0.22814\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2257 - val_loss: 0.2503\n",
      "Epoch 15/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2293\n",
      "Epoch 00015: val_loss improved from 0.22814 to 0.22684, saving model to nn_model_3.hdf5\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2294 - val_loss: 0.2268\n",
      "Epoch 16/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.2439\n",
      "Epoch 00016: val_loss did not improve from 0.22684\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2441 - val_loss: 0.2496\n",
      "Epoch 17/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.8047\n",
      "Epoch 00017: val_loss did not improve from 0.22684\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.8080 - val_loss: 0.8623\n",
      "Epoch 18/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2641\n",
      "Epoch 00018: val_loss improved from 0.22684 to 0.22459, saving model to nn_model_3.hdf5\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2633 - val_loss: 0.2246\n",
      "Epoch 19/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.2279\n",
      "Epoch 00019: val_loss did not improve from 0.22459\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2276 - val_loss: 0.2253\n",
      "Epoch 20/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2231\n",
      "Epoch 00020: val_loss did not improve from 0.22459\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2231 - val_loss: 0.2251\n",
      "Epoch 21/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.2197\n",
      "Epoch 00021: val_loss did not improve from 0.22459\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2197 - val_loss: 0.2384\n",
      "Epoch 22/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2248\n",
      "Epoch 00022: val_loss did not improve from 0.22459\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2249 - val_loss: 0.2680\n",
      "Epoch 23/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2296\n",
      "Epoch 00023: val_loss did not improve from 0.22459\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2298 - val_loss: 0.2651\n",
      "Epoch 24/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2305\n",
      "Epoch 00024: val_loss did not improve from 0.22459\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2303 - val_loss: 0.2328\n",
      "Epoch 25/1000\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.2223\n",
      "Epoch 00025: val_loss did not improve from 0.22459\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2223 - val_loss: 0.2549\n",
      "Epoch 26/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2087\n",
      "Epoch 00026: val_loss improved from 0.22459 to 0.21443, saving model to nn_model_3.hdf5\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2087 - val_loss: 0.2144\n",
      "Epoch 27/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2061\n",
      "Epoch 00027: val_loss did not improve from 0.21443\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2060 - val_loss: 0.2154\n",
      "Epoch 28/1000\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.2062\n",
      "Epoch 00028: val_loss did not improve from 0.21443\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2062 - val_loss: 0.2152\n",
      "Epoch 29/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2060\n",
      "Epoch 00029: val_loss improved from 0.21443 to 0.21425, saving model to nn_model_3.hdf5\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2060 - val_loss: 0.2142\n",
      "Epoch 30/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2056\n",
      "Epoch 00030: val_loss did not improve from 0.21425\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2056 - val_loss: 0.2152\n",
      "Epoch 31/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2056\n",
      "Epoch 00031: val_loss improved from 0.21425 to 0.21362, saving model to nn_model_3.hdf5\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2056 - val_loss: 0.2136\n",
      "Epoch 32/1000\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.2054\n",
      "Epoch 00032: val_loss did not improve from 0.21362\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2054 - val_loss: 0.2148\n",
      "Epoch 33/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.2056- ETA: 0s - loss: \n",
      "Epoch 00033: val_loss did not improve from 0.21362\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2056 - val_loss: 0.2160\n",
      "Epoch 34/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.2061\n",
      "Epoch 00034: val_loss did not improve from 0.21362\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2062 - val_loss: 0.2155\n",
      "Epoch 35/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.2053\n",
      "Epoch 00035: val_loss did not improve from 0.21362\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2055 - val_loss: 0.2154\n",
      "Epoch 36/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2057\n",
      "Epoch 00036: val_loss did not improve from 0.21362\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2058 - val_loss: 0.2163\n",
      "Epoch 37/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2064\n",
      "Epoch 00037: val_loss did not improve from 0.21362\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2064 - val_loss: 0.2143\n",
      "Epoch 38/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.2060\n",
      "Epoch 00038: val_loss did not improve from 0.21362\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2060 - val_loss: 0.2156\n",
      "Epoch 39/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2033\n",
      "Epoch 00039: val_loss improved from 0.21362 to 0.21356, saving model to nn_model_3.hdf5\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2033 - val_loss: 0.2136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2029\n",
      "Epoch 00040: val_loss improved from 0.21356 to 0.21284, saving model to nn_model_3.hdf5\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2029 - val_loss: 0.2128\n",
      "Epoch 41/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2028\n",
      "Epoch 00041: val_loss did not improve from 0.21284\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2029 - val_loss: 0.2135\n",
      "Epoch 42/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2028\n",
      "Epoch 00042: val_loss did not improve from 0.21284\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2028 - val_loss: 0.2142\n",
      "Epoch 43/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2029\n",
      "Epoch 00043: val_loss did not improve from 0.21284\n",
      "168/168 [==============================] - 3s 18ms/step - loss: 0.2029 - val_loss: 0.2137\n",
      "Epoch 44/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2027\n",
      "Epoch 00044: val_loss did not improve from 0.21284\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.2028 - val_loss: 0.2147\n",
      "Epoch 45/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2028\n",
      "Epoch 00045: val_loss did not improve from 0.21284\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2028 - val_loss: 0.2139\n",
      "Epoch 46/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2027\n",
      "Epoch 00046: val_loss did not improve from 0.21284\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2026 - val_loss: 0.2145\n",
      "Epoch 47/1000\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.2028\n",
      "Epoch 00047: val_loss did not improve from 0.21284\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2028 - val_loss: 0.2161\n",
      "Epoch 48/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2021\n",
      "Epoch 00048: val_loss did not improve from 0.21284\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2020 - val_loss: 0.2134\n",
      "Epoch 49/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2019\n",
      "Epoch 00049: val_loss did not improve from 0.21284\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2019 - val_loss: 0.2131\n",
      "Epoch 50/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2019\n",
      "Epoch 00050: val_loss did not improve from 0.21284\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.2018 - val_loss: 0.2134\n",
      "Epoch 51/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2018\n",
      "Epoch 00051: val_loss did not improve from 0.21284\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2018 - val_loss: 0.2132\n",
      "Epoch 52/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2019\n",
      "Epoch 00052: val_loss did not improve from 0.21284\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2018 - val_loss: 0.2132\n",
      "Epoch 53/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2018\n",
      "Epoch 00053: val_loss did not improve from 0.21284\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2018 - val_loss: 0.2133\n",
      "Epoch 54/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2018\n",
      "Epoch 00054: val_loss did not improve from 0.21284\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2018 - val_loss: 0.2136\n",
      "Epoch 55/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2016\n",
      "Epoch 00055: val_loss did not improve from 0.21284\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2016 - val_loss: 0.2133\n",
      "Epoch 56/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.2015\n",
      "Epoch 00056: val_loss did not improve from 0.21284\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.2016 - val_loss: 0.2132\n",
      "Epoch 57/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2015\n",
      "Epoch 00057: val_loss did not improve from 0.21284\n",
      "168/168 [==============================] - 3s 17ms/step - loss: 0.2015 - val_loss: 0.2132\n",
      "Epoch 58/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2016\n",
      "Epoch 00058: val_loss did not improve from 0.21284\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.2015 - val_loss: 0.2131\n",
      "Epoch 59/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2015\n",
      "Epoch 00059: val_loss did not improve from 0.21284\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2015 - val_loss: 0.2134\n",
      "Epoch 60/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2015\n",
      "Epoch 00060: val_loss did not improve from 0.21284\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2015 - val_loss: 0.2133\n",
      "Fold 4 NN: 0.21284\n",
      "CV 5/5\n",
      "Epoch 1/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 10.0093\n",
      "Epoch 00001: val_loss improved from inf to 1.10995, saving model to nn_model_4.hdf5\n",
      "168/168 [==============================] - 3s 17ms/step - loss: 9.9796 - val_loss: 1.1099\n",
      "Epoch 2/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 1.2747\n",
      "Epoch 00002: val_loss improved from 1.10995 to 0.96685, saving model to nn_model_4.hdf5\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 1.2733 - val_loss: 0.9668\n",
      "Epoch 3/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.6996\n",
      "Epoch 00003: val_loss improved from 0.96685 to 0.69044, saving model to nn_model_4.hdf5\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.6985 - val_loss: 0.6904\n",
      "Epoch 4/1000\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.6177\n",
      "Epoch 00004: val_loss improved from 0.69044 to 0.60819, saving model to nn_model_4.hdf5\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.6177 - val_loss: 0.6082\n",
      "Epoch 5/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.6663\n",
      "Epoch 00005: val_loss did not improve from 0.60819\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.6653 - val_loss: 0.6299\n",
      "Epoch 6/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.6724\n",
      "Epoch 00006: val_loss improved from 0.60819 to 0.50759, saving model to nn_model_4.hdf5\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.6714 - val_loss: 0.5076\n",
      "Epoch 7/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.5605\n",
      "Epoch 00007: val_loss did not improve from 0.50759\n",
      "168/168 [==============================] - 3s 17ms/step - loss: 0.5606 - val_loss: 0.6013\n",
      "Epoch 8/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.5395\n",
      "Epoch 00008: val_loss did not improve from 0.50759\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.5391 - val_loss: 0.5592\n",
      "Epoch 9/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 1.0104\n",
      "Epoch 00009: val_loss did not improve from 0.50759\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 1.0063 - val_loss: 1.1221\n",
      "Epoch 10/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.5168- \n",
      "Epoch 00010: val_loss improved from 0.50759 to 0.46508, saving model to nn_model_4.hdf5\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.5170 - val_loss: 0.4651\n",
      "Epoch 11/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.4456\n",
      "Epoch 00011: val_loss improved from 0.46508 to 0.40687, saving model to nn_model_4.hdf5\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.4467 - val_loss: 0.4069\n",
      "Epoch 12/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.4849\n",
      "Epoch 00012: val_loss improved from 0.40687 to 0.40359, saving model to nn_model_4.hdf5\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.4845 - val_loss: 0.4036\n",
      "Epoch 13/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.4407\n",
      "Epoch 00013: val_loss did not improve from 0.40359\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.4405 - val_loss: 0.4739\n",
      "Epoch 14/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.6466\n",
      "Epoch 00014: val_loss improved from 0.40359 to 0.23012, saving model to nn_model_4.hdf5\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.6454 - val_loss: 0.2301\n",
      "Epoch 15/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2355\n",
      "Epoch 00015: val_loss did not improve from 0.23012\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2359 - val_loss: 0.2622\n",
      "Epoch 16/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2617\n",
      "Epoch 00016: val_loss did not improve from 0.23012\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2616 - val_loss: 0.2438\n",
      "Epoch 17/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2452\n",
      "Epoch 00017: val_loss did not improve from 0.23012\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2451 - val_loss: 0.2594\n",
      "Epoch 18/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2572\n",
      "Epoch 00018: val_loss did not improve from 0.23012\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2570 - val_loss: 0.2371\n",
      "Epoch 19/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2768\n",
      "Epoch 00019: val_loss did not improve from 0.23012\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2771 - val_loss: 0.5198\n",
      "Epoch 20/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2792\n",
      "Epoch 00020: val_loss did not improve from 0.23012\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2798 - val_loss: 0.3169\n",
      "Epoch 21/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 1.7492\n",
      "Epoch 00021: val_loss did not improve from 0.23012\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 1.7267 - val_loss: 0.2454\n",
      "Epoch 22/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2261\n",
      "Epoch 00022: val_loss improved from 0.23012 to 0.22516, saving model to nn_model_4.hdf5\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2260 - val_loss: 0.2252\n",
      "Epoch 23/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.2209\n",
      "Epoch 00023: val_loss improved from 0.22516 to 0.22327, saving model to nn_model_4.hdf5\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2208 - val_loss: 0.2233\n",
      "Epoch 24/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2190\n",
      "Epoch 00024: val_loss improved from 0.22327 to 0.22219, saving model to nn_model_4.hdf5\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2190 - val_loss: 0.2222\n",
      "Epoch 25/1000\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.2186\n",
      "Epoch 00025: val_loss improved from 0.22219 to 0.22055, saving model to nn_model_4.hdf5\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2186 - val_loss: 0.2205\n",
      "Epoch 26/1000\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.2169\n",
      "Epoch 00026: val_loss did not improve from 0.22055\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2169 - val_loss: 0.2224\n",
      "Epoch 27/1000\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.2150\n",
      "Epoch 00027: val_loss did not improve from 0.22055\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2150 - val_loss: 0.2317\n",
      "Epoch 28/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2153\n",
      "Epoch 00028: val_loss did not improve from 0.22055\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2154 - val_loss: 0.2217\n",
      "Epoch 29/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2144\n",
      "Epoch 00029: val_loss improved from 0.22055 to 0.21811, saving model to nn_model_4.hdf5\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2144 - val_loss: 0.2181\n",
      "Epoch 30/1000\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.2139\n",
      "Epoch 00030: val_loss did not improve from 0.21811\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2139 - val_loss: 0.2213\n",
      "Epoch 31/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.2130\n",
      "Epoch 00031: val_loss did not improve from 0.21811\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2131 - val_loss: 0.2266\n",
      "Epoch 32/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.2151\n",
      "Epoch 00032: val_loss improved from 0.21811 to 0.21708, saving model to nn_model_4.hdf5\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2149 - val_loss: 0.2171\n",
      "Epoch 33/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2127\n",
      "Epoch 00033: val_loss did not improve from 0.21708\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2127 - val_loss: 0.2193\n",
      "Epoch 34/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2117\n",
      "Epoch 00034: val_loss did not improve from 0.21708\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2118 - val_loss: 0.2238\n",
      "Epoch 35/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2128\n",
      "Epoch 00035: val_loss did not improve from 0.21708\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2128 - val_loss: 0.2223\n",
      "Epoch 36/1000\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.2121- ETA: 0s - loss:\n",
      "Epoch 00036: val_loss did not improve from 0.21708\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.2121 - val_loss: 0.2183\n",
      "Epoch 37/1000\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.2115\n",
      "Epoch 00037: val_loss did not improve from 0.21708\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2115 - val_loss: 0.2259\n",
      "Epoch 38/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.2120\n",
      "Epoch 00038: val_loss did not improve from 0.21708\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2120 - val_loss: 0.2188\n",
      "Epoch 39/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2112\n",
      "Epoch 00039: val_loss improved from 0.21708 to 0.21556, saving model to nn_model_4.hdf5\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2112 - val_loss: 0.2156\n",
      "Epoch 40/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2117\n",
      "Epoch 00040: val_loss did not improve from 0.21556\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2117 - val_loss: 0.2267\n",
      "Epoch 41/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2147\n",
      "Epoch 00041: val_loss did not improve from 0.21556\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2147 - val_loss: 0.2174\n",
      "Epoch 42/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.2101\n",
      "Epoch 00042: val_loss did not improve from 0.21556\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2105 - val_loss: 0.2432\n",
      "Epoch 43/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.2165\n",
      "Epoch 00043: val_loss did not improve from 0.21556\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2165 - val_loss: 0.2171\n",
      "Epoch 44/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.2094\n",
      "Epoch 00044: val_loss did not improve from 0.21556\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2094 - val_loss: 0.2180\n",
      "Epoch 45/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2114\n",
      "Epoch 00045: val_loss did not improve from 0.21556\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2114 - val_loss: 0.2243\n",
      "Epoch 46/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2140\n",
      "Epoch 00046: val_loss did not improve from 0.21556\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2139 - val_loss: 0.2177\n",
      "Epoch 47/1000\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.2050\n",
      "Epoch 00047: val_loss improved from 0.21556 to 0.21401, saving model to nn_model_4.hdf5\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2050 - val_loss: 0.2140\n",
      "Epoch 48/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2043\n",
      "Epoch 00048: val_loss did not improve from 0.21401\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2043 - val_loss: 0.2145\n",
      "Epoch 49/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2041\n",
      "Epoch 00049: val_loss did not improve from 0.21401\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2041 - val_loss: 0.2163\n",
      "Epoch 50/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.2039\n",
      "Epoch 00050: val_loss did not improve from 0.21401\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2039 - val_loss: 0.2151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/1000\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.2038\n",
      "Epoch 00051: val_loss did not improve from 0.21401\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2038 - val_loss: 0.2145\n",
      "Epoch 52/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2039\n",
      "Epoch 00052: val_loss improved from 0.21401 to 0.21393, saving model to nn_model_4.hdf5\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2039 - val_loss: 0.2139\n",
      "Epoch 53/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2038\n",
      "Epoch 00053: val_loss did not improve from 0.21393\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2039 - val_loss: 0.2140\n",
      "Epoch 54/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2039\n",
      "Epoch 00054: val_loss did not improve from 0.21393\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2040 - val_loss: 0.2145\n",
      "Epoch 55/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2027\n",
      "Epoch 00055: val_loss did not improve from 0.21393\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2027 - val_loss: 0.2150\n",
      "Epoch 56/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2026\n",
      "Epoch 00056: val_loss did not improve from 0.21393\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2026 - val_loss: 0.2141\n",
      "Epoch 57/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2025\n",
      "Epoch 00057: val_loss improved from 0.21393 to 0.21377, saving model to nn_model_4.hdf5\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2025 - val_loss: 0.2138\n",
      "Epoch 58/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2026\n",
      "Epoch 00058: val_loss did not improve from 0.21377\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2025 - val_loss: 0.2141\n",
      "Epoch 59/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2024\n",
      "Epoch 00059: val_loss did not improve from 0.21377\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2024 - val_loss: 0.2143\n",
      "Epoch 60/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2024\n",
      "Epoch 00060: val_loss did not improve from 0.21377\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2024 - val_loss: 0.2149\n",
      "Epoch 61/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.2025\n",
      "Epoch 00061: val_loss did not improve from 0.21377\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2024 - val_loss: 0.2145\n",
      "Epoch 62/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2023\n",
      "Epoch 00062: val_loss did not improve from 0.21377\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2023 - val_loss: 0.2140\n",
      "Epoch 63/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2024\n",
      "Epoch 00063: val_loss did not improve from 0.21377\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2024 - val_loss: 0.2147\n",
      "Epoch 64/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2024\n",
      "Epoch 00064: val_loss improved from 0.21377 to 0.21375, saving model to nn_model_4.hdf5\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2024 - val_loss: 0.2138\n",
      "Epoch 65/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.2021\n",
      "Epoch 00065: val_loss improved from 0.21375 to 0.21345, saving model to nn_model_4.hdf5\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2020 - val_loss: 0.2135\n",
      "Epoch 66/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2019\n",
      "Epoch 00066: val_loss did not improve from 0.21345\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2019 - val_loss: 0.2137\n",
      "Epoch 67/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2019\n",
      "Epoch 00067: val_loss did not improve from 0.21345\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2019 - val_loss: 0.2137\n",
      "Epoch 68/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2019\n",
      "Epoch 00068: val_loss did not improve from 0.21345\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2019 - val_loss: 0.2138\n",
      "Epoch 69/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.2018- E\n",
      "Epoch 00069: val_loss did not improve from 0.21345\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2019 - val_loss: 0.2141\n",
      "Epoch 70/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2019\n",
      "Epoch 00070: val_loss did not improve from 0.21345\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2019 - val_loss: 0.2136\n",
      "Epoch 71/1000\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.2019\n",
      "Epoch 00071: val_loss did not improve from 0.21345\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2019 - val_loss: 0.2138\n",
      "Epoch 72/1000\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.2018\n",
      "Epoch 00072: val_loss did not improve from 0.21345\n",
      "168/168 [==============================] - 3s 17ms/step - loss: 0.2018 - val_loss: 0.2138\n",
      "Epoch 73/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2018\n",
      "Epoch 00073: val_loss did not improve from 0.21345\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.2018 - val_loss: 0.2139\n",
      "Epoch 74/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2017\n",
      "Epoch 00074: val_loss did not improve from 0.21345\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2017 - val_loss: 0.2137\n",
      "Epoch 75/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2018\n",
      "Epoch 00075: val_loss did not improve from 0.21345\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2017 - val_loss: 0.2138\n",
      "Epoch 76/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.2018\n",
      "Epoch 00076: val_loss did not improve from 0.21345\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2017 - val_loss: 0.2138\n",
      "Epoch 77/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2017\n",
      "Epoch 00077: val_loss did not improve from 0.21345\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2017 - val_loss: 0.2137\n",
      "Epoch 78/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.2017\n",
      "Epoch 00078: val_loss did not improve from 0.21345\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2017 - val_loss: 0.2138\n",
      "Epoch 79/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2017\n",
      "Epoch 00079: val_loss did not improve from 0.21345\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2017 - val_loss: 0.2138\n",
      "Epoch 80/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2017\n",
      "Epoch 00080: val_loss did not improve from 0.21345\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2017 - val_loss: 0.2138\n",
      "Epoch 81/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2018\n",
      "Epoch 00081: val_loss did not improve from 0.21345\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2017 - val_loss: 0.2138\n",
      "Epoch 82/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2017\n",
      "Epoch 00082: val_loss did not improve from 0.21345\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2017 - val_loss: 0.2138\n",
      "Epoch 83/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2018\n",
      "Epoch 00083: val_loss did not improve from 0.21345\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2017 - val_loss: 0.2138\n",
      "Epoch 84/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2017\n",
      "Epoch 00084: val_loss did not improve from 0.21345\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2017 - val_loss: 0.2138\n",
      "Epoch 85/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2017\n",
      "Epoch 00085: val_loss did not improve from 0.21345\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2017 - val_loss: 0.2138\n",
      "Fold 5 NN: 0.21345\n"
     ]
    }
   ],
   "source": [
    "target_name='target'\n",
    "scores_folds = {}\n",
    "model_name = 'NN'\n",
    "pred_name = 'pred_{}'.format(model_name)\n",
    "\n",
    "n_folds = 5\n",
    "kf = model_selection.KFold(n_splits=n_folds, shuffle=True, random_state=2020)\n",
    "scores_folds[model_name] = []\n",
    "counter = 1\n",
    "\n",
    "features_to_consider = list(train_nn)\n",
    "\n",
    "features_to_consider.remove('time_id')\n",
    "features_to_consider.remove('target')\n",
    "try:\n",
    "    features_to_consider.remove('pred_NN')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "train_nn[features_to_consider] = train_nn[features_to_consider].fillna(train_nn[features_to_consider].mean())\n",
    "test_nn[features_to_consider] = test_nn[features_to_consider].fillna(train_nn[features_to_consider].mean())\n",
    "\n",
    "train_nn[pred_name] = 0\n",
    "test_nn[target_name] = 0\n",
    "test_predictions_nn = np.zeros(test_nn.shape[0])\n",
    "\n",
    "for n_count in range(n_folds):\n",
    "    print('CV {}/{}'.format(counter, n_folds))\n",
    "    \n",
    "    indexes = np.arange(nfolds).astype(int)    \n",
    "    indexes = np.delete(indexes,obj=n_count, axis=0) \n",
    "    \n",
    "    indexes = np.r_[values[indexes[0]],values[indexes[1]],values[indexes[2]],values[indexes[3]]]\n",
    "    \n",
    "    X_train = train_nn.loc[train_nn.time_id.isin(indexes), features_to_consider]\n",
    "    y_train = train_nn.loc[train_nn.time_id.isin(indexes), target_name]\n",
    "    X_test = train_nn.loc[train_nn.time_id.isin(values[n_count]), features_to_consider]\n",
    "    y_test = train_nn.loc[train_nn.time_id.isin(values[n_count]), target_name]\n",
    "    \n",
    "    #############################################################################################\n",
    "    # NN\n",
    "    #############################################################################################\n",
    "    \n",
    "    model = base_model()\n",
    "    \n",
    "    model.compile(\n",
    "        keras.optimizers.Adam(learning_rate=0.006),\n",
    "        loss=root_mean_squared_per_error\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        features_to_consider.remove('stock_id')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    num_data = X_train[features_to_consider]\n",
    "    \n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))         \n",
    "    num_data = scaler.fit_transform(num_data.values)    \n",
    "    \n",
    "    cat_data = X_train['stock_id']    \n",
    "    target =  y_train\n",
    "    \n",
    "    num_data_test = X_test[features_to_consider]\n",
    "    num_data_test = scaler.transform(num_data_test.values)\n",
    "    cat_data_test = X_test['stock_id']\n",
    "    filepath = 'nn_model_'+str(n_count)+'.hdf5'\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=filepath, \n",
    "                             monitor='val_loss',\n",
    "                             verbose=1, \n",
    "                             save_best_only=True,\n",
    "                             mode='min')\n",
    "    model.fit([cat_data, num_data], \n",
    "              target,               \n",
    "              batch_size=2048,\n",
    "              epochs=1000,\n",
    "              validation_data=([cat_data_test, num_data_test], y_test),\n",
    "              callbacks=[es, plateau,checkpoint],\n",
    "              validation_batch_size=len(y_test),\n",
    "              shuffle=True,\n",
    "             verbose = 1)\n",
    "\n",
    "    preds = model.predict([cat_data_test, num_data_test]).reshape(1,-1)[0]\n",
    "    \n",
    "    score = round(rmspe(y_true = y_test, y_pred = preds),5)\n",
    "    print('Fold {} {}: {}'.format(counter, model_name, score))\n",
    "    scores_folds[model_name].append(score)\n",
    "    \n",
    "    tt =scaler.transform(test_nn[features_to_consider].values)\n",
    "    #test_nn[target_name] += model.predict([test_nn['stock_id'], tt]).reshape(1,-1)[0].clip(0,1e10)\n",
    "    test_predictions_nn += model.predict([test_nn['stock_id'], tt]).reshape(1,-1)[0].clip(0,1e10)/n_folds\n",
    "    #test[target_name] += model.predict([test['stock_id'], test[features_to_consider]]).reshape(1,-1)[0].clip(0,1e10)\n",
    "       \n",
    "    counter += 1\n",
    "    features_to_consider.append('stock_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-16T17:18:03.999096Z",
     "iopub.status.busy": "2021-09-16T17:18:03.998722Z",
     "iopub.status.idle": "2021-09-16T17:18:04.004957Z",
     "shell.execute_reply": "2021-09-16T17:18:04.004124Z",
     "shell.execute_reply.started": "2021-09-16T17:18:03.999058Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00226717])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predictions_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-16T17:18:05.206845Z",
     "iopub.status.busy": "2021-09-16T17:18:05.206484Z",
     "iopub.status.idle": "2021-09-16T17:30:22.524822Z",
     "shell.execute_reply": "2021-09-16T17:30:22.523408Z",
     "shell.execute_reply.started": "2021-09-16T17:18:05.206804Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV 1/5\n",
      "Epoch 1/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 6.6394\n",
      "Epoch 00001: val_loss improved from inf to 1.52920, saving model to nn2_model_0.hdf5\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 6.5519 - val_loss: 1.5292\n",
      "Epoch 2/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.8187\n",
      "Epoch 00002: val_loss improved from 1.52920 to 0.43473, saving model to nn2_model_0.hdf5\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.8156 - val_loss: 0.4347\n",
      "Epoch 3/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.6267\n",
      "Epoch 00003: val_loss did not improve from 0.43473\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.6260 - val_loss: 0.6577\n",
      "Epoch 4/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.5842\n",
      "Epoch 00004: val_loss did not improve from 0.43473\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.5855 - val_loss: 0.6252\n",
      "Epoch 5/1000\n",
      "167/168 [============================>.] - ETA: 9s - loss: 0.5738 \n",
      "Epoch 00005: val_loss did not improve from 0.43473\n",
      "168/168 [==============================] - 1642s 10s/step - loss: 0.5736 - val_loss: 0.5103\n",
      "Epoch 6/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.6274\n",
      "Epoch 00006: val_loss did not improve from 0.43473\n",
      "168/168 [==============================] - 3s 21ms/step - loss: 0.6264 - val_loss: 0.6381\n",
      "Epoch 7/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.4835\n",
      "Epoch 00007: val_loss improved from 0.43473 to 0.42344, saving model to nn2_model_0.hdf5\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.4833 - val_loss: 0.4234\n",
      "Epoch 8/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.4931\n",
      "Epoch 00008: val_loss did not improve from 0.42344\n",
      "168/168 [==============================] - 3s 17ms/step - loss: 0.4940 - val_loss: 0.6958\n",
      "Epoch 9/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.4665- ETA: 0s -\n",
      "Epoch 00009: val_loss improved from 0.42344 to 0.32460, saving model to nn2_model_0.hdf5\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.4667 - val_loss: 0.3246\n",
      "Epoch 10/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.4178\n",
      "Epoch 00010: val_loss did not improve from 0.32460\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.4190 - val_loss: 0.3855\n",
      "Epoch 11/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.3234\n",
      "Epoch 00011: val_loss improved from 0.32460 to 0.21601, saving model to nn2_model_0.hdf5\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.3219 - val_loss: 0.2160\n",
      "Epoch 12/1000\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.2509\n",
      "Epoch 00012: val_loss did not improve from 0.21601\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2509 - val_loss: 0.2211\n",
      "Epoch 13/1000\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.2509\n",
      "Epoch 00013: val_loss did not improve from 0.21601\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2509 - val_loss: 0.4138\n",
      "Epoch 14/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2691\n",
      "Epoch 00014: val_loss did not improve from 0.21601\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2694 - val_loss: 0.2646\n",
      "Epoch 15/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2929\n",
      "Epoch 00015: val_loss did not improve from 0.21601\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2934 - val_loss: 1.3584\n",
      "Epoch 16/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 2.6842\n",
      "Epoch 00016: val_loss did not improve from 0.21601\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 2.6476 - val_loss: 0.2892\n",
      "Epoch 17/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2508\n",
      "Epoch 00017: val_loss did not improve from 0.21601\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2507 - val_loss: 0.2256\n",
      "Epoch 18/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2375\n",
      "Epoch 00018: val_loss did not improve from 0.21601\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2377 - val_loss: 0.2603\n",
      "Epoch 19/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.2226\n",
      "Epoch 00019: val_loss did not improve from 0.21601\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2226 - val_loss: 0.2173\n",
      "Epoch 20/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2209\n",
      "Epoch 00020: val_loss did not improve from 0.21601\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2208 - val_loss: 0.2171\n",
      "Epoch 21/1000\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.2196\n",
      "Epoch 00021: val_loss did not improve from 0.21601\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2196 - val_loss: 0.2164\n",
      "Epoch 22/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.2202\n",
      "Epoch 00022: val_loss did not improve from 0.21601\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2202 - val_loss: 0.2176\n",
      "Epoch 23/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.2190\n",
      "Epoch 00023: val_loss did not improve from 0.21601\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2191 - val_loss: 0.2263\n",
      "Epoch 24/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.2189\n",
      "Epoch 00024: val_loss improved from 0.21601 to 0.21595, saving model to nn2_model_0.hdf5\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2189 - val_loss: 0.2159\n",
      "Epoch 25/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2194\n",
      "Epoch 00025: val_loss did not improve from 0.21595\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2194 - val_loss: 0.2207\n",
      "Epoch 26/1000\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.2159\n",
      "Epoch 00026: val_loss improved from 0.21595 to 0.21448, saving model to nn2_model_0.hdf5\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2159 - val_loss: 0.2145\n",
      "Epoch 27/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2156\n",
      "Epoch 00027: val_loss improved from 0.21448 to 0.21418, saving model to nn2_model_0.hdf5\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2155 - val_loss: 0.2142\n",
      "Epoch 28/1000\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.2155\n",
      "Epoch 00028: val_loss did not improve from 0.21418\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2155 - val_loss: 0.2142\n",
      "Epoch 29/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2154\n",
      "Epoch 00029: val_loss did not improve from 0.21418\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2153 - val_loss: 0.2143\n",
      "Epoch 30/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2151- ETA: 0s - los\n",
      "Epoch 00030: val_loss did not improve from 0.21418\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2152 - val_loss: 0.2143\n",
      "Epoch 31/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2151\n",
      "Epoch 00031: val_loss improved from 0.21418 to 0.21410, saving model to nn2_model_0.hdf5\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2151 - val_loss: 0.2141\n",
      "Epoch 32/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2150\n",
      "Epoch 00032: val_loss did not improve from 0.21410\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2150 - val_loss: 0.2142\n",
      "Epoch 33/1000\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.2151\n",
      "Epoch 00033: val_loss improved from 0.21410 to 0.21365, saving model to nn2_model_0.hdf5\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2151 - val_loss: 0.2136\n",
      "Epoch 34/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2146\n",
      "Epoch 00034: val_loss did not improve from 0.21365\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2146 - val_loss: 0.2139\n",
      "Epoch 35/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2145\n",
      "Epoch 00035: val_loss improved from 0.21365 to 0.21355, saving model to nn2_model_0.hdf5\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2146 - val_loss: 0.2136\n",
      "Epoch 36/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2144\n",
      "Epoch 00036: val_loss did not improve from 0.21355\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2144 - val_loss: 0.2137\n",
      "Epoch 37/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2144\n",
      "Epoch 00037: val_loss did not improve from 0.21355\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2144 - val_loss: 0.2155\n",
      "Epoch 38/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2143\n",
      "Epoch 00038: val_loss improved from 0.21355 to 0.21326, saving model to nn2_model_0.hdf5\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2143 - val_loss: 0.2133\n",
      "Epoch 39/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2139\n",
      "Epoch 00039: val_loss improved from 0.21326 to 0.21287, saving model to nn2_model_0.hdf5\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2138 - val_loss: 0.2129\n",
      "Epoch 40/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2138\n",
      "Epoch 00040: val_loss improved from 0.21287 to 0.21270, saving model to nn2_model_0.hdf5\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2138 - val_loss: 0.2127\n",
      "Epoch 41/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.2132\n",
      "Epoch 00041: val_loss did not improve from 0.21270\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2132 - val_loss: 0.2135\n",
      "Epoch 42/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2133\n",
      "Epoch 00042: val_loss did not improve from 0.21270\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2132 - val_loss: 0.2130\n",
      "Epoch 43/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.2130\n",
      "Epoch 00043: val_loss did not improve from 0.21270\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2130 - val_loss: 0.2127\n",
      "Epoch 44/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2126\n",
      "Epoch 00044: val_loss improved from 0.21270 to 0.21200, saving model to nn2_model_0.hdf5\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2128 - val_loss: 0.2120\n",
      "Epoch 45/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2127\n",
      "Epoch 00045: val_loss did not improve from 0.21200\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2128 - val_loss: 0.2121\n",
      "Epoch 46/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2123\n",
      "Epoch 00046: val_loss improved from 0.21200 to 0.21153, saving model to nn2_model_0.hdf5\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2123 - val_loss: 0.2115\n",
      "Epoch 47/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2120\n",
      "Epoch 00047: val_loss did not improve from 0.21153\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2120 - val_loss: 0.2119\n",
      "Epoch 48/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2118\n",
      "Epoch 00048: val_loss did not improve from 0.21153\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2118 - val_loss: 0.2117\n",
      "Epoch 49/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2119\n",
      "Epoch 00049: val_loss improved from 0.21153 to 0.21153, saving model to nn2_model_0.hdf5\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2118 - val_loss: 0.2115\n",
      "Epoch 50/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2114\n",
      "Epoch 00050: val_loss improved from 0.21153 to 0.21110, saving model to nn2_model_0.hdf5\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2115 - val_loss: 0.2111\n",
      "Epoch 51/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2109\n",
      "Epoch 00051: val_loss did not improve from 0.21110\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2109 - val_loss: 0.2116\n",
      "Epoch 52/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2109\n",
      "Epoch 00052: val_loss did not improve from 0.21110\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2109 - val_loss: 0.2129\n",
      "Epoch 53/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2104\n",
      "Epoch 00053: val_loss improved from 0.21110 to 0.21080, saving model to nn2_model_0.hdf5\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2104 - val_loss: 0.2108\n",
      "Epoch 54/1000\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.2107\n",
      "Epoch 00054: val_loss did not improve from 0.21080\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2107 - val_loss: 0.2109\n",
      "Epoch 55/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2107- ETA: 0s - loss: \n",
      "Epoch 00055: val_loss improved from 0.21080 to 0.21030, saving model to nn2_model_0.hdf5\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2107 - val_loss: 0.2103\n",
      "Epoch 56/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2098\n",
      "Epoch 00056: val_loss did not improve from 0.21030\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.2098 - val_loss: 0.2106\n",
      "Epoch 57/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2108\n",
      "Epoch 00057: val_loss did not improve from 0.21030\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.2108 - val_loss: 0.2118\n",
      "Epoch 58/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2101\n",
      "Epoch 00058: val_loss did not improve from 0.21030\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2101 - val_loss: 0.2104\n",
      "Epoch 59/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2099\n",
      "Epoch 00059: val_loss did not improve from 0.21030\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2099 - val_loss: 0.2105\n",
      "Epoch 60/1000\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.2094\n",
      "Epoch 00060: val_loss improved from 0.21030 to 0.21027, saving model to nn2_model_0.hdf5\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2094 - val_loss: 0.2103\n",
      "Epoch 61/1000\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.2088\n",
      "Epoch 00061: val_loss did not improve from 0.21027\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2088 - val_loss: 0.2132\n",
      "Epoch 62/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2087\n",
      "Epoch 00062: val_loss did not improve from 0.21027\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2087 - val_loss: 0.2106\n",
      "Epoch 63/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2073\n",
      "Epoch 00063: val_loss improved from 0.21027 to 0.20973, saving model to nn2_model_0.hdf5\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2073 - val_loss: 0.2097\n",
      "Epoch 64/1000\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.2072\n",
      "Epoch 00064: val_loss did not improve from 0.20973\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2072 - val_loss: 0.2098\n",
      "Epoch 65/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2072\n",
      "Epoch 00065: val_loss did not improve from 0.20973\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2072 - val_loss: 0.2098\n",
      "Epoch 66/1000\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.2071\n",
      "Epoch 00066: val_loss did not improve from 0.20973\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2071 - val_loss: 0.2098\n",
      "Epoch 67/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2071\n",
      "Epoch 00067: val_loss did not improve from 0.20973\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2070 - val_loss: 0.2101\n",
      "Epoch 68/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2070- ETA: 0s - lo\n",
      "Epoch 00068: val_loss did not improve from 0.20973\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2070 - val_loss: 0.2100\n",
      "Epoch 69/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2069\n",
      "Epoch 00069: val_loss did not improve from 0.20973\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2069 - val_loss: 0.2100\n",
      "Epoch 70/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2069\n",
      "Epoch 00070: val_loss did not improve from 0.20973\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2070 - val_loss: 0.2106\n",
      "Epoch 71/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165/168 [============================>.] - ETA: 0s - loss: 0.2066\n",
      "Epoch 00071: val_loss improved from 0.20973 to 0.20969, saving model to nn2_model_0.hdf5\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2067 - val_loss: 0.2097\n",
      "Epoch 72/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2066\n",
      "Epoch 00072: val_loss did not improve from 0.20969\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2065 - val_loss: 0.2097\n",
      "Epoch 73/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2065\n",
      "Epoch 00073: val_loss improved from 0.20969 to 0.20960, saving model to nn2_model_0.hdf5\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2065 - val_loss: 0.2096\n",
      "Epoch 74/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2066\n",
      "Epoch 00074: val_loss did not improve from 0.20960\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2065 - val_loss: 0.2096\n",
      "Epoch 75/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2066\n",
      "Epoch 00075: val_loss improved from 0.20960 to 0.20957, saving model to nn2_model_0.hdf5\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2065 - val_loss: 0.2096\n",
      "Epoch 76/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2066\n",
      "Epoch 00076: val_loss did not improve from 0.20957\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2065 - val_loss: 0.2098\n",
      "Epoch 77/1000\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.2065\n",
      "Epoch 00077: val_loss did not improve from 0.20957\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2065 - val_loss: 0.2098\n",
      "Epoch 78/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2065\n",
      "Epoch 00078: val_loss did not improve from 0.20957\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2065 - val_loss: 0.2099\n",
      "Epoch 79/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2065\n",
      "Epoch 00079: val_loss did not improve from 0.20957\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2065 - val_loss: 0.2096\n",
      "Epoch 80/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2065\n",
      "Epoch 00080: val_loss did not improve from 0.20957\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2065 - val_loss: 0.2096\n",
      "Epoch 81/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2064\n",
      "Epoch 00081: val_loss did not improve from 0.20957\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2064 - val_loss: 0.2096\n",
      "Epoch 82/1000\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.2063\n",
      "Epoch 00082: val_loss did not improve from 0.20957\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2063 - val_loss: 0.2096\n",
      "Epoch 83/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2064\n",
      "Epoch 00083: val_loss did not improve from 0.20957\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2063 - val_loss: 0.2096\n",
      "Epoch 84/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2064\n",
      "Epoch 00084: val_loss did not improve from 0.20957\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2063 - val_loss: 0.2096\n",
      "Epoch 85/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2064\n",
      "Epoch 00085: val_loss did not improve from 0.20957\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2063 - val_loss: 0.2096\n",
      "Epoch 86/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2064\n",
      "Epoch 00086: val_loss did not improve from 0.20957\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2064 - val_loss: 0.2097\n",
      "Epoch 87/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2064\n",
      "Epoch 00087: val_loss did not improve from 0.20957\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2063 - val_loss: 0.2096\n",
      "Epoch 88/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2063\n",
      "Epoch 00088: val_loss improved from 0.20957 to 0.20956, saving model to nn2_model_0.hdf5\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2063 - val_loss: 0.2096\n",
      "Epoch 89/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2063\n",
      "Epoch 00089: val_loss did not improve from 0.20956\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2063 - val_loss: 0.2096\n",
      "Epoch 90/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2063\n",
      "Epoch 00090: val_loss did not improve from 0.20956\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2063 - val_loss: 0.2096\n",
      "Epoch 91/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.2062\n",
      "Epoch 00091: val_loss did not improve from 0.20956\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2063 - val_loss: 0.2096\n",
      "Epoch 92/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2063\n",
      "Epoch 00092: val_loss did not improve from 0.20956\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2063 - val_loss: 0.2096\n",
      "Epoch 93/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.2062\n",
      "Epoch 00093: val_loss improved from 0.20956 to 0.20956, saving model to nn2_model_0.hdf5\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2063 - val_loss: 0.2096\n",
      "Epoch 94/1000\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.2063\n",
      "Epoch 00094: val_loss improved from 0.20956 to 0.20955, saving model to nn2_model_0.hdf5\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2063 - val_loss: 0.2096\n",
      "Epoch 95/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2063\n",
      "Epoch 00095: val_loss did not improve from 0.20955\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2063 - val_loss: 0.2096\n",
      "Epoch 96/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2063\n",
      "Epoch 00096: val_loss did not improve from 0.20955\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2063 - val_loss: 0.2096\n",
      "Epoch 97/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2063\n",
      "Epoch 00097: val_loss did not improve from 0.20955\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2063 - val_loss: 0.2096\n",
      "Epoch 98/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.2063\n",
      "Epoch 00098: val_loss did not improve from 0.20955\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2063 - val_loss: 0.2096\n",
      "Epoch 99/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2063\n",
      "Epoch 00099: val_loss did not improve from 0.20955\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2063 - val_loss: 0.2096\n",
      "Epoch 100/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2063\n",
      "Epoch 00100: val_loss did not improve from 0.20955\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2063 - val_loss: 0.2096\n",
      "Epoch 101/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2063\n",
      "Epoch 00101: val_loss did not improve from 0.20955\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2063 - val_loss: 0.2096\n",
      "Epoch 102/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2063\n",
      "Epoch 00102: val_loss did not improve from 0.20955\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2063 - val_loss: 0.2096\n",
      "Epoch 103/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2064\n",
      "Epoch 00103: val_loss did not improve from 0.20955\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2063 - val_loss: 0.2096\n",
      "Epoch 104/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.2063\n",
      "Epoch 00104: val_loss did not improve from 0.20955\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2063 - val_loss: 0.2096\n",
      "Epoch 105/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2063\n",
      "Epoch 00105: val_loss did not improve from 0.20955\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2063 - val_loss: 0.2096\n",
      "Epoch 106/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2063\n",
      "Epoch 00106: val_loss did not improve from 0.20955\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2063 - val_loss: 0.2096\n",
      "Epoch 107/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2063\n",
      "Epoch 00107: val_loss did not improve from 0.20955\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2063 - val_loss: 0.2096\n",
      "Epoch 108/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2063\n",
      "Epoch 00108: val_loss did not improve from 0.20955\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2063 - val_loss: 0.2096\n",
      "Epoch 109/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.2063\n",
      "Epoch 00109: val_loss did not improve from 0.20955\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2063 - val_loss: 0.2096\n",
      "Epoch 110/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.2061\n",
      "Epoch 00110: val_loss did not improve from 0.20955\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2063 - val_loss: 0.2096\n",
      "Epoch 111/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2063\n",
      "Epoch 00111: val_loss did not improve from 0.20955\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2063 - val_loss: 0.2096\n",
      "Epoch 112/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.2062\n",
      "Epoch 00112: val_loss did not improve from 0.20955\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2063 - val_loss: 0.2096\n",
      "Epoch 113/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2063\n",
      "Epoch 00113: val_loss did not improve from 0.20955\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2063 - val_loss: 0.2096\n",
      "Epoch 114/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2062\n",
      "Epoch 00114: val_loss did not improve from 0.20955\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.2063 - val_loss: 0.2096\n",
      "Fold 1 NN: 0.20955\n",
      "CV 2/5\n",
      "Epoch 1/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 10.2050\n",
      "Epoch 00001: val_loss improved from inf to 1.73685, saving model to nn2_model_1.hdf5\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 10.1321 - val_loss: 1.7368\n",
      "Epoch 2/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 2.1556\n",
      "Epoch 00002: val_loss improved from 1.73685 to 0.74066, saving model to nn2_model_1.hdf5\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 2.1394 - val_loss: 0.7407\n",
      "Epoch 3/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.7703\n",
      "Epoch 00003: val_loss improved from 0.74066 to 0.52697, saving model to nn2_model_1.hdf5\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.7664 - val_loss: 0.5270\n",
      "Epoch 4/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.6901\n",
      "Epoch 00004: val_loss did not improve from 0.52697\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.6897 - val_loss: 0.7102\n",
      "Epoch 5/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.6429\n",
      "Epoch 00005: val_loss did not improve from 0.52697\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.6423 - val_loss: 0.6494\n",
      "Epoch 6/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.6211- ETA: 0s - l - ETA: 0s - loss: 0\n",
      "Epoch 00006: val_loss did not improve from 0.52697\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.6214 - val_loss: 0.6309\n",
      "Epoch 7/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.8019- ETA\n",
      "Epoch 00007: val_loss did not improve from 0.52697\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.8085 - val_loss: 1.6206\n",
      "Epoch 8/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.6658\n",
      "Epoch 00008: val_loss improved from 0.52697 to 0.24095, saving model to nn2_model_1.hdf5\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.6594 - val_loss: 0.2409\n",
      "Epoch 9/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.4120\n",
      "Epoch 00009: val_loss did not improve from 0.24095\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.4128 - val_loss: 0.5821\n",
      "Epoch 10/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.5438\n",
      "Epoch 00010: val_loss did not improve from 0.24095\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.5425 - val_loss: 0.4013\n",
      "Epoch 11/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.5448\n",
      "Epoch 00011: val_loss did not improve from 0.24095\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.5451 - val_loss: 0.4603\n",
      "Epoch 12/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.5005\n",
      "Epoch 00012: val_loss did not improve from 0.24095\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.5001 - val_loss: 0.5087\n",
      "Epoch 13/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.4799\n",
      "Epoch 00013: val_loss did not improve from 0.24095\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.4792 - val_loss: 0.3622\n",
      "Epoch 14/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.4104\n",
      "Epoch 00014: val_loss did not improve from 0.24095\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.4101 - val_loss: 0.3899\n",
      "Epoch 15/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.4077\n",
      "Epoch 00015: val_loss did not improve from 0.24095\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.4071 - val_loss: 0.3127\n",
      "Epoch 16/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2168\n",
      "Epoch 00016: val_loss improved from 0.24095 to 0.22544, saving model to nn2_model_1.hdf5\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2169 - val_loss: 0.2254\n",
      "Epoch 17/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.2132\n",
      "Epoch 00017: val_loss improved from 0.22544 to 0.22186, saving model to nn2_model_1.hdf5\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2132 - val_loss: 0.2219\n",
      "Epoch 18/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2117\n",
      "Epoch 00018: val_loss improved from 0.22186 to 0.21916, saving model to nn2_model_1.hdf5\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2117 - val_loss: 0.2192\n",
      "Epoch 19/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2118- ETA: 0s - \n",
      "Epoch 00019: val_loss did not improve from 0.21916\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2117 - val_loss: 0.2223\n",
      "Epoch 20/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.2103\n",
      "Epoch 00020: val_loss improved from 0.21916 to 0.21862, saving model to nn2_model_1.hdf5\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2103 - val_loss: 0.2186\n",
      "Epoch 21/1000\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.2095\n",
      "Epoch 00021: val_loss did not improve from 0.21862\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2095 - val_loss: 0.2195\n",
      "Epoch 22/1000\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.2098\n",
      "Epoch 00022: val_loss did not improve from 0.21862\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2098 - val_loss: 0.2215\n",
      "Epoch 23/1000\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.2090\n",
      "Epoch 00023: val_loss did not improve from 0.21862\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2090 - val_loss: 0.2206\n",
      "Epoch 24/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2135\n",
      "Epoch 00024: val_loss did not improve from 0.21862\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2135 - val_loss: 0.2213\n",
      "Epoch 25/1000\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.2128\n",
      "Epoch 00025: val_loss did not improve from 0.21862\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2128 - val_loss: 0.2242\n",
      "Epoch 26/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2113\n",
      "Epoch 00026: val_loss did not improve from 0.21862\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2113 - val_loss: 0.2205\n",
      "Epoch 27/1000\n",
      "163/168 [============================>.] - ETA: 0s - loss: 0.2104\n",
      "Epoch 00027: val_loss improved from 0.21862 to 0.21646, saving model to nn2_model_1.hdf5\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2102 - val_loss: 0.2165\n",
      "Epoch 28/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164/168 [============================>.] - ETA: 0s - loss: 0.2108\n",
      "Epoch 00028: val_loss did not improve from 0.21646\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2109 - val_loss: 0.2217\n",
      "Epoch 29/1000\n",
      "163/168 [============================>.] - ETA: 0s - loss: 0.2099\n",
      "Epoch 00029: val_loss did not improve from 0.21646\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2099 - val_loss: 0.2210\n",
      "Epoch 30/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2086\n",
      "Epoch 00030: val_loss did not improve from 0.21646\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2086 - val_loss: 0.2183\n",
      "Epoch 31/1000\n",
      "163/168 [============================>.] - ETA: 0s - loss: 0.2099\n",
      "Epoch 00031: val_loss did not improve from 0.21646\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2100 - val_loss: 0.2299\n",
      "Epoch 32/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2092\n",
      "Epoch 00032: val_loss did not improve from 0.21646\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2092 - val_loss: 0.2193\n",
      "Epoch 33/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2104\n",
      "Epoch 00033: val_loss did not improve from 0.21646\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2105 - val_loss: 0.2179\n",
      "Epoch 34/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2096\n",
      "Epoch 00034: val_loss did not improve from 0.21646\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2097 - val_loss: 0.2199\n",
      "Epoch 35/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2039\n",
      "Epoch 00035: val_loss improved from 0.21646 to 0.21457, saving model to nn2_model_1.hdf5\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2038 - val_loss: 0.2146\n",
      "Epoch 36/1000\n",
      "163/168 [============================>.] - ETA: 0s - loss: 0.2035\n",
      "Epoch 00036: val_loss did not improve from 0.21457\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2035 - val_loss: 0.2161\n",
      "Epoch 37/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2036- ET\n",
      "Epoch 00037: val_loss did not improve from 0.21457\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2037 - val_loss: 0.2156\n",
      "Epoch 38/1000\n",
      "163/168 [============================>.] - ETA: 0s - loss: 0.2035\n",
      "Epoch 00038: val_loss did not improve from 0.21457\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2034 - val_loss: 0.2152\n",
      "Epoch 39/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.2036\n",
      "Epoch 00039: val_loss did not improve from 0.21457\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2035 - val_loss: 0.2153\n",
      "Epoch 40/1000\n",
      "163/168 [============================>.] - ETA: 0s - loss: 0.2038\n",
      "Epoch 00040: val_loss did not improve from 0.21457\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2039 - val_loss: 0.2179\n",
      "Epoch 41/1000\n",
      "163/168 [============================>.] - ETA: 0s - loss: 0.2033\n",
      "Epoch 00041: val_loss improved from 0.21457 to 0.21417, saving model to nn2_model_1.hdf5\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2034 - val_loss: 0.2142\n",
      "Epoch 42/1000\n",
      "163/168 [============================>.] - ETA: 0s - loss: 0.2030\n",
      "Epoch 00042: val_loss improved from 0.21417 to 0.21390, saving model to nn2_model_1.hdf5\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2031 - val_loss: 0.2139\n",
      "Epoch 43/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.2035\n",
      "Epoch 00043: val_loss improved from 0.21390 to 0.21377, saving model to nn2_model_1.hdf5\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2035 - val_loss: 0.2138\n",
      "Epoch 44/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2035\n",
      "Epoch 00044: val_loss did not improve from 0.21377\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2036 - val_loss: 0.2154\n",
      "Epoch 45/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2036\n",
      "Epoch 00045: val_loss did not improve from 0.21377\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2036 - val_loss: 0.2166\n",
      "Epoch 46/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.2034\n",
      "Epoch 00046: val_loss did not improve from 0.21377\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2034 - val_loss: 0.2155\n",
      "Epoch 47/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2032\n",
      "Epoch 00047: val_loss did not improve from 0.21377\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2032 - val_loss: 0.2138\n",
      "Epoch 48/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2032\n",
      "Epoch 00048: val_loss did not improve from 0.21377\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2032 - val_loss: 0.2172\n",
      "Epoch 49/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2034\n",
      "Epoch 00049: val_loss did not improve from 0.21377\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2034 - val_loss: 0.2159\n",
      "Epoch 50/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2036\n",
      "Epoch 00050: val_loss did not improve from 0.21377\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2035 - val_loss: 0.2171\n",
      "Epoch 51/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2012\n",
      "Epoch 00051: val_loss did not improve from 0.21377\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2011 - val_loss: 0.2146\n",
      "Epoch 52/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2010\n",
      "Epoch 00052: val_loss did not improve from 0.21377\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2009 - val_loss: 0.2147\n",
      "Epoch 53/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2012\n",
      "Epoch 00053: val_loss did not improve from 0.21377\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2012 - val_loss: 0.2146\n",
      "Epoch 54/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2009\n",
      "Epoch 00054: val_loss improved from 0.21377 to 0.21364, saving model to nn2_model_1.hdf5\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2009 - val_loss: 0.2136\n",
      "Epoch 55/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2009\n",
      "Epoch 00055: val_loss did not improve from 0.21364\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2009 - val_loss: 0.2150\n",
      "Epoch 56/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.2009\n",
      "Epoch 00056: val_loss did not improve from 0.21364\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2009 - val_loss: 0.2141\n",
      "Epoch 57/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.2008\n",
      "Epoch 00057: val_loss did not improve from 0.21364\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2009 - val_loss: 0.2169\n",
      "Epoch 58/1000\n",
      "163/168 [============================>.] - ETA: 0s - loss: 0.2009\n",
      "Epoch 00058: val_loss did not improve from 0.21364\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2010 - val_loss: 0.2140\n",
      "Epoch 59/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2008\n",
      "Epoch 00059: val_loss did not improve from 0.21364\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2008 - val_loss: 0.2138\n",
      "Epoch 60/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2008\n",
      "Epoch 00060: val_loss did not improve from 0.21364\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2008 - val_loss: 0.2145\n",
      "Epoch 61/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2008\n",
      "Epoch 00061: val_loss did not improve from 0.21364\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2008 - val_loss: 0.2160\n",
      "Epoch 62/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2001\n",
      "Epoch 00062: val_loss did not improve from 0.21364\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2001 - val_loss: 0.2137\n",
      "Epoch 63/1000\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.2002\n",
      "Epoch 00063: val_loss improved from 0.21364 to 0.21346, saving model to nn2_model_1.hdf5\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2002 - val_loss: 0.2135\n",
      "Epoch 64/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.2000\n",
      "Epoch 00064: val_loss did not improve from 0.21346\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2001 - val_loss: 0.2138\n",
      "Epoch 65/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.2002\n",
      "Epoch 00065: val_loss did not improve from 0.21346\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2001 - val_loss: 0.2143\n",
      "Epoch 66/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2000\n",
      "Epoch 00066: val_loss did not improve from 0.21346\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2000 - val_loss: 0.2147\n",
      "Epoch 67/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2001\n",
      "Epoch 00067: val_loss did not improve from 0.21346\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2001 - val_loss: 0.2150\n",
      "Epoch 68/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2000\n",
      "Epoch 00068: val_loss did not improve from 0.21346\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2000 - val_loss: 0.2146\n",
      "Epoch 69/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2001\n",
      "Epoch 00069: val_loss did not improve from 0.21346\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2001 - val_loss: 0.2147\n",
      "Epoch 70/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2000\n",
      "Epoch 00070: val_loss did not improve from 0.21346\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2000 - val_loss: 0.2141\n",
      "Epoch 71/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.1999\n",
      "Epoch 00071: val_loss did not improve from 0.21346\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.1999 - val_loss: 0.2143\n",
      "Epoch 72/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.1999\n",
      "Epoch 00072: val_loss did not improve from 0.21346\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.1998 - val_loss: 0.2143\n",
      "Epoch 73/1000\n",
      "163/168 [============================>.] - ETA: 0s - loss: 0.2000\n",
      "Epoch 00073: val_loss did not improve from 0.21346\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.1999 - val_loss: 0.2143\n",
      "Epoch 74/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.1999\n",
      "Epoch 00074: val_loss did not improve from 0.21346\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.1998 - val_loss: 0.2142\n",
      "Epoch 75/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.1998\n",
      "Epoch 00075: val_loss did not improve from 0.21346\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.1998 - val_loss: 0.2141\n",
      "Epoch 76/1000\n",
      "163/168 [============================>.] - ETA: 0s - loss: 0.1999\n",
      "Epoch 00076: val_loss did not improve from 0.21346\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.1998 - val_loss: 0.2142\n",
      "Epoch 77/1000\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.1998\n",
      "Epoch 00077: val_loss did not improve from 0.21346\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.1998 - val_loss: 0.2143\n",
      "Epoch 78/1000\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.1998\n",
      "Epoch 00078: val_loss did not improve from 0.21346\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.1998 - val_loss: 0.2144\n",
      "Epoch 79/1000\n",
      "163/168 [============================>.] - ETA: 0s - loss: 0.1996\n",
      "Epoch 00079: val_loss did not improve from 0.21346\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.1998 - val_loss: 0.2142\n",
      "Epoch 80/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.1998\n",
      "Epoch 00080: val_loss did not improve from 0.21346\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.1998 - val_loss: 0.2142\n",
      "Epoch 81/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.1998\n",
      "Epoch 00081: val_loss did not improve from 0.21346\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.1998 - val_loss: 0.2142\n",
      "Epoch 82/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.1997\n",
      "Epoch 00082: val_loss did not improve from 0.21346\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.1998 - val_loss: 0.2141\n",
      "Epoch 83/1000\n",
      "163/168 [============================>.] - ETA: 0s - loss: 0.1998\n",
      "Epoch 00083: val_loss did not improve from 0.21346\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.1998 - val_loss: 0.2141\n",
      "Fold 2 NN: 0.21346\n",
      "CV 3/5\n",
      "Epoch 1/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 10.6576\n",
      "Epoch 00001: val_loss improved from inf to 1.31196, saving model to nn2_model_2.hdf5\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 10.6266 - val_loss: 1.3120\n",
      "Epoch 2/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.8801\n",
      "Epoch 00002: val_loss improved from 1.31196 to 0.67491, saving model to nn2_model_2.hdf5\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.8763 - val_loss: 0.6749\n",
      "Epoch 3/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.6354\n",
      "Epoch 00003: val_loss improved from 0.67491 to 0.64220, saving model to nn2_model_2.hdf5\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.6352 - val_loss: 0.6422\n",
      "Epoch 4/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.6166\n",
      "Epoch 00004: val_loss improved from 0.64220 to 0.60927, saving model to nn2_model_2.hdf5\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.6165 - val_loss: 0.6093\n",
      "Epoch 5/1000\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.6608\n",
      "Epoch 00005: val_loss did not improve from 0.60927\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.6608 - val_loss: 0.6475\n",
      "Epoch 6/1000\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.7275\n",
      "Epoch 00006: val_loss improved from 0.60927 to 0.24287, saving model to nn2_model_2.hdf5\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.7275 - val_loss: 0.2429\n",
      "Epoch 7/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2871\n",
      "Epoch 00007: val_loss did not improve from 0.24287\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2872 - val_loss: 0.4023\n",
      "Epoch 8/1000\n",
      "162/168 [===========================>..] - ETA: 0s - loss: 1.1818\n",
      "Epoch 00008: val_loss did not improve from 0.24287\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 1.1655 - val_loss: 0.6648\n",
      "Epoch 9/1000\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.6853\n",
      "Epoch 00009: val_loss did not improve from 0.24287\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.6853 - val_loss: 0.5084\n",
      "Epoch 10/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.4802\n",
      "Epoch 00010: val_loss did not improve from 0.24287\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.4800 - val_loss: 0.5579\n",
      "Epoch 11/1000\n",
      "163/168 [============================>.] - ETA: 0s - loss: 0.4759\n",
      "Epoch 00011: val_loss did not improve from 0.24287\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.4761 - val_loss: 0.4278\n",
      "Epoch 12/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.4720\n",
      "Epoch 00012: val_loss did not improve from 0.24287\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.4715 - val_loss: 0.5034\n",
      "Epoch 13/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.4601\n",
      "Epoch 00013: val_loss did not improve from 0.24287\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.4626 - val_loss: 0.6505\n",
      "Epoch 14/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2349\n",
      "Epoch 00014: val_loss improved from 0.24287 to 0.21426, saving model to nn2_model_2.hdf5\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2345 - val_loss: 0.2143\n",
      "Epoch 15/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.2156\n",
      "Epoch 00015: val_loss did not improve from 0.21426\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2156 - val_loss: 0.2153\n",
      "Epoch 16/1000\n",
      "163/168 [============================>.] - ETA: 0s - loss: 0.2150\n",
      "Epoch 00016: val_loss improved from 0.21426 to 0.21296, saving model to nn2_model_2.hdf5\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2149 - val_loss: 0.2130\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2143\n",
      "Epoch 00017: val_loss did not improve from 0.21296\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2143 - val_loss: 0.2186\n",
      "Epoch 18/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2133\n",
      "Epoch 00018: val_loss improved from 0.21296 to 0.21290, saving model to nn2_model_2.hdf5\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2133 - val_loss: 0.2129\n",
      "Epoch 19/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2126\n",
      "Epoch 00019: val_loss did not improve from 0.21290\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2126 - val_loss: 0.2135\n",
      "Epoch 20/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2127\n",
      "Epoch 00020: val_loss did not improve from 0.21290\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2128 - val_loss: 0.2156\n",
      "Epoch 21/1000\n",
      "163/168 [============================>.] - ETA: 0s - loss: 0.2111\n",
      "Epoch 00021: val_loss did not improve from 0.21290\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2111 - val_loss: 0.2131\n",
      "Epoch 22/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2116\n",
      "Epoch 00022: val_loss did not improve from 0.21290\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2116 - val_loss: 0.2260\n",
      "Epoch 23/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2154\n",
      "Epoch 00023: val_loss did not improve from 0.21290\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2154 - val_loss: 0.2144\n",
      "Epoch 24/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2084\n",
      "Epoch 00024: val_loss improved from 0.21290 to 0.21014, saving model to nn2_model_2.hdf5\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2084 - val_loss: 0.2101\n",
      "Epoch 25/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2078\n",
      "Epoch 00025: val_loss did not improve from 0.21014\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2079 - val_loss: 0.2102\n",
      "Epoch 26/1000\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.2078\n",
      "Epoch 00026: val_loss did not improve from 0.21014\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2078 - val_loss: 0.2103\n",
      "Epoch 27/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2078\n",
      "Epoch 00027: val_loss did not improve from 0.21014\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2078 - val_loss: 0.2105\n",
      "Epoch 28/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2079\n",
      "Epoch 00028: val_loss did not improve from 0.21014\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2079 - val_loss: 0.2110\n",
      "Epoch 29/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2077\n",
      "Epoch 00029: val_loss did not improve from 0.21014\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2076 - val_loss: 0.2103\n",
      "Epoch 30/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2074\n",
      "Epoch 00030: val_loss did not improve from 0.21014\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2074 - val_loss: 0.2105\n",
      "Epoch 31/1000\n",
      "163/168 [============================>.] - ETA: 0s - loss: 0.2073\n",
      "Epoch 00031: val_loss did not improve from 0.21014\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2074 - val_loss: 0.2114\n",
      "Epoch 32/1000\n",
      "163/168 [============================>.] - ETA: 0s - loss: 0.2064\n",
      "Epoch 00032: val_loss improved from 0.21014 to 0.20936, saving model to nn2_model_2.hdf5\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2064 - val_loss: 0.2094\n",
      "Epoch 33/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2063\n",
      "Epoch 00033: val_loss did not improve from 0.20936\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2063 - val_loss: 0.2095\n",
      "Epoch 34/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2062\n",
      "Epoch 00034: val_loss did not improve from 0.20936\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2062 - val_loss: 0.2099\n",
      "Epoch 35/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.2063- ETA: 0s - loss: 0.2\n",
      "Epoch 00035: val_loss did not improve from 0.20936\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2062 - val_loss: 0.2095\n",
      "Epoch 36/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.2062\n",
      "Epoch 00036: val_loss did not improve from 0.20936\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2062 - val_loss: 0.2094\n",
      "Epoch 37/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2062\n",
      "Epoch 00037: val_loss did not improve from 0.20936\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2062 - val_loss: 0.2097\n",
      "Epoch 38/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2061\n",
      "Epoch 00038: val_loss did not improve from 0.20936\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2061 - val_loss: 0.2096\n",
      "Epoch 39/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2062\n",
      "Epoch 00039: val_loss did not improve from 0.20936\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2062 - val_loss: 0.2097\n",
      "Epoch 40/1000\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.2058\n",
      "Epoch 00040: val_loss did not improve from 0.20936\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2058 - val_loss: 0.2094\n",
      "Epoch 41/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2056\n",
      "Epoch 00041: val_loss did not improve from 0.20936\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2058 - val_loss: 0.2096\n",
      "Epoch 42/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2059\n",
      "Epoch 00042: val_loss improved from 0.20936 to 0.20935, saving model to nn2_model_2.hdf5\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2058 - val_loss: 0.2094\n",
      "Epoch 43/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2058\n",
      "Epoch 00043: val_loss improved from 0.20935 to 0.20929, saving model to nn2_model_2.hdf5\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2058 - val_loss: 0.2093\n",
      "Epoch 44/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2057\n",
      "Epoch 00044: val_loss did not improve from 0.20929\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2058 - val_loss: 0.2094\n",
      "Epoch 45/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2057\n",
      "Epoch 00045: val_loss did not improve from 0.20929\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2057 - val_loss: 0.2093\n",
      "Epoch 46/1000\n",
      "163/168 [============================>.] - ETA: 0s - loss: 0.2058\n",
      "Epoch 00046: val_loss improved from 0.20929 to 0.20924, saving model to nn2_model_2.hdf5\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2058 - val_loss: 0.2092\n",
      "Epoch 47/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2058\n",
      "Epoch 00047: val_loss did not improve from 0.20924\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2058 - val_loss: 0.2093\n",
      "Epoch 48/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2057\n",
      "Epoch 00048: val_loss did not improve from 0.20924\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2057 - val_loss: 0.2094\n",
      "Epoch 49/1000\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.2057\n",
      "Epoch 00049: val_loss did not improve from 0.20924\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2057 - val_loss: 0.2094\n",
      "Epoch 50/1000\n",
      "163/168 [============================>.] - ETA: 0s - loss: 0.2058\n",
      "Epoch 00050: val_loss did not improve from 0.20924\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2057 - val_loss: 0.2094\n",
      "Epoch 51/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.2057\n",
      "Epoch 00051: val_loss did not improve from 0.20924\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2057 - val_loss: 0.2097\n",
      "Epoch 52/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.2057\n",
      "Epoch 00052: val_loss did not improve from 0.20924\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2056 - val_loss: 0.2095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/1000\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.2057\n",
      "Epoch 00053: val_loss did not improve from 0.20924\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2057 - val_loss: 0.2094\n",
      "Epoch 54/1000\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.2055\n",
      "Epoch 00054: val_loss did not improve from 0.20924\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2055 - val_loss: 0.2093\n",
      "Epoch 55/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2054\n",
      "Epoch 00055: val_loss did not improve from 0.20924\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2055 - val_loss: 0.2094\n",
      "Epoch 56/1000\n",
      "163/168 [============================>.] - ETA: 0s - loss: 0.2055\n",
      "Epoch 00056: val_loss did not improve from 0.20924\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2055 - val_loss: 0.2094\n",
      "Epoch 57/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.2055\n",
      "Epoch 00057: val_loss did not improve from 0.20924\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2055 - val_loss: 0.2095\n",
      "Epoch 58/1000\n",
      "163/168 [============================>.] - ETA: 0s - loss: 0.2055\n",
      "Epoch 00058: val_loss did not improve from 0.20924\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2055 - val_loss: 0.2093\n",
      "Epoch 59/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.2054\n",
      "Epoch 00059: val_loss did not improve from 0.20924\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2055 - val_loss: 0.2093\n",
      "Epoch 60/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2055- ETA: 0s - lo\n",
      "Epoch 00060: val_loss did not improve from 0.20924\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2055 - val_loss: 0.2094\n",
      "Epoch 61/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2054\n",
      "Epoch 00061: val_loss did not improve from 0.20924\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2054 - val_loss: 0.2094\n",
      "Epoch 62/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.2054\n",
      "Epoch 00062: val_loss did not improve from 0.20924\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2054 - val_loss: 0.2094\n",
      "Epoch 63/1000\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.2054\n",
      "Epoch 00063: val_loss did not improve from 0.20924\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2054 - val_loss: 0.2093\n",
      "Epoch 64/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.2055\n",
      "Epoch 00064: val_loss did not improve from 0.20924\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2054 - val_loss: 0.2093\n",
      "Epoch 65/1000\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.2054\n",
      "Epoch 00065: val_loss did not improve from 0.20924\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2054 - val_loss: 0.2093\n",
      "Epoch 66/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2055\n",
      "Epoch 00066: val_loss did not improve from 0.20924\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2054 - val_loss: 0.2093\n",
      "Fold 3 NN: 0.20924\n",
      "CV 4/5\n",
      "Epoch 1/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 10.4143\n",
      "Epoch 00001: val_loss improved from inf to 0.83098, saving model to nn2_model_3.hdf5\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 10.2156 - val_loss: 0.8310\n",
      "Epoch 2/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 1.4685\n",
      "Epoch 00002: val_loss improved from 0.83098 to 0.64050, saving model to nn2_model_3.hdf5\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 1.4626 - val_loss: 0.6405\n",
      "Epoch 3/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.8277\n",
      "Epoch 00003: val_loss did not improve from 0.64050\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.8259 - val_loss: 0.7478\n",
      "Epoch 4/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.7572\n",
      "Epoch 00004: val_loss did not improve from 0.64050\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.7573 - val_loss: 0.7021\n",
      "Epoch 5/1000\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.6967\n",
      "Epoch 00005: val_loss did not improve from 0.64050\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.6967 - val_loss: 0.7989\n",
      "Epoch 6/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.8484\n",
      "Epoch 00006: val_loss did not improve from 0.64050\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.8470 - val_loss: 0.8378\n",
      "Epoch 7/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.7093\n",
      "Epoch 00007: val_loss did not improve from 0.64050\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.7081 - val_loss: 0.6876\n",
      "Epoch 8/1000\n",
      "163/168 [============================>.] - ETA: 0s - loss: 0.7626- ETA: 1\n",
      "Epoch 00008: val_loss improved from 0.64050 to 0.63463, saving model to nn2_model_3.hdf5\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.7588 - val_loss: 0.6346\n",
      "Epoch 9/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.6762\n",
      "Epoch 00009: val_loss improved from 0.63463 to 0.48182, saving model to nn2_model_3.hdf5\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.6775 - val_loss: 0.4818\n",
      "Epoch 10/1000\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.5951\n",
      "Epoch 00010: val_loss did not improve from 0.48182\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.5951 - val_loss: 0.6828\n",
      "Epoch 11/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.5724\n",
      "Epoch 00011: val_loss improved from 0.48182 to 0.45606, saving model to nn2_model_3.hdf5\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.5718 - val_loss: 0.4561\n",
      "Epoch 12/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.5804\n",
      "Epoch 00012: val_loss improved from 0.45606 to 0.44277, saving model to nn2_model_3.hdf5\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.5807 - val_loss: 0.4428\n",
      "Epoch 13/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 1.0972\n",
      "Epoch 00013: val_loss did not improve from 0.44277\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 1.0902 - val_loss: 0.6087\n",
      "Epoch 14/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.4516\n",
      "Epoch 00014: val_loss improved from 0.44277 to 0.37511, saving model to nn2_model_3.hdf5\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.4527 - val_loss: 0.3751\n",
      "Epoch 15/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.4564\n",
      "Epoch 00015: val_loss did not improve from 0.37511\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.4557 - val_loss: 0.4373\n",
      "Epoch 16/1000\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.4114\n",
      "Epoch 00016: val_loss did not improve from 0.37511\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.4114 - val_loss: 0.4045\n",
      "Epoch 17/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.4073\n",
      "Epoch 00017: val_loss did not improve from 0.37511\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.4056 - val_loss: 0.4223\n",
      "Epoch 18/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.3882\n",
      "Epoch 00018: val_loss did not improve from 0.37511\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.3884 - val_loss: 0.3788\n",
      "Epoch 19/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2570\n",
      "Epoch 00019: val_loss improved from 0.37511 to 0.23369, saving model to nn2_model_3.hdf5\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2576 - val_loss: 0.2337\n",
      "Epoch 20/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2361\n",
      "Epoch 00020: val_loss improved from 0.23369 to 0.23242, saving model to nn2_model_3.hdf5\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2361 - val_loss: 0.2324\n",
      "Epoch 21/1000\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.2441\n",
      "Epoch 00021: val_loss improved from 0.23242 to 0.22704, saving model to nn2_model_3.hdf5\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2441 - val_loss: 0.2270\n",
      "Epoch 22/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2391\n",
      "Epoch 00022: val_loss did not improve from 0.22704\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2392 - val_loss: 0.3293\n",
      "Epoch 23/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2590\n",
      "Epoch 00023: val_loss did not improve from 0.22704\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2588 - val_loss: 0.2340\n",
      "Epoch 24/1000\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.2494\n",
      "Epoch 00024: val_loss did not improve from 0.22704\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2494 - val_loss: 0.3884\n",
      "Epoch 25/1000\n",
      "163/168 [============================>.] - ETA: 0s - loss: 0.2575\n",
      "Epoch 00025: val_loss did not improve from 0.22704\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2578 - val_loss: 0.2526\n",
      "Epoch 26/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.2617\n",
      "Epoch 00026: val_loss did not improve from 0.22704\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2612 - val_loss: 0.2563\n",
      "Epoch 27/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2682\n",
      "Epoch 00027: val_loss did not improve from 0.22704\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2683 - val_loss: 0.2592\n",
      "Epoch 28/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.2950\n",
      "Epoch 00028: val_loss did not improve from 0.22704\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2942 - val_loss: 0.2814\n",
      "Epoch 29/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2106\n",
      "Epoch 00029: val_loss improved from 0.22704 to 0.21903, saving model to nn2_model_3.hdf5\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2106 - val_loss: 0.2190\n",
      "Epoch 30/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2078\n",
      "Epoch 00030: val_loss did not improve from 0.21903\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2078 - val_loss: 0.2196\n",
      "Epoch 31/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2066\n",
      "Epoch 00031: val_loss improved from 0.21903 to 0.21723, saving model to nn2_model_3.hdf5\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2066 - val_loss: 0.2172\n",
      "Epoch 32/1000\n",
      "163/168 [============================>.] - ETA: 0s - loss: 0.2070\n",
      "Epoch 00032: val_loss did not improve from 0.21723\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2071 - val_loss: 0.2197\n",
      "Epoch 33/1000\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.2064\n",
      "Epoch 00033: val_loss improved from 0.21723 to 0.21658, saving model to nn2_model_3.hdf5\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2064 - val_loss: 0.2166\n",
      "Epoch 34/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2055\n",
      "Epoch 00034: val_loss did not improve from 0.21658\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2055 - val_loss: 0.2167\n",
      "Epoch 35/1000\n",
      "163/168 [============================>.] - ETA: 0s - loss: 0.2061\n",
      "Epoch 00035: val_loss did not improve from 0.21658\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2061 - val_loss: 0.2170\n",
      "Epoch 36/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2051\n",
      "Epoch 00036: val_loss did not improve from 0.21658\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2051 - val_loss: 0.2172\n",
      "Epoch 37/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2045\n",
      "Epoch 00037: val_loss did not improve from 0.21658\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2046 - val_loss: 0.2171\n",
      "Epoch 38/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2066\n",
      "Epoch 00038: val_loss did not improve from 0.21658\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2066 - val_loss: 0.2192\n",
      "Epoch 39/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2054\n",
      "Epoch 00039: val_loss did not improve from 0.21658\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2054 - val_loss: 0.2179\n",
      "Epoch 40/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2069\n",
      "Epoch 00040: val_loss did not improve from 0.21658\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2069 - val_loss: 0.2307\n",
      "Epoch 41/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2025\n",
      "Epoch 00041: val_loss improved from 0.21658 to 0.21633, saving model to nn2_model_3.hdf5\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2025 - val_loss: 0.2163\n",
      "Epoch 42/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2016\n",
      "Epoch 00042: val_loss improved from 0.21633 to 0.21615, saving model to nn2_model_3.hdf5\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2016 - val_loss: 0.2162\n",
      "Epoch 43/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.2016\n",
      "Epoch 00043: val_loss did not improve from 0.21615\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2016 - val_loss: 0.2163\n",
      "Epoch 44/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2015\n",
      "Epoch 00044: val_loss did not improve from 0.21615\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2015 - val_loss: 0.2166\n",
      "Epoch 45/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2016\n",
      "Epoch 00045: val_loss did not improve from 0.21615\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2015 - val_loss: 0.2167\n",
      "Epoch 46/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2015\n",
      "Epoch 00046: val_loss improved from 0.21615 to 0.21548, saving model to nn2_model_3.hdf5\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2015 - val_loss: 0.2155\n",
      "Epoch 47/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2015\n",
      "Epoch 00047: val_loss did not improve from 0.21548\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2015 - val_loss: 0.2170\n",
      "Epoch 48/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2020\n",
      "Epoch 00048: val_loss did not improve from 0.21548\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2020 - val_loss: 0.2187\n",
      "Epoch 49/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2021\n",
      "Epoch 00049: val_loss did not improve from 0.21548\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2022 - val_loss: 0.2172\n",
      "Epoch 50/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2015\n",
      "Epoch 00050: val_loss did not improve from 0.21548\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2014 - val_loss: 0.2169\n",
      "Epoch 51/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2014\n",
      "Epoch 00051: val_loss did not improve from 0.21548\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2013 - val_loss: 0.2168\n",
      "Epoch 52/1000\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.2017\n",
      "Epoch 00052: val_loss did not improve from 0.21548\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2017 - val_loss: 0.2168\n",
      "Epoch 53/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2013\n",
      "Epoch 00053: val_loss did not improve from 0.21548\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2013 - val_loss: 0.2189\n",
      "Epoch 54/1000\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.2001\n",
      "Epoch 00054: val_loss did not improve from 0.21548\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2001 - val_loss: 0.2168\n",
      "Epoch 55/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.1998\n",
      "Epoch 00055: val_loss did not improve from 0.21548\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.1999 - val_loss: 0.2167\n",
      "Epoch 56/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.1999- ETA: 0s - loss: 0\n",
      "Epoch 00056: val_loss did not improve from 0.21548\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.1999 - val_loss: 0.2167\n",
      "Epoch 57/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.1998\n",
      "Epoch 00057: val_loss did not improve from 0.21548\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.1998 - val_loss: 0.2169\n",
      "Epoch 58/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168/168 [==============================] - ETA: 0s - loss: 0.1998- ETA: 0s - loss: 0.199\n",
      "Epoch 00058: val_loss did not improve from 0.21548\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.1998 - val_loss: 0.2165\n",
      "Epoch 59/1000\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.1998\n",
      "Epoch 00059: val_loss did not improve from 0.21548\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.1998 - val_loss: 0.2167\n",
      "Epoch 60/1000\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.1998\n",
      "Epoch 00060: val_loss did not improve from 0.21548\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.1998 - val_loss: 0.2182\n",
      "Epoch 61/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.1994\n",
      "Epoch 00061: val_loss did not improve from 0.21548\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.1995 - val_loss: 0.2165\n",
      "Epoch 62/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.1993\n",
      "Epoch 00062: val_loss did not improve from 0.21548\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.1994 - val_loss: 0.2166\n",
      "Epoch 63/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.1994\n",
      "Epoch 00063: val_loss did not improve from 0.21548\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.1994 - val_loss: 0.2170\n",
      "Epoch 64/1000\n",
      "163/168 [============================>.] - ETA: 0s - loss: 0.1993\n",
      "Epoch 00064: val_loss did not improve from 0.21548\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.1994 - val_loss: 0.2168\n",
      "Epoch 65/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.1994- ET\n",
      "Epoch 00065: val_loss did not improve from 0.21548\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.1994 - val_loss: 0.2168\n",
      "Epoch 66/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.1994\n",
      "Epoch 00066: val_loss did not improve from 0.21548\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.1994 - val_loss: 0.2167\n",
      "Fold 4 NN: 0.21548\n",
      "CV 5/5\n",
      "Epoch 1/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 6.6469\n",
      "Epoch 00001: val_loss improved from inf to 1.02545, saving model to nn2_model_4.hdf5\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 6.6305 - val_loss: 1.0255\n",
      "Epoch 2/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 1.0535\n",
      "Epoch 00002: val_loss improved from 1.02545 to 1.00328, saving model to nn2_model_4.hdf5\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 1.0509 - val_loss: 1.0033\n",
      "Epoch 3/1000\n",
      "163/168 [============================>.] - ETA: 0s - loss: 0.8164\n",
      "Epoch 00003: val_loss improved from 1.00328 to 0.77523, saving model to nn2_model_4.hdf5\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.8143 - val_loss: 0.7752\n",
      "Epoch 4/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.8539\n",
      "Epoch 00004: val_loss did not improve from 0.77523\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.8540 - val_loss: 1.6555\n",
      "Epoch 5/1000\n",
      "163/168 [============================>.] - ETA: 0s - loss: 0.7440\n",
      "Epoch 00005: val_loss improved from 0.77523 to 0.69185, saving model to nn2_model_4.hdf5\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.7417 - val_loss: 0.6918\n",
      "Epoch 6/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.6627\n",
      "Epoch 00006: val_loss did not improve from 0.69185\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.6626 - val_loss: 0.7012\n",
      "Epoch 7/1000\n",
      "163/168 [============================>.] - ETA: 0s - loss: 0.5954\n",
      "Epoch 00007: val_loss did not improve from 0.69185\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.5981 - val_loss: 0.7890\n",
      "Epoch 8/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.5583\n",
      "Epoch 00008: val_loss improved from 0.69185 to 0.58264, saving model to nn2_model_4.hdf5\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.5615 - val_loss: 0.5826\n",
      "Epoch 9/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.5245\n",
      "Epoch 00009: val_loss did not improve from 0.58264\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.5248 - val_loss: 0.6648\n",
      "Epoch 10/1000\n",
      "163/168 [============================>.] - ETA: 0s - loss: 0.6312\n",
      "Epoch 00010: val_loss improved from 0.58264 to 0.37182, saving model to nn2_model_4.hdf5\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.6255 - val_loss: 0.3718\n",
      "Epoch 11/1000\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.4051\n",
      "Epoch 00011: val_loss did not improve from 0.37182\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.4051 - val_loss: 0.5088\n",
      "Epoch 12/1000\n",
      "163/168 [============================>.] - ETA: 0s - loss: 0.3510\n",
      "Epoch 00012: val_loss improved from 0.37182 to 0.24439, saving model to nn2_model_4.hdf5\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.3484 - val_loss: 0.2444\n",
      "Epoch 13/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.2422\n",
      "Epoch 00013: val_loss did not improve from 0.24439\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2418 - val_loss: 0.2526\n",
      "Epoch 14/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2296\n",
      "Epoch 00014: val_loss improved from 0.24439 to 0.22152, saving model to nn2_model_4.hdf5\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2296 - val_loss: 0.2215\n",
      "Epoch 15/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2563\n",
      "Epoch 00015: val_loss did not improve from 0.22152\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2618 - val_loss: 0.4543\n",
      "Epoch 16/1000\n",
      "168/168 [==============================] - ETA: 0s - loss: 1.2653\n",
      "Epoch 00016: val_loss did not improve from 0.22152\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 1.2653 - val_loss: 0.3559\n",
      "Epoch 17/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.3020\n",
      "Epoch 00017: val_loss did not improve from 0.22152\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.3008 - val_loss: 0.2479\n",
      "Epoch 18/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2308\n",
      "Epoch 00018: val_loss did not improve from 0.22152\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2308 - val_loss: 0.2473\n",
      "Epoch 19/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2392\n",
      "Epoch 00019: val_loss did not improve from 0.22152\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2391 - val_loss: 0.2414\n",
      "Epoch 20/1000\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.2346\n",
      "Epoch 00020: val_loss did not improve from 0.22152\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2346 - val_loss: 0.2271\n",
      "Epoch 21/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2274\n",
      "Epoch 00021: val_loss did not improve from 0.22152\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2274 - val_loss: 0.2341\n",
      "Epoch 22/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.2098\n",
      "Epoch 00022: val_loss improved from 0.22152 to 0.21532, saving model to nn2_model_4.hdf5\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2097 - val_loss: 0.2153\n",
      "Epoch 23/1000\n",
      "162/168 [===========================>..] - ETA: 0s - loss: 0.2089\n",
      "Epoch 00023: val_loss did not improve from 0.21532\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2089 - val_loss: 0.2156\n",
      "Epoch 24/1000\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.2092\n",
      "Epoch 00024: val_loss improved from 0.21532 to 0.21510, saving model to nn2_model_4.hdf5\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2092 - val_loss: 0.2151\n",
      "Epoch 25/1000\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.2082\n",
      "Epoch 00025: val_loss did not improve from 0.21510\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2082 - val_loss: 0.2164\n",
      "Epoch 26/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2078\n",
      "Epoch 00026: val_loss did not improve from 0.21510\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2078 - val_loss: 0.2168\n",
      "Epoch 27/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2084\n",
      "Epoch 00027: val_loss did not improve from 0.21510\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2085 - val_loss: 0.2168\n",
      "Epoch 28/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.2084\n",
      "Epoch 00028: val_loss did not improve from 0.21510\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2084 - val_loss: 0.2199\n",
      "Epoch 29/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2077\n",
      "Epoch 00029: val_loss did not improve from 0.21510\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2076 - val_loss: 0.2156\n",
      "Epoch 30/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2076\n",
      "Epoch 00030: val_loss did not improve from 0.21510\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2077 - val_loss: 0.2158\n",
      "Epoch 31/1000\n",
      "163/168 [============================>.] - ETA: 0s - loss: 0.2081\n",
      "Epoch 00031: val_loss did not improve from 0.21510\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2081 - val_loss: 0.2152\n",
      "Epoch 32/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2049\n",
      "Epoch 00032: val_loss improved from 0.21510 to 0.21483, saving model to nn2_model_4.hdf5\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2049 - val_loss: 0.2148\n",
      "Epoch 33/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2047\n",
      "Epoch 00033: val_loss improved from 0.21483 to 0.21447, saving model to nn2_model_4.hdf5\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2047 - val_loss: 0.2145\n",
      "Epoch 34/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2047\n",
      "Epoch 00034: val_loss improved from 0.21447 to 0.21441, saving model to nn2_model_4.hdf5\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2047 - val_loss: 0.2144\n",
      "Epoch 35/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2047\n",
      "Epoch 00035: val_loss improved from 0.21441 to 0.21409, saving model to nn2_model_4.hdf5\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2047 - val_loss: 0.2141\n",
      "Epoch 36/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.2047- ETA: 0s -\n",
      "Epoch 00036: val_loss did not improve from 0.21409\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2046 - val_loss: 0.2144\n",
      "Epoch 37/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2045\n",
      "Epoch 00037: val_loss did not improve from 0.21409\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2045 - val_loss: 0.2152\n",
      "Epoch 38/1000\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.2045\n",
      "Epoch 00038: val_loss did not improve from 0.21409\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2045 - val_loss: 0.2145\n",
      "Epoch 39/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2044\n",
      "Epoch 00039: val_loss improved from 0.21409 to 0.21394, saving model to nn2_model_4.hdf5\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2044 - val_loss: 0.2139\n",
      "Epoch 40/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2046\n",
      "Epoch 00040: val_loss improved from 0.21394 to 0.21371, saving model to nn2_model_4.hdf5\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2046 - val_loss: 0.2137\n",
      "Epoch 41/1000\n",
      "163/168 [============================>.] - ETA: 0s - loss: 0.2045\n",
      "Epoch 00041: val_loss did not improve from 0.21371\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2045 - val_loss: 0.2158\n",
      "Epoch 42/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2041\n",
      "Epoch 00042: val_loss did not improve from 0.21371\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2041 - val_loss: 0.2141\n",
      "Epoch 43/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2048\n",
      "Epoch 00043: val_loss did not improve from 0.21371\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2048 - val_loss: 0.2174\n",
      "Epoch 44/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2051\n",
      "Epoch 00044: val_loss did not improve from 0.21371\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2051 - val_loss: 0.2165\n",
      "Epoch 45/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2044\n",
      "Epoch 00045: val_loss did not improve from 0.21371\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2044 - val_loss: 0.2141\n",
      "Epoch 46/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2053\n",
      "Epoch 00046: val_loss did not improve from 0.21371\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2053 - val_loss: 0.2161\n",
      "Epoch 47/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2043\n",
      "Epoch 00047: val_loss did not improve from 0.21371\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2043 - val_loss: 0.2149\n",
      "Epoch 48/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2029\n",
      "Epoch 00048: val_loss did not improve from 0.21371\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2029 - val_loss: 0.2137\n",
      "Epoch 49/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2028\n",
      "Epoch 00049: val_loss did not improve from 0.21371\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2028 - val_loss: 0.2141\n",
      "Epoch 50/1000\n",
      "162/168 [===========================>..] - ETA: 0s - loss: 0.2028\n",
      "Epoch 00050: val_loss did not improve from 0.21371\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2028 - val_loss: 0.2138\n",
      "Epoch 51/1000\n",
      "163/168 [============================>.] - ETA: 0s - loss: 0.2027\n",
      "Epoch 00051: val_loss improved from 0.21371 to 0.21346, saving model to nn2_model_4.hdf5\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2027 - val_loss: 0.2135\n",
      "Epoch 52/1000\n",
      "166/168 [============================>.] - ETA: 0s - loss: 0.2027\n",
      "Epoch 00052: val_loss did not improve from 0.21346\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2027 - val_loss: 0.2136\n",
      "Epoch 53/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2027\n",
      "Epoch 00053: val_loss did not improve from 0.21346\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2027 - val_loss: 0.2144\n",
      "Epoch 54/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.2027\n",
      "Epoch 00054: val_loss did not improve from 0.21346\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2027 - val_loss: 0.2138\n",
      "Epoch 55/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2027\n",
      "Epoch 00055: val_loss did not improve from 0.21346\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2027 - val_loss: 0.2138\n",
      "Epoch 56/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2027\n",
      "Epoch 00056: val_loss did not improve from 0.21346\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2026 - val_loss: 0.2145\n",
      "Epoch 57/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2026\n",
      "Epoch 00057: val_loss did not improve from 0.21346\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2026 - val_loss: 0.2149\n",
      "Epoch 58/1000\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.2026\n",
      "Epoch 00058: val_loss did not improve from 0.21346\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2026 - val_loss: 0.2140\n",
      "Epoch 59/1000\n",
      "163/168 [============================>.] - ETA: 0s - loss: 0.2022\n",
      "Epoch 00059: val_loss did not improve from 0.21346\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2022 - val_loss: 0.2136\n",
      "Epoch 60/1000\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.2022\n",
      "Epoch 00060: val_loss did not improve from 0.21346\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2022 - val_loss: 0.2136\n",
      "Epoch 61/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2021\n",
      "Epoch 00061: val_loss did not improve from 0.21346\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2021 - val_loss: 0.2137\n",
      "Epoch 62/1000\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.2021\n",
      "Epoch 00062: val_loss did not improve from 0.21346\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2021 - val_loss: 0.2135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2022\n",
      "Epoch 00063: val_loss did not improve from 0.21346\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2021 - val_loss: 0.2137\n",
      "Epoch 64/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2022\n",
      "Epoch 00064: val_loss did not improve from 0.21346\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2022 - val_loss: 0.2142\n",
      "Epoch 65/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.2021\n",
      "Epoch 00065: val_loss did not improve from 0.21346\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2021 - val_loss: 0.2139\n",
      "Epoch 66/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2020\n",
      "Epoch 00066: val_loss did not improve from 0.21346\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2020 - val_loss: 0.2136\n",
      "Epoch 67/1000\n",
      "163/168 [============================>.] - ETA: 0s - loss: 0.2021\n",
      "Epoch 00067: val_loss did not improve from 0.21346\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2020 - val_loss: 0.2137\n",
      "Epoch 68/1000\n",
      "164/168 [============================>.] - ETA: 0s - loss: 0.2019\n",
      "Epoch 00068: val_loss did not improve from 0.21346\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2020 - val_loss: 0.2136\n",
      "Epoch 69/1000\n",
      "163/168 [============================>.] - ETA: 0s - loss: 0.2020\n",
      "Epoch 00069: val_loss did not improve from 0.21346\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2020 - val_loss: 0.2136\n",
      "Epoch 70/1000\n",
      "165/168 [============================>.] - ETA: 0s - loss: 0.2020\n",
      "Epoch 00070: val_loss did not improve from 0.21346\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2020 - val_loss: 0.2138\n",
      "Epoch 71/1000\n",
      "167/168 [============================>.] - ETA: 0s - loss: 0.2020\n",
      "Epoch 00071: val_loss did not improve from 0.21346\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2020 - val_loss: 0.2137\n",
      "Fold 5 NN: 0.21346\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import seed\n",
    "seed(41)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(41)\n",
    "from tensorflow import keras\n",
    "\n",
    "target_name='target'\n",
    "scores_folds = {}\n",
    "model_name = 'NN'\n",
    "pred_name = 'pred_{}'.format(model_name)\n",
    "\n",
    "n_folds = 5\n",
    "kf = model_selection.KFold(n_splits=n_folds, shuffle=True, random_state=2021)\n",
    "scores_folds[model_name] = []\n",
    "counter = 1\n",
    "\n",
    "features_to_consider = list(train1)\n",
    "\n",
    "features_to_consider.remove('time_id')\n",
    "features_to_consider.remove('target')\n",
    "try:\n",
    "    features_to_consider.remove('pred_NN')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "train1[features_to_consider] = train1[features_to_consider].fillna(train1[features_to_consider].mean())\n",
    "test1[features_to_consider] = test1[features_to_consider].fillna(train1[features_to_consider].mean())\n",
    "\n",
    "train1[pred_name] = 0\n",
    "test1[target_name] = 0\n",
    "test_predictions_nn1 = np.zeros(test_nn.shape[0])\n",
    "\n",
    "for n_count in range(n_folds):\n",
    "    print('CV {}/{}'.format(counter, n_folds))\n",
    "    \n",
    "    indexes = np.arange(nfolds).astype(int)    \n",
    "    indexes = np.delete(indexes,obj=n_count, axis=0) \n",
    "    \n",
    "    indexes = np.r_[values[indexes[0]],values[indexes[1]],values[indexes[2]],values[indexes[3]]]\n",
    "    \n",
    "    X_train = train1.loc[train1.time_id.isin(indexes), features_to_consider]\n",
    "    y_train = train1.loc[train1.time_id.isin(indexes), target_name]\n",
    "    X_test = train1.loc[train1.time_id.isin(values[n_count]), features_to_consider]\n",
    "    y_test = train1.loc[train1.time_id.isin(values[n_count]), target_name]\n",
    "    \n",
    "    #############################################################################################\n",
    "    # NN\n",
    "    #############################################################################################\n",
    "    \n",
    "    model = base_model()\n",
    "    \n",
    "    model.compile(\n",
    "        keras.optimizers.Adam(learning_rate=0.006),\n",
    "        loss=root_mean_squared_per_error\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        features_to_consider.remove('stock_id')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    num_data = X_train[features_to_consider]\n",
    "    \n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))         \n",
    "    num_data = scaler.fit_transform(num_data.values)    \n",
    "    \n",
    "    cat_data = X_train['stock_id']    \n",
    "    target =  y_train\n",
    "    \n",
    "    num_data_test = X_test[features_to_consider]\n",
    "    num_data_test = scaler.transform(num_data_test.values)\n",
    "    cat_data_test = X_test['stock_id']\n",
    "    filepath = 'nn2_model_'+str(n_count)+'.hdf5'\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=filepath, \n",
    "                             monitor='val_loss',\n",
    "                             verbose=1, \n",
    "                             save_best_only=True,\n",
    "                             mode='min')\n",
    "    model.fit([cat_data, num_data], \n",
    "              target,               \n",
    "              batch_size=2048,\n",
    "              epochs=1000,\n",
    "              validation_data=([cat_data_test, num_data_test], y_test),\n",
    "              callbacks=[es, plateau,checkpoint],\n",
    "              validation_batch_size=len(y_test),\n",
    "              shuffle=True,\n",
    "             verbose = 1)\n",
    "\n",
    "    preds = model.predict([cat_data_test, num_data_test]).reshape(1,-1)[0]\n",
    "    \n",
    "    score = round(rmspe(y_true = y_test, y_pred = preds),5)\n",
    "    print('Fold {} {}: {}'.format(counter, model_name, score))\n",
    "    scores_folds[model_name].append(score)\n",
    "    \n",
    "    tt =scaler.transform(test_nn[features_to_consider].values)\n",
    "    #test_nn[target_name] += model.predict([test_nn['stock_id'], tt]).reshape(1,-1)[0].clip(0,1e10)\n",
    "    test_predictions_nn1 += model.predict([test1['stock_id'], tt]).reshape(1,-1)[0].clip(0,1e10)/n_folds\n",
    "    #test[target_name] += model.predict([test['stock_id'], test[features_to_consider]]).reshape(1,-1)[0].clip(0,1e10)\n",
    "       \n",
    "    counter += 1\n",
    "    features_to_consider.append('stock_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-16T17:30:22.526524Z",
     "iopub.status.busy": "2021-09-16T17:30:22.526250Z",
     "iopub.status.idle": "2021-09-16T17:30:22.550376Z",
     "shell.execute_reply": "2021-09-16T17:30:22.549336Z",
     "shell.execute_reply.started": "2021-09-16T17:30:22.526497Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iqr_p21</th>\n",
       "      <th>iqr_p22</th>\n",
       "      <th>iqr_p23</th>\n",
       "      <th>iqr_p24</th>\n",
       "      <th>mean_return1</th>\n",
       "      <th>mean_return2</th>\n",
       "      <th>mean_return3</th>\n",
       "      <th>mean_return4</th>\n",
       "      <th>iqr_p2</th>\n",
       "      <th>mean_return</th>\n",
       "      <th>...</th>\n",
       "      <th>bid_ask_spread_sum_1c1</th>\n",
       "      <th>bid_ask_spread_sum_3c1</th>\n",
       "      <th>bid_ask_spread_sum_4c1</th>\n",
       "      <th>bid_ask_spread_sum_6c1</th>\n",
       "      <th>size_tau2_0c1</th>\n",
       "      <th>size_tau2_1c1</th>\n",
       "      <th>size_tau2_3c1</th>\n",
       "      <th>size_tau2_4c1</th>\n",
       "      <th>size_tau2_6c1</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.999618</td>\n",
       "      <td>-1.417362</td>\n",
       "      <td>-1.467298</td>\n",
       "      <td>-2.473543</td>\n",
       "      <td>3.358185</td>\n",
       "      <td>3.308578</td>\n",
       "      <td>-3.145525</td>\n",
       "      <td>-1.345737</td>\n",
       "      <td>-2.913011</td>\n",
       "      <td>-1.938082</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.199337</td>\n",
       "      <td>0.035373</td>\n",
       "      <td>0.221821</td>\n",
       "      <td>0.431936</td>\n",
       "      <td>-0.086921</td>\n",
       "      <td>3.161571</td>\n",
       "      <td>0.590543</td>\n",
       "      <td>-0.161021</td>\n",
       "      <td>-0.482473</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 257 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    iqr_p21   iqr_p22   iqr_p23   iqr_p24  mean_return1  mean_return2  \\\n",
       "0 -0.999618 -1.417362 -1.467298 -2.473543      3.358185      3.308578   \n",
       "\n",
       "   mean_return3  mean_return4    iqr_p2  mean_return  ...  \\\n",
       "0     -3.145525     -1.345737 -2.913011    -1.938082  ...   \n",
       "\n",
       "   bid_ask_spread_sum_1c1  bid_ask_spread_sum_3c1  bid_ask_spread_sum_4c1  \\\n",
       "0               -5.199337                0.035373                0.221821   \n",
       "\n",
       "   bid_ask_spread_sum_6c1  size_tau2_0c1  size_tau2_1c1  size_tau2_3c1  \\\n",
       "0                0.431936      -0.086921       3.161571       0.590543   \n",
       "\n",
       "   size_tau2_4c1  size_tau2_6c1  target  \n",
       "0      -0.161021      -0.482473       0  \n",
       "\n",
       "[1 rows x 257 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-16T17:30:23.059753Z",
     "iopub.status.busy": "2021-09-16T17:30:23.059421Z",
     "iopub.status.idle": "2021-09-16T17:30:23.079544Z",
     "shell.execute_reply": "2021-09-16T17:30:23.078564Z",
     "shell.execute_reply.started": "2021-09-16T17:30:23.059721Z"
    }
   },
   "outputs": [],
   "source": [
    "test=pd.read_csv(data_dir+\"test.csv\")\n",
    "a=test_predictions_nn*0.60+predictions_lgb2*0.40\n",
    "b=test_predictions_nn1*0.55+predictions_lgb1*0.45\n",
    "test[target_name] = (a+b)/2\n",
    "\n",
    "display(test[['row_id', target_name]].head(3))\n",
    "test[['row_id', target_name]].to_csv('submission.csv',index = False)\n",
    "#test[['row_id', target_name]].to_csv('submission.csv',index = False)\n",
    "#kmeans N=5 [0.2101, 0.21399, 0.20923, 0.21398, 0.21175]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-01T04:49:10.045134Z",
     "iopub.status.busy": "2021-09-01T04:49:10.044636Z",
     "iopub.status.idle": "2021-09-01T04:49:10.055087Z",
     "shell.execute_reply": "2021-09-01T04:49:10.05377Z",
     "shell.execute_reply.started": "2021-09-01T04:49:10.045089Z"
    }
   },
   "outputs": [],
   "source": [
    "temp2.to_pickle('train_cu.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
