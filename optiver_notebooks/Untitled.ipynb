{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU, Conv1D\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.layers import Input, Embedding, Dense, Conv2D, MaxPool2D, concatenate\n",
    "from keras.layers import Reshape, Flatten, Concatenate, Dropout, SpatialDropout1D\n",
    "from keras.layers import BatchNormalization, InputSpec, add\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model, load_model\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers, activations\n",
    "from keras.callbacks import Callback, EarlyStopping, ModelCheckpoint\n",
    "from keras.utils import Sequence\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "train_df = pd.read_csv('train_2.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lb = LabelBinarizer()\n",
    "# y_train = lb.fit_transform(train_df['Label'])\n",
    "train_df['len'] = train_df['len']/600\n",
    "train_df['len_last_min'] = train_df['len_last_min']/60\n",
    "train_df['len_last_5'] = train_df['len_last_5']/300\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_df.target_x.values\n",
    "X = train_df.drop(['target_x','row_id','time_id','stock_id','target_y'],axis = 1)\n",
    "# X['target_y'].values[X['target_y'] > 0.009] = 0.0031"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>len</th>\n",
       "      <th>len_last_min</th>\n",
       "      <th>len_last_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.004115</td>\n",
       "      <td>0.501667</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.463333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001268</td>\n",
       "      <td>0.331667</td>\n",
       "      <td>0.366667</td>\n",
       "      <td>0.383333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.002719</td>\n",
       "      <td>0.311667</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.226667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.002625</td>\n",
       "      <td>0.198333</td>\n",
       "      <td>0.116667</td>\n",
       "      <td>0.176667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001901</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>0.183333</td>\n",
       "      <td>0.296667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428927</th>\n",
       "      <td>0.003338</td>\n",
       "      <td>0.515000</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.473333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428928</th>\n",
       "      <td>0.003347</td>\n",
       "      <td>0.370000</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.350000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428929</th>\n",
       "      <td>0.003695</td>\n",
       "      <td>0.425000</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.556667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428930</th>\n",
       "      <td>0.003375</td>\n",
       "      <td>0.663333</td>\n",
       "      <td>0.616667</td>\n",
       "      <td>0.636667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428931</th>\n",
       "      <td>0.001984</td>\n",
       "      <td>0.360000</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.320000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>428932 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          target       len  len_last_min  len_last_5\n",
       "0       0.004115  0.501667      0.466667    0.463333\n",
       "1       0.001268  0.331667      0.366667    0.383333\n",
       "2       0.002719  0.311667      0.133333    0.226667\n",
       "3       0.002625  0.198333      0.116667    0.176667\n",
       "4       0.001901  0.291667      0.183333    0.296667\n",
       "...          ...       ...           ...         ...\n",
       "428927  0.003338  0.515000      0.533333    0.473333\n",
       "428928  0.003347  0.370000      0.533333    0.350000\n",
       "428929  0.003695  0.425000      0.533333    0.556667\n",
       "428930  0.003375  0.663333      0.616667    0.636667\n",
       "428931  0.001984  0.360000      0.450000    0.320000\n",
       "\n",
       "[428932 rows x 4 columns]"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale= 100\n",
    "X[['target']] = X[['target']]*scale\n",
    "y = y*scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (X['target_y']>0.1).any()\n",
    "from tensorflow import keras\n",
    "layers = keras.layers\n",
    "models = keras.models\n",
    "# Build the model\n",
    "from keras import backend as K \n",
    "\n",
    "# Do some code, e.g. train and save model\n",
    "\n",
    "# K.clear_session()\n",
    "# seed_value= 0\n",
    "\n",
    "# 1. Set the `PYTHONHASHSEED` environment variable at a fixed value\n",
    "import os\n",
    "# os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "# 2. Set the `python` built-in pseudo-random generator at a fixed value\n",
    "import random\n",
    "# random.seed(seed_value)\n",
    "\n",
    "# 3. Set the `numpy` pseudo-random generator at a fixed value\n",
    "import numpy as np\n",
    "# np.random.seed(seed_value)\n",
    "\n",
    "# 4. Set the `tensorflow` pseudo-random generator at a fixed value\n",
    "import tensorflow as tf\n",
    "# tf.random.set_seed(seed_value)\n",
    "# def rmspe(y_true, y_pred):\n",
    "#     '''\n",
    "#     Compute Root Mean Square Percentage Error between two arrays.\n",
    "#     '''\n",
    "#     loss = np.sqrt(np.mean(np.square(((y_true - y_pred) / y_true)), axis=0))\n",
    "\n",
    "#     return loss\n",
    "from keras import backend as K\n",
    "def rmspe(y_true, y_pred):\n",
    "    sume = K.sqrt(K.mean(K.square( (y_true - y_pred) /\n",
    "          K.clip(K.abs(y_true),K.epsilon(),None) ), axis=-1) )\n",
    "    return sume\n",
    "def build_base_model():\n",
    "    K.clear_session()\n",
    "    seed_value = 0\n",
    "    tf.random.set_seed(seed_value)\n",
    "    os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    base_model = models.Sequential()\n",
    "    base_model.add(layers.Dense(5, input_shape=(4,)))\n",
    "    # model.add(layers.BatchNormalization())\n",
    "    base_model.add(layers.Activation('linear'))\n",
    "#     base_model.add(layers.Dropout(0.2))\n",
    "#     base_model.add(layers.Dense(112))\n",
    "#     base_model.add(layers.BatchNormalization())\n",
    "#     base_model.add(layers.Activation('linear'))\n",
    "#     base_model.add(layers.Dense(11))\n",
    "#     base_model.add(layers.BatchNormalization())\n",
    "#     base_model.add(layers.Activation('linear'))\n",
    "    # model.add(layers.Dense(512))\n",
    "    # # model.add(layers.BatchNormalization())\n",
    "    # model.add(layers.Activation('relu'))\n",
    "    # model.add(layers.Dense(128))\n",
    "    # # model.add(layers.BatchNormalization())\n",
    "    # model.add(layers.Activation('relu'))\n",
    "\n",
    "    # model.add(layers.Dropout(drop_ratio))\n",
    "    base_model.add(layers.Dense(1))\n",
    "    base_model.add(layers.Activation('linear'))\n",
    "\n",
    "    base_model.compile(loss='mse',\n",
    "                  optimizer='adam',\n",
    "                  metrics=[rmspe])\n",
    "    return base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1e-07"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.epsilon()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = X.columns.tolist()\n",
    "col_target = 'target_x'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler, Callback, ReduceLROnPlateau\n",
    "file_path = \"weights_base.best.hdf5\"\n",
    "def benchmark():\n",
    "    print('_' * 80)\n",
    "    print(\"Training: \")\n",
    "#     print(clf)\n",
    "#     t0 = time()\n",
    "    skf = KFold(n_splits=5, random_state=0)\n",
    "    models, preds, scores = [], [],[]\n",
    "#     vectorizer = vect(max_df = 0.5)\n",
    "    i = 0\n",
    "    for train, test in skf.split(X, y):\n",
    "#     print(train, test)\n",
    "#     clf = LogisticRegression(penalty='l1')\n",
    "#         clf.fit(vectorizer.transform(), data_train.Label.loc[data_train.index.intersection(train)])\n",
    "#         K.clear_session()\n",
    "        clf = build_base_model()\n",
    "        X_train = train_df[features].loc[train_df.index.intersection(train)].values\n",
    "        X_val = train_df[features].loc[train_df.index.intersection(test)].values\n",
    "        y_train = train_df[col_target].loc[train_df.index.intersection(train)].values\n",
    "        y_val = train_df[col_target].loc[train_df.index.intersection(test)].values\n",
    "#         X_train = vect.transform(X_train).toarray()\n",
    "#         X_val = vect.transform(X_val).toarray()\n",
    "#         X_test = vect.transform(test_df.Text).toarray()\n",
    "        print(str(i)*80)\n",
    "        i = i+1\n",
    "        \n",
    "        \n",
    "        clf.fit(X_train, y_train,\n",
    "                    batch_size=100,\n",
    "                    epochs=10,\n",
    "                    verbose=1,\n",
    "                   validation_data = (X_val,y_val),\n",
    "                   callbacks=[\n",
    "#               RocAucEvaluation(verbose=True),\n",
    "              ModelCheckpoint(file_path,    monitor='val_loss', mode='min', save_best_only=True),\n",
    "              EarlyStopping(patience=10,    monitor=\"val_loss\", mode=\"min\"),\n",
    "              ReduceLROnPlateau(patience=4, monitor='val_loss', mode='min', cooldown=2, min_lr=1e-7, factor=0.3)])\n",
    "#         preds.append(clf.predict(X_test))\n",
    "        models.append(clf)\n",
    "        scores.append(clf.evaluate(X_val,y_val))\n",
    "#         y_pred = clf.predict(X)\n",
    "#         print(rmspe(y/scale,y_pred/scale))\n",
    "#         coefs.append(clf.coef_[0])\n",
    "#         clf.fit(X_train, y_train)\n",
    "#     train_time = time() - t0\n",
    "#     print(\"train time: %0.3fs\" % train_time)\n",
    "\n",
    "#     t0 = time()\n",
    "#     pred = clf.predict(X_test)\n",
    "#     test_time = time() - t0\n",
    "#     print(\"test time:  %0.3fs\" % test_time)\n",
    "#     pred = np.mean(preds,axis = 0)\n",
    "#     score = metrics.accuracy_score(data_test.Label, pred)\n",
    "#     print(\"accuracy:   %0.3f\" % score)\n",
    "    return models, scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "Training: \n",
      "00000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "Epoch 1/10\n",
      "3432/3432 [==============================] - 4s 1ms/step - loss: 5.3648e-04 - rmspe: 4.8799 - val_loss: 5.1816e-05 - val_rmspe: 1.8397\n",
      "Epoch 2/10\n",
      "3432/3432 [==============================] - 4s 1ms/step - loss: 1.2802e-05 - rmspe: 0.8204 - val_loss: 2.4913e-06 - val_rmspe: 0.2547\n",
      "Epoch 3/10\n",
      "3432/3432 [==============================] - 4s 1ms/step - loss: 2.3322e-06 - rmspe: 0.3051 - val_loss: 3.0911e-06 - val_rmspe: 0.2473\n",
      "Epoch 4/10\n",
      "3432/3432 [==============================] - 4s 1ms/step - loss: 2.4341e-06 - rmspe: 0.3231 - val_loss: 2.8702e-06 - val_rmspe: 0.2297\n",
      "Epoch 5/10\n",
      "3432/3432 [==============================] - 4s 1ms/step - loss: 2.3838e-06 - rmspe: 0.3137 - val_loss: 2.6411e-06 - val_rmspe: 0.3484\n",
      "Epoch 6/10\n",
      "3432/3432 [==============================] - 4s 1ms/step - loss: 2.1536e-06 - rmspe: 0.2724 - val_loss: 2.4582e-06 - val_rmspe: 0.2542\n",
      "Epoch 7/10\n",
      "3432/3432 [==============================] - 4s 1ms/step - loss: 2.1621e-06 - rmspe: 0.2746 - val_loss: 2.6522e-06 - val_rmspe: 0.2165\n",
      "Epoch 8/10\n",
      "3432/3432 [==============================] - 4s 1ms/step - loss: 2.1514e-06 - rmspe: 0.2723 - val_loss: 2.5090e-06 - val_rmspe: 0.2316\n",
      "Epoch 9/10\n",
      "3432/3432 [==============================] - 4s 1ms/step - loss: 2.1662e-06 - rmspe: 0.2753 - val_loss: 2.4743e-06 - val_rmspe: 0.2399\n",
      "Epoch 10/10\n",
      "3432/3432 [==============================] - 4s 1ms/step - loss: 2.1587e-06 - rmspe: 0.2731 - val_loss: 2.4612e-06 - val_rmspe: 0.2589\n",
      "2681/2681 [==============================] - 2s 806us/step - loss: 2.4612e-06 - rmspe: 0.2589\n",
      "11111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
      "Epoch 1/10\n",
      "3432/3432 [==============================] - 4s 1ms/step - loss: 5.1373e-04 - rmspe: 4.6128 - val_loss: 3.5648e-05 - val_rmspe: 1.8472\n",
      "Epoch 2/10\n",
      "3432/3432 [==============================] - 4s 1ms/step - loss: 1.0719e-05 - rmspe: 0.7122 - val_loss: 2.2329e-06 - val_rmspe: 0.3550\n",
      "Epoch 3/10\n",
      "3432/3432 [==============================] - 4s 1ms/step - loss: 2.4040e-06 - rmspe: 0.2912 - val_loss: 2.6214e-06 - val_rmspe: 0.3110\n",
      "Epoch 4/10\n",
      "3432/3432 [==============================] - 4s 1ms/step - loss: 2.5042e-06 - rmspe: 0.3063 - val_loss: 2.2393e-06 - val_rmspe: 0.2189\n",
      "Epoch 5/10\n",
      "3432/3432 [==============================] - 4s 1ms/step - loss: 2.4836e-06 - rmspe: 0.3035 - val_loss: 2.5832e-06 - val_rmspe: 0.4986\n",
      "Epoch 6/10\n",
      "3432/3432 [==============================] - 4s 1ms/step - loss: 2.2257e-06 - rmspe: 0.2628 - val_loss: 2.4045e-06 - val_rmspe: 0.4346\n",
      "Epoch 7/10\n",
      "3432/3432 [==============================] - 4s 1ms/step - loss: 2.2422e-06 - rmspe: 0.2663 - val_loss: 2.1553e-06 - val_rmspe: 0.2937\n",
      "Epoch 8/10\n",
      "3432/3432 [==============================] - 4s 1ms/step - loss: 2.2266e-06 - rmspe: 0.2631 - val_loss: 2.2002e-06 - val_rmspe: 0.3508\n",
      "Epoch 9/10\n",
      "3432/3432 [==============================] - 4s 1ms/step - loss: 2.2479e-06 - rmspe: 0.2669 - val_loss: 2.1504e-06 - val_rmspe: 0.2713\n",
      "Epoch 10/10\n",
      "3432/3432 [==============================] - 5s 2ms/step - loss: 2.2361e-06 - rmspe: 0.2647 - val_loss: 2.2300e-06 - val_rmspe: 0.3656\n",
      "   1/2681 [..............................] - ETA: 0s - loss: 1.9042e-06 - rmspe: 0.1955WARNING:tensorflow:Callbacks method `on_test_batch_begin` is slow compared to the batch time (batch time: 0.0000s vs `on_test_batch_begin` time: 0.0010s). Check your callbacks.\n",
      "2681/2681 [==============================] - 2s 820us/step - loss: 2.2300e-06 - rmspe: 0.3656\n",
      "22222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
      "Epoch 1/10\n",
      "3432/3432 [==============================] - 4s 1ms/step - loss: 5.3259e-04 - rmspe: 4.7242 - val_loss: 4.0926e-05 - val_rmspe: 1.9005\n",
      "Epoch 2/10\n",
      "3432/3432 [==============================] - 5s 1ms/step - loss: 1.2169e-05 - rmspe: 0.7671 - val_loss: 1.9005e-06 - val_rmspe: 0.2162\n",
      "Epoch 3/10\n",
      "3432/3432 [==============================] - 4s 1ms/step - loss: 2.5175e-06 - rmspe: 0.3090 - val_loss: 2.2271e-06 - val_rmspe: 0.2492\n",
      "Epoch 4/10\n",
      "3432/3432 [==============================] - 4s 1ms/step - loss: 2.5738e-06 - rmspe: 0.3196 - val_loss: 2.1058e-06 - val_rmspe: 0.2333\n",
      "Epoch 5/10\n",
      "3432/3432 [==============================] - 4s 1ms/step - loss: 2.5578e-06 - rmspe: 0.3161 - val_loss: 1.7750e-06 - val_rmspe: 0.2286\n",
      "Epoch 6/10\n",
      "3432/3432 [==============================] - 4s 1ms/step - loss: 2.3172e-06 - rmspe: 0.2731 - val_loss: 1.9052e-06 - val_rmspe: 0.3288\n",
      "Epoch 7/10\n",
      "3432/3432 [==============================] - 4s 1ms/step - loss: 2.3397e-06 - rmspe: 0.2782 - val_loss: 1.8785e-06 - val_rmspe: 0.2086\n",
      "Epoch 8/10\n",
      "3432/3432 [==============================] - 4s 1ms/step - loss: 2.3421e-06 - rmspe: 0.2788 - val_loss: 2.8124e-06 - val_rmspe: 0.3316\n",
      "Epoch 9/10\n",
      "3432/3432 [==============================] - 4s 1ms/step - loss: 2.3299e-06 - rmspe: 0.2768 - val_loss: 1.8261e-06 - val_rmspe: 0.2845\n",
      "Epoch 10/10\n",
      "3432/3432 [==============================] - 4s 1ms/step - loss: 2.3425e-06 - rmspe: 0.2787 - val_loss: 3.3361e-06 - val_rmspe: 0.6144\n",
      "2681/2681 [==============================] - 2s 814us/step - loss: 3.3361e-06 - rmspe: 0.6144\n",
      "33333333333333333333333333333333333333333333333333333333333333333333333333333333\n",
      "Epoch 1/10\n",
      "3432/3432 [==============================] - 4s 1ms/step - loss: 5.3708e-04 - rmspe: 4.9058 - val_loss: 4.6391e-05 - val_rmspe: 1.7403\n",
      "Epoch 2/10\n",
      "3432/3432 [==============================] - 4s 1ms/step - loss: 1.2546e-05 - rmspe: 0.8093 - val_loss: 2.5112e-06 - val_rmspe: 0.2152\n",
      "Epoch 3/10\n",
      "3432/3432 [==============================] - 5s 1ms/step - loss: 2.3733e-06 - rmspe: 0.3096 - val_loss: 2.4858e-06 - val_rmspe: 0.3154\n",
      "Epoch 4/10\n",
      "3432/3432 [==============================] - 5s 1ms/step - loss: 2.4150e-06 - rmspe: 0.3169 - val_loss: 2.3545e-06 - val_rmspe: 0.2401\n",
      "Epoch 5/10\n",
      "3432/3432 [==============================] - 5s 1ms/step - loss: 2.4029e-06 - rmspe: 0.3141 - val_loss: 2.4394e-06 - val_rmspe: 0.2087\n",
      "Epoch 6/10\n",
      "3432/3432 [==============================] - 5s 1ms/step - loss: 2.1711e-06 - rmspe: 0.2703 - val_loss: 2.4591e-06 - val_rmspe: 0.3126\n",
      "Epoch 7/10\n",
      "3432/3432 [==============================] - 5s 1ms/step - loss: 2.2007e-06 - rmspe: 0.2767 - val_loss: 2.3196e-06 - val_rmspe: 0.2317\n",
      "Epoch 8/10\n",
      "3432/3432 [==============================] - 5s 1ms/step - loss: 2.2031e-06 - rmspe: 0.2772 - val_loss: 2.8744e-06 - val_rmspe: 0.2480\n",
      "Epoch 9/10\n",
      "3432/3432 [==============================] - 4s 1ms/step - loss: 2.1955e-06 - rmspe: 0.2764 - val_loss: 2.3672e-06 - val_rmspe: 0.2717\n",
      "Epoch 10/10\n",
      "3432/3432 [==============================] - 5s 1ms/step - loss: 2.1939e-06 - rmspe: 0.2749 - val_loss: 2.3771e-06 - val_rmspe: 0.2809\n",
      "2681/2681 [==============================] - 3s 1ms/step - loss: 2.3771e-06 - rmspe: 0.2809\n",
      "44444444444444444444444444444444444444444444444444444444444444444444444444444444\n",
      "Epoch 1/10\n",
      "3432/3432 [==============================] - 4s 1ms/step - loss: 5.2993e-04 - rmspe: 4.8041 - val_loss: 4.6815e-05 - val_rmspe: 1.8886\n",
      "Epoch 2/10\n",
      "3432/3432 [==============================] - 6s 2ms/step - loss: 1.2312e-05 - rmspe: 0.7908 - val_loss: 2.0566e-06 - val_rmspe: 0.2164\n",
      "Epoch 3/10\n",
      "3432/3432 [==============================] - 6s 2ms/step - loss: 2.4626e-06 - rmspe: 0.3080 - val_loss: 2.8919e-06 - val_rmspe: 0.5245\n",
      "Epoch 4/10\n",
      "3432/3432 [==============================] - 4s 1ms/step - loss: 2.5052e-06 - rmspe: 0.3159 - val_loss: 1.9741e-06 - val_rmspe: 0.2616\n",
      "Epoch 5/10\n",
      "3432/3432 [==============================] - 4s 1ms/step - loss: 2.4983e-06 - rmspe: 0.3125 - val_loss: 2.0112e-06 - val_rmspe: 0.2200\n",
      "Epoch 6/10\n",
      "3432/3432 [==============================] - 4s 1ms/step - loss: 2.2606e-06 - rmspe: 0.2710 - val_loss: 2.1958e-06 - val_rmspe: 0.3714\n",
      "Epoch 7/10\n",
      "3432/3432 [==============================] - 4s 1ms/step - loss: 2.2928e-06 - rmspe: 0.2783 - val_loss: 2.0491e-06 - val_rmspe: 0.3115\n",
      "Epoch 8/10\n",
      "3432/3432 [==============================] - 4s 1ms/step - loss: 2.2865e-06 - rmspe: 0.2774 - val_loss: 2.0881e-06 - val_rmspe: 0.2153\n",
      "Epoch 9/10\n",
      "3432/3432 [==============================] - 4s 1ms/step - loss: 2.2924e-06 - rmspe: 0.2778 - val_loss: 1.9698e-06 - val_rmspe: 0.2573\n",
      "Epoch 10/10\n",
      "3432/3432 [==============================] - 4s 1ms/step - loss: 2.2747e-06 - rmspe: 0.2746 - val_loss: 2.1991e-06 - val_rmspe: 0.3661\n",
      "2681/2681 [==============================] - 2s 759us/step - loss: 2.1991e-06 - rmspe: 0.3661\n"
     ]
    }
   ],
   "source": [
    "# base_model.evaluate(x_val,y_val)\n",
    "# train_X.shape\n",
    "# (y_train == train_y).all()\n",
    "from __future__ import print_function\n",
    "\n",
    "import logging\n",
    "import numpy as np\n",
    "from optparse import OptionParser\n",
    "import sys\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold,StratifiedKFold,cross_val_score,train_test_split,StratifiedShuffleSplit\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import Perceptron,LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils.extmath import density\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "base_models,scores = benchmark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float64, numpy=0.555363809502127>"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X.index.intersection(test)\n",
    "# clf\n",
    "y_pred_0 = base_models[4].predict(X)\n",
    "rmspe(y/scale,y_pred_0.reshape(1,-1)[0]/scale)\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stock_id</th>\n",
       "      <th>time_id</th>\n",
       "      <th>target_x</th>\n",
       "      <th>row_id</th>\n",
       "      <th>target_y</th>\n",
       "      <th>target</th>\n",
       "      <th>len</th>\n",
       "      <th>len_last_min</th>\n",
       "      <th>len_last_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.004136</td>\n",
       "      <td>0-5</td>\n",
       "      <td>0.003793</td>\n",
       "      <td>0.004115</td>\n",
       "      <td>0.501667</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.463333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0.001445</td>\n",
       "      <td>0-11</td>\n",
       "      <td>0.001438</td>\n",
       "      <td>0.001268</td>\n",
       "      <td>0.331667</td>\n",
       "      <td>0.366667</td>\n",
       "      <td>0.383333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0.002168</td>\n",
       "      <td>0-16</td>\n",
       "      <td>0.003178</td>\n",
       "      <td>0.002719</td>\n",
       "      <td>0.311667</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.226667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>0.002195</td>\n",
       "      <td>0-31</td>\n",
       "      <td>0.003836</td>\n",
       "      <td>0.002625</td>\n",
       "      <td>0.198333</td>\n",
       "      <td>0.116667</td>\n",
       "      <td>0.176667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.001747</td>\n",
       "      <td>0-62</td>\n",
       "      <td>0.002299</td>\n",
       "      <td>0.001901</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>0.183333</td>\n",
       "      <td>0.296667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428927</th>\n",
       "      <td>126</td>\n",
       "      <td>32751</td>\n",
       "      <td>0.003461</td>\n",
       "      <td>126-32751</td>\n",
       "      <td>0.003037</td>\n",
       "      <td>0.003338</td>\n",
       "      <td>0.515000</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.473333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428928</th>\n",
       "      <td>126</td>\n",
       "      <td>32753</td>\n",
       "      <td>0.003113</td>\n",
       "      <td>126-32753</td>\n",
       "      <td>0.003578</td>\n",
       "      <td>0.003347</td>\n",
       "      <td>0.370000</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.350000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428929</th>\n",
       "      <td>126</td>\n",
       "      <td>32758</td>\n",
       "      <td>0.004070</td>\n",
       "      <td>126-32758</td>\n",
       "      <td>0.003701</td>\n",
       "      <td>0.003695</td>\n",
       "      <td>0.425000</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.556667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428930</th>\n",
       "      <td>126</td>\n",
       "      <td>32763</td>\n",
       "      <td>0.003357</td>\n",
       "      <td>126-32763</td>\n",
       "      <td>0.002707</td>\n",
       "      <td>0.003375</td>\n",
       "      <td>0.663333</td>\n",
       "      <td>0.616667</td>\n",
       "      <td>0.636667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428931</th>\n",
       "      <td>126</td>\n",
       "      <td>32767</td>\n",
       "      <td>0.002090</td>\n",
       "      <td>126-32767</td>\n",
       "      <td>0.002158</td>\n",
       "      <td>0.001984</td>\n",
       "      <td>0.360000</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.320000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>428932 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        stock_id  time_id  target_x     row_id  target_y    target       len  \\\n",
       "0              0        5  0.004136        0-5  0.003793  0.004115  0.501667   \n",
       "1              0       11  0.001445       0-11  0.001438  0.001268  0.331667   \n",
       "2              0       16  0.002168       0-16  0.003178  0.002719  0.311667   \n",
       "3              0       31  0.002195       0-31  0.003836  0.002625  0.198333   \n",
       "4              0       62  0.001747       0-62  0.002299  0.001901  0.291667   \n",
       "...          ...      ...       ...        ...       ...       ...       ...   \n",
       "428927       126    32751  0.003461  126-32751  0.003037  0.003338  0.515000   \n",
       "428928       126    32753  0.003113  126-32753  0.003578  0.003347  0.370000   \n",
       "428929       126    32758  0.004070  126-32758  0.003701  0.003695  0.425000   \n",
       "428930       126    32763  0.003357  126-32763  0.002707  0.003375  0.663333   \n",
       "428931       126    32767  0.002090  126-32767  0.002158  0.001984  0.360000   \n",
       "\n",
       "        len_last_min  len_last_5  \n",
       "0           0.466667    0.463333  \n",
       "1           0.366667    0.383333  \n",
       "2           0.133333    0.226667  \n",
       "3           0.116667    0.176667  \n",
       "4           0.183333    0.296667  \n",
       "...              ...         ...  \n",
       "428927      0.533333    0.473333  \n",
       "428928      0.533333    0.350000  \n",
       "428929      0.533333    0.556667  \n",
       "428930      0.616667    0.636667  \n",
       "428931      0.450000    0.320000  \n",
       "\n",
       "[428932 rows x 9 columns]"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # rmspe(y/scale,X.target_y/100)\n",
    "# import shap\n",
    "\n",
    "# # load your data here, e.g. X and y\n",
    "# # create and fit your model here\n",
    "\n",
    "# # load JS visualization code to notebook\n",
    "# shap.initjs()\n",
    "\n",
    "# # explain the model's predictions using SHAP\n",
    "# # (same syntax works for LightGBM, CatBoost, scikit-learn and spark models)\n",
    "# explainer = shap.TreeExplainer(base_models[0])\n",
    "# shap_values = explainer.shap_values(X)\n",
    "\n",
    "# # visualize the first prediction's explanation (use matplotlib=True to avoid Javascript)\n",
    "# shap.force_plot(explainer.expected_value, shap_values[0,:], X.iloc[0,:])\n",
    "\n",
    "# shap.summary_plot(shap_values, X, plot_type=\"bar\")\n",
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amakr\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\seaborn\\distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "C:\\Users\\amakr\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\seaborn\\distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x25f10820948>"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# X.target\n",
    "import seaborn as sns\n",
    "sns.distplot((y_pred_0))\n",
    "sns.distplot(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stock_id       -0.021637\n",
       "time_id        -0.010331\n",
       "target_x        1.000000\n",
       "target_y       -0.000400\n",
       "target          0.866859\n",
       "len            -0.054558\n",
       "len_last_min   -0.048217\n",
       "len_last_5     -0.049933\n",
       "Name: target_x, dtype: float64"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# np.log(y)\n",
    "# np.log(X.target_y)\n",
    "(train_df).corr()['target_x']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
