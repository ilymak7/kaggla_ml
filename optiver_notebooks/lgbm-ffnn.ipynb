{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-08-30T09:03:50.659878Z",
     "iopub.status.busy": "2021-08-30T09:03:50.659414Z",
     "iopub.status.idle": "2021-08-30T09:03:51.880976Z",
     "shell.execute_reply": "2021-08-30T09:03:51.879920Z",
     "shell.execute_reply.started": "2021-08-30T09:03:50.659773Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import glob\n",
    "import os\n",
    "import gc\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from sklearn import preprocessing, model_selection\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler,LabelEncoder\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import numpy.matlib\n",
    "\n",
    "\n",
    "path_submissions = '/'\n",
    "\n",
    "target_name = 'target'\n",
    "scores_folds = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### I just tried different parameters and weights of 2  models. Cross validation with different folds was also attempted, but failed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-30T09:03:51.882968Z",
     "iopub.status.busy": "2021-08-30T09:03:51.882652Z",
     "iopub.status.idle": "2021-08-30T09:03:51.933267Z",
     "shell.execute_reply": "2021-08-30T09:03:51.932031Z",
     "shell.execute_reply.started": "2021-08-30T09:03:51.882938Z"
    }
   },
   "outputs": [],
   "source": [
    "# data directory\n",
    "data_dir = '../data/'\n",
    "\n",
    "# Function to calculate first WAP\n",
    "def calc_wap1(df):\n",
    "    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) / (df['bid_size1'] + df['ask_size1'])\n",
    "    return wap\n",
    "\n",
    "# Function to calculate second WAP\n",
    "def calc_wap2(df):\n",
    "    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) / (df['bid_size2'] + df['ask_size2'])\n",
    "    return wap\n",
    "\n",
    "def calc_wap3(df):\n",
    "    wap = (df['bid_price1'] * df['bid_size1'] + df['ask_price1'] * df['ask_size1']) / (df['bid_size1'] + df['ask_size1'])\n",
    "    return wap\n",
    "\n",
    "def calc_wap4(df):\n",
    "    wap = (df['bid_price2'] * df['bid_size2'] + df['ask_price2'] * df['ask_size2']) / (df['bid_size2'] + df['ask_size2'])\n",
    "    return wap\n",
    "\n",
    "# Function to calculate the log of the return\n",
    "# Remember that logb(x / y) = logb(x) - logb(y)\n",
    "def log_return(series):\n",
    "    return np.log(series).diff()\n",
    "\n",
    "# Calculate the realized volatility\n",
    "def realized_volatility(series):\n",
    "    return np.sqrt(np.sum(series**2))\n",
    "\n",
    "# Function to count unique elements of a series\n",
    "def count_unique(series):\n",
    "    return len(np.unique(series))\n",
    "\n",
    "# Function to read our base train and test set\n",
    "def read_train_test():\n",
    "    train = pd.read_csv(data_dir+'train.csv')\n",
    "    test = pd.read_csv(data_dir+'test.csv')\n",
    "    # Create a key to merge with book and trade data\n",
    "    train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n",
    "    test['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n",
    "    print(f'Our training set has {train.shape[0]} rows')\n",
    "    return train, test\n",
    "\n",
    "# Function to preprocess book data (for each stock id)\n",
    "def book_preprocessor(file_path):\n",
    "    df = pd.read_parquet(file_path)\n",
    "    # Calculate Wap\n",
    "    df['wap1'] = calc_wap1(df)\n",
    "    df['wap2'] = calc_wap2(df)\n",
    "    df['wap3'] = calc_wap3(df)\n",
    "    df['wap4'] = calc_wap4(df)\n",
    "    # Calculate log returns\n",
    "    df['log_return1'] = df.groupby(['time_id'])['wap1'].apply(log_return)\n",
    "    df['log_return2'] = df.groupby(['time_id'])['wap2'].apply(log_return)\n",
    "    df['log_return3'] = df.groupby(['time_id'])['wap3'].apply(log_return)\n",
    "    df['log_return4'] = df.groupby(['time_id'])['wap4'].apply(log_return)\n",
    "    # Calculate wap balance\n",
    "    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n",
    "    # Calculate spread\n",
    "    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) / ((df['ask_price1'] + df['bid_price1']) / 2)\n",
    "    df['price_spread2'] = (df['ask_price2'] - df['bid_price2']) / ((df['ask_price2'] + df['bid_price2']) / 2)\n",
    "    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n",
    "    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n",
    "    df[\"bid_ask_spread\"] = abs(df['bid_spread'] - df['ask_spread'])\n",
    "    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n",
    "    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n",
    "    \n",
    "    # Dict for aggregations\n",
    "    create_feature_dict = {\n",
    "        'wap1': [np.sum, np.std],\n",
    "        'wap2': [np.sum, np.std],\n",
    "        'wap3': [np.sum, np.std],\n",
    "        'wap4': [np.sum, np.std],\n",
    "        'log_return1': [realized_volatility],\n",
    "        'log_return2': [realized_volatility],\n",
    "        'log_return3': [realized_volatility],\n",
    "        'log_return4': [realized_volatility],\n",
    "        'wap_balance': [np.sum, np.max],\n",
    "        'price_spread':[np.sum, np.max],\n",
    "        'price_spread2':[np.sum, np.max],\n",
    "        'bid_spread':[np.sum, np.max],\n",
    "        'ask_spread':[np.sum, np.max],\n",
    "        'total_volume':[np.sum, np.max],\n",
    "        'volume_imbalance':[np.sum, np.max],\n",
    "        \"bid_ask_spread\":[np.sum,  np.max],\n",
    "    }\n",
    "    create_feature_dict_time = {\n",
    "        'log_return1': [realized_volatility],\n",
    "        'log_return2': [realized_volatility],\n",
    "        'log_return3': [realized_volatility],\n",
    "        'log_return4': [realized_volatility],\n",
    "    }\n",
    "    \n",
    "    # Function to get group stats for different windows (seconds in bucket)\n",
    "    def get_stats_window(fe_dict,seconds_in_bucket, add_suffix = False):\n",
    "        # Group by the window\n",
    "        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(fe_dict).reset_index()\n",
    "        # Rename columns joining suffix\n",
    "        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n",
    "        # Add a suffix to differentiate windows\n",
    "        if add_suffix:\n",
    "            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n",
    "        return df_feature\n",
    "    \n",
    "    # Get the stats for different windows\n",
    "    df_feature = get_stats_window(create_feature_dict,seconds_in_bucket = 0, add_suffix = False)\n",
    "    df_feature_500 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 500, add_suffix = True)\n",
    "    df_feature_400 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 400, add_suffix = True)\n",
    "    df_feature_300 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 300, add_suffix = True)\n",
    "    df_feature_200 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 200, add_suffix = True)\n",
    "    df_feature_100 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 100, add_suffix = True)\n",
    "\n",
    "    # Merge all\n",
    "    df_feature = df_feature.merge(df_feature_500, how = 'left', left_on = 'time_id_', right_on = 'time_id__500')\n",
    "    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n",
    "    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n",
    "    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n",
    "    df_feature = df_feature.merge(df_feature_100, how = 'left', left_on = 'time_id_', right_on = 'time_id__100')\n",
    "    # Drop unnecesary time_ids\n",
    "    df_feature.drop(['time_id__500','time_id__400', 'time_id__300', 'time_id__200','time_id__100'], axis = 1, inplace = True)\n",
    "    \n",
    "    \n",
    "    # Create row_id so we can merge\n",
    "    stock_id = file_path.split('=')[1]\n",
    "    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x: f'{stock_id}-{x}')\n",
    "    df_feature.drop(['time_id_'], axis = 1, inplace = True)\n",
    "    return df_feature\n",
    "\n",
    "# Function to preprocess trade data (for each stock id)\n",
    "def trade_preprocessor(file_path):\n",
    "    df = pd.read_parquet(file_path)\n",
    "    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n",
    "    df['amount']=df['price']*df['size']\n",
    "    # Dict for aggregations\n",
    "    create_feature_dict = {\n",
    "        'log_return':[realized_volatility],\n",
    "        'seconds_in_bucket':[count_unique],\n",
    "        'size':[np.sum, np.max, np.min],\n",
    "        'order_count':[np.sum,np.max],\n",
    "        'amount':[np.sum,np.max,np.min],\n",
    "    }\n",
    "    create_feature_dict_time = {\n",
    "        'log_return':[realized_volatility],\n",
    "        'seconds_in_bucket':[count_unique],\n",
    "        'size':[np.sum],\n",
    "        'order_count':[np.sum],\n",
    "    }\n",
    "    # Function to get group stats for different windows (seconds in bucket)\n",
    "    def get_stats_window(fe_dict,seconds_in_bucket, add_suffix = False):\n",
    "        # Group by the window\n",
    "        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(fe_dict).reset_index()\n",
    "        # Rename columns joining suffix\n",
    "        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n",
    "        # Add a suffix to differentiate windows\n",
    "        if add_suffix:\n",
    "            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n",
    "        return df_feature\n",
    "    \n",
    "\n",
    "    # Get the stats for different windows\n",
    "    df_feature = get_stats_window(create_feature_dict,seconds_in_bucket = 0, add_suffix = False)\n",
    "    df_feature_500 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 500, add_suffix = True)\n",
    "    df_feature_400 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 400, add_suffix = True)\n",
    "    df_feature_300 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 300, add_suffix = True)\n",
    "    df_feature_200 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 200, add_suffix = True)\n",
    "    df_feature_100 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 100, add_suffix = True)\n",
    "    \n",
    "    def tendency(price, vol):    \n",
    "        df_diff = np.diff(price)\n",
    "        val = (df_diff/price[1:])*100\n",
    "        power = np.sum(val*vol[1:])\n",
    "        return(power)\n",
    "    \n",
    "    lis = []\n",
    "    for n_time_id in df['time_id'].unique():\n",
    "        df_id = df[df['time_id'] == n_time_id]        \n",
    "        tendencyV = tendency(df_id['price'].values, df_id['size'].values)      \n",
    "        f_max = np.sum(df_id['price'].values > np.mean(df_id['price'].values))\n",
    "        f_min = np.sum(df_id['price'].values < np.mean(df_id['price'].values))\n",
    "        df_max =  np.sum(np.diff(df_id['price'].values) > 0)\n",
    "        df_min =  np.sum(np.diff(df_id['price'].values) < 0)\n",
    "        # new\n",
    "        abs_diff = np.median(np.abs( df_id['price'].values - np.mean(df_id['price'].values)))        \n",
    "        energy = np.mean(df_id['price'].values**2)\n",
    "        iqr_p = np.percentile(df_id['price'].values,75) - np.percentile(df_id['price'].values,25)\n",
    "        \n",
    "        # vol vars\n",
    "        \n",
    "        abs_diff_v = np.median(np.abs( df_id['size'].values - np.mean(df_id['size'].values)))        \n",
    "        energy_v = np.sum(df_id['size'].values**2)\n",
    "        iqr_p_v = np.percentile(df_id['size'].values,75) - np.percentile(df_id['size'].values,25)\n",
    "        \n",
    "        lis.append({'time_id':n_time_id,'tendency':tendencyV,'f_max':f_max,'f_min':f_min,'df_max':df_max,'df_min':df_min,\n",
    "                   'abs_diff':abs_diff,'energy':energy,'iqr_p':iqr_p,'abs_diff_v':abs_diff_v,'energy_v':energy_v,'iqr_p_v':iqr_p_v})\n",
    "    \n",
    "    df_lr = pd.DataFrame(lis)\n",
    "        \n",
    "   \n",
    "    df_feature = df_feature.merge(df_lr, how = 'left', left_on = 'time_id_', right_on = 'time_id')\n",
    "    \n",
    "    # Merge all\n",
    "    df_feature = df_feature.merge(df_feature_500, how = 'left', left_on = 'time_id_', right_on = 'time_id__500')\n",
    "    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n",
    "    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n",
    "    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n",
    "    df_feature = df_feature.merge(df_feature_100, how = 'left', left_on = 'time_id_', right_on = 'time_id__100')\n",
    "    # Drop unnecesary time_ids\n",
    "    df_feature.drop(['time_id__500','time_id__400', 'time_id__300', 'time_id__200','time_id','time_id__100'], axis = 1, inplace = True)\n",
    "    \n",
    "    \n",
    "    df_feature = df_feature.add_prefix('trade_')\n",
    "    stock_id = file_path.split('=')[1]\n",
    "    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n",
    "    df_feature.drop(['trade_time_id_'], axis = 1, inplace = True)\n",
    "    return df_feature\n",
    "\n",
    "# Function to get group stats for the stock_id and time_id\n",
    "def get_time_stock(df):\n",
    "    vol_cols = ['log_return1_realized_volatility', 'log_return2_realized_volatility', 'log_return1_realized_volatility_400', 'log_return2_realized_volatility_400', \n",
    "                'log_return1_realized_volatility_300', 'log_return2_realized_volatility_300', 'log_return1_realized_volatility_200', 'log_return2_realized_volatility_200', \n",
    "                'trade_log_return_realized_volatility', 'trade_log_return_realized_volatility_400', 'trade_log_return_realized_volatility_300', 'trade_log_return_realized_volatility_200']\n",
    "\n",
    "\n",
    "    # Group by the stock id\n",
    "    df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n",
    "    # Rename columns joining suffix\n",
    "    df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n",
    "    df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n",
    "\n",
    "    # Group by the stock id\n",
    "    df_time_id = df.groupby(['time_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n",
    "    # Rename columns joining suffix\n",
    "    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n",
    "    df_time_id = df_time_id.add_suffix('_' + 'time')\n",
    "    \n",
    "    # Merge with original dataframe\n",
    "    df = df.merge(df_stock_id, how = 'left', left_on = ['stock_id'], right_on = ['stock_id__stock'])\n",
    "    df = df.merge(df_time_id, how = 'left', left_on = ['time_id'], right_on = ['time_id__time'])\n",
    "    df.drop(['stock_id__stock', 'time_id__time'], axis = 1, inplace = True)\n",
    "    return df\n",
    "    \n",
    "# Funtion to make preprocessing function in parallel (for each stock id)\n",
    "def preprocessor(list_stock_ids, is_train = True):\n",
    "    \n",
    "    # Parrallel for loop\n",
    "    def for_joblib(stock_id):\n",
    "        # Train\n",
    "        if is_train:\n",
    "            file_path_book = data_dir + \"book_train.parquet/stock_id=\" + str(stock_id)\n",
    "            file_path_trade = data_dir + \"trade_train.parquet/stock_id=\" + str(stock_id)\n",
    "        # Test\n",
    "        else:\n",
    "            file_path_book = data_dir + \"book_test.parquet/stock_id=\" + str(stock_id)\n",
    "            file_path_trade = data_dir + \"trade_test.parquet/stock_id=\" + str(stock_id)\n",
    "    \n",
    "        # Preprocess book and trade data and merge them\n",
    "        df_tmp = pd.merge(book_preprocessor(file_path_book), trade_preprocessor(file_path_trade), on = 'row_id', how = 'left')\n",
    "        \n",
    "        # Return the merge dataframe\n",
    "        return df_tmp\n",
    "    \n",
    "    # Use parallel api to call paralle for loop\n",
    "    df = Parallel(n_jobs = -1, verbose = 1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\n",
    "    # Concatenate all the dataframes that return from Parallel\n",
    "    df = pd.concat(df, ignore_index = True)\n",
    "    return df\n",
    "\n",
    "# Function to calculate the root mean squared percentage error\n",
    "def rmspe(y_true, y_pred):\n",
    "    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
    "\n",
    "# Function to early stop with root mean squared percentage error\n",
    "def feval_rmspe(y_pred, lgb_train):\n",
    "    y_true = lgb_train.get_label()\n",
    "    return 'RMSPE', rmspe(y_true, y_pred), False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-30T09:03:51.935512Z",
     "iopub.status.busy": "2021-08-30T09:03:51.935192Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our training set has 428932 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  8.9min\n",
      "[Parallel(n_jobs=-1)]: Done 112 out of 112 | elapsed: 26.2min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    4.0s finished\n"
     ]
    }
   ],
   "source": [
    "# Read train and test\n",
    "train, test = read_train_test()\n",
    "\n",
    "# Get unique stock ids \n",
    "train_stock_ids = train['stock_id'].unique()\n",
    "# Preprocess them using Parallel and our single stock id functions\n",
    "train_ = preprocessor(train_stock_ids, is_train = True)\n",
    "train = train.merge(train_, on = ['row_id'], how = 'left')\n",
    "\n",
    "# Get unique stock ids \n",
    "test_stock_ids = test['stock_id'].unique()\n",
    "# Preprocess them using Parallel and our single stock id functions\n",
    "test_ = preprocessor(test_stock_ids, is_train = False)\n",
    "test = test.merge(test_, on = ['row_id'], how = 'left')\n",
    "\n",
    "# Get group stats of time_id and stock_id\n",
    "train = get_time_stock(train)\n",
    "test = get_time_stock(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace by order sum (tau)\n",
    "train['size_tau'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique'] )\n",
    "test['size_tau'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique'] )\n",
    "#train['size_tau_450'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_450'] )\n",
    "#test['size_tau_450'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_450'] )\n",
    "train['size_tau_400'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_400'] )\n",
    "test['size_tau_400'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_400'] )\n",
    "train['size_tau_300'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_300'] )\n",
    "test['size_tau_300'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_300'] )\n",
    "#train['size_tau_150'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_150'] )\n",
    "#test['size_tau_150'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_150'] )\n",
    "train['size_tau_200'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_200'] )\n",
    "test['size_tau_200'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_200'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['size_tau2'] = np.sqrt( 1/ train['trade_order_count_sum'] )\n",
    "test['size_tau2'] = np.sqrt( 1/ test['trade_order_count_sum'] )\n",
    "#train['size_tau2_450'] = np.sqrt( 0.25/ train['trade_order_count_sum'] )\n",
    "#test['size_tau2_450'] = np.sqrt( 0.25/ test['trade_order_count_sum'] )\n",
    "train['size_tau2_400'] = np.sqrt( 0.33/ train['trade_order_count_sum'] )\n",
    "test['size_tau2_400'] = np.sqrt( 0.33/ test['trade_order_count_sum'] )\n",
    "train['size_tau2_300'] = np.sqrt( 0.5/ train['trade_order_count_sum'] )\n",
    "test['size_tau2_300'] = np.sqrt( 0.5/ test['trade_order_count_sum'] )\n",
    "#train['size_tau2_150'] = np.sqrt( 0.75/ train['trade_order_count_sum'] )\n",
    "#test['size_tau2_150'] = np.sqrt( 0.75/ test['trade_order_count_sum'] )\n",
    "train['size_tau2_200'] = np.sqrt( 0.66/ train['trade_order_count_sum'] )\n",
    "test['size_tau2_200'] = np.sqrt( 0.66/ test['trade_order_count_sum'] )\n",
    "\n",
    "# delta tau\n",
    "train['size_tau2_d'] = train['size_tau2_400'] - train['size_tau2']\n",
    "test['size_tau2_d'] = test['size_tau2_400'] - test['size_tau2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.to_csv('train_preproc.csv',index = False)\n",
    "# test.to_csv('test_preproc.csv',index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "train= pd.read_csv('train_preproc.csv')\n",
    "test= pd.read_csv('test_preproc.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "cor = train.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_rows', 500)\n",
    "# pd.set_option('display.max_columns', 500)\n",
    "# pd.set_option('display.width', 1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# abs(cor['target']).sort_values(ascending = False)\n",
    "# tr_0 = trade_preprocessor(data_dir + \"trade_train.parquet/stock_id=\" + str(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_b = pd.read_csv(data_dir+'train.csv')\n",
    "# tr_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del train_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18544"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "194"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colNames = [col for col in list(train.columns)\n",
    "            if col not in {\"stock_id\", \"time_id\", \"target\", \"row_id\"}]\n",
    "len(colNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "# making agg features\n",
    "\n",
    "train_p = pd.read_csv(data_dir+'train.csv')\n",
    "train_p = train_p.pivot(index='time_id', columns='stock_id', values='target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "242"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_p\n",
    "del train \n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 4 2 1 1 2 4 6 2 1 0 4 4 1 1 1 2 4 4 4 0 1 1 3 1 1 4 3 4 3 4 4 1 3 3 4\n",
      " 3 4 1 4 1 4 4 1 0 4 4 1 0 0 3 3 3 2 0 2 4 1 4 4 1 4 1 0 3 3 0 3 0 6 5 3 3\n",
      " 0 1 2 0 3 3 3 4 1 1 0 2 3 3 1 0 1 4 4 4 4 4 1 3 1 0 1 4 1 0 1 4 1 0 4 0 4\n",
      " 0]\n"
     ]
    }
   ],
   "source": [
    "# corr = train_p.corr()\n",
    "\n",
    "ids = corr.index\n",
    "n_clusters = 7\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(corr.values)\n",
    "print(kmeans.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cor_2 = train.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "distortions = []\n",
    "K = range(1,10)\n",
    "for k in K:\n",
    "    kmeanModel = KMeans(n_clusters=k)\n",
    "    kmeanModel.fit(corr.values)\n",
    "    distortions.append(kmeanModel.inertia_)\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(K, distortions, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('The Elbow Method showing the optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 11, 22, 50, 55, 56, 62, 73, 76, 78, 84, 87, 96, 101, 112, 116, 122, 124, 126]\n",
      "[0, 4, 5, 10, 15, 16, 17, 23, 26, 28, 29, 36, 42, 44, 48, 53, 66, 69, 72, 85, 94, 95, 100, 102, 109, 111, 113, 115, 118, 120]\n",
      "[3, 6, 9, 18, 61, 63, 86, 97]\n",
      "[27, 31, 33, 37, 38, 40, 58, 59, 60, 74, 75, 77, 82, 83, 88, 89, 90, 98, 99, 110]\n",
      "[2, 7, 13, 14, 19, 20, 21, 30, 32, 34, 35, 39, 41, 43, 46, 47, 51, 52, 64, 67, 68, 70, 93, 103, 104, 105, 107, 108, 114, 119, 123, 125]\n",
      "[81]\n",
      "[8, 80]\n"
     ]
    }
   ],
   "source": [
    "l = []\n",
    "for n in range(n_clusters):\n",
    "    l.append ( [ (x-1) for x in ( (ids+1)*(kmeans.labels_ == n)) if x > 0] )\n",
    "    \n",
    "\n",
    "mat = []\n",
    "matTest = []\n",
    "\n",
    "n = 0\n",
    "for ind in l:\n",
    "    print(ind)\n",
    "    newDf = train.loc[train['stock_id'].isin(ind) ]\n",
    "    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n",
    "    newDf.loc[:,'stock_id'] = str(n)+'c2'\n",
    "    mat.append ( newDf )\n",
    "    \n",
    "    newDf = test.loc[test['stock_id'].isin(ind) ]    \n",
    "    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n",
    "    newDf.loc[:,'stock_id'] = str(n)+'c1'\n",
    "    matTest.append ( newDf )\n",
    "    \n",
    "    n+=1\n",
    "    \n",
    "mat1 = pd.concat(mat).reset_index()\n",
    "mat1.drop(columns=['target'],inplace=True)\n",
    "\n",
    "mat2 = pd.concat(matTest).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mat2\n",
    "# l\n",
    "# import seaborn as sns \n",
    "# (np.log(train.loc[train['stock_id'].isin(l[0]),'target'].values)).mean(),(np.log(train.loc[train['stock_id'].isin(l[1]),'target'].values)).mean(),(np.log(train.loc[train['stock_id'].isin(l[2]),'target'].values)).mean()\n",
    "# mat1.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat2 = pd.concat([mat2,mat1.loc[mat1.time_id==5]])\n",
    "mat1 = mat1.pivot(index='time_id', columns='stock_id')\n",
    "mat1.columns = [\"_\".join(x) for x in mat1.columns.ravel()]\n",
    "mat1.reset_index(inplace=True)\n",
    "\n",
    "mat2 = mat2.pivot(index='time_id', columns='stock_id')\n",
    "mat2.columns = [\"_\".join(x) for x in mat2.columns.ravel()]\n",
    "mat2.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(428932, 198)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mat2.columns.tolist()\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['bid_spread_sum_6c1', 'ask_spread_sum_6c1', 'trade_order_count_sum_6c1', 'trade_order_count_sum_3c1', 'trade_order_count_sum_0c1', 'ask_spread_sum_4c1', 'size_tau2_3c1', 'log_return1_realized_volatility_1c1', 'size_tau2_1c1', 'volume_imbalance_sum_1c1', 'ask_spread_sum_1c1', 'bid_spread_sum_3c1', 'size_tau2_0c1', 'bid_ask_spread_sum_0c1', 'total_volume_sum_0c1', 'trade_size_sum_4c1', 'bid_spread_sum_4c1', 'trade_size_sum_3c1', 'trade_size_sum_6c1', 'bid_spread_sum_0c1', 'bid_spread_sum_1c1', 'price_spread_sum_3c1', 'price_spread_sum_4c1', 'log_return1_realized_volatility_4c1', 'bid_ask_spread_sum_4c1', 'total_volume_sum_6c1', 'trade_order_count_sum_1c1', 'total_volume_sum_3c1', 'log_return1_realized_volatility_0c1', 'volume_imbalance_sum_3c1', 'log_return1_realized_volatility_6c1', 'trade_size_sum_0c1', 'ask_spread_sum_0c1', 'price_spread_sum_6c1', 'trade_order_count_sum_4c1', 'volume_imbalance_sum_6c1', 'size_tau2_4c1', 'bid_ask_spread_sum_6c1', 'price_spread_sum_1c1', 'size_tau2_6c1', 'volume_imbalance_sum_0c1', 'bid_ask_spread_sum_1c1', 'bid_ask_spread_sum_3c1', 'log_return1_realized_volatility_3c1', 'price_spread_sum_0c1', 'total_volume_sum_1c1', 'volume_imbalance_sum_4c1', 'total_volume_sum_4c1', 'ask_spread_sum_3c1', 'trade_size_sum_1c1'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-150-19c6195535f8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     82\u001b[0m      \u001b[1;34m'size_tau2_4c1'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m      'size_tau2_6c1']\n\u001b[1;32m---> 84\u001b[1;33m \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmat1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnnn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'left'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'time_id'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmat2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnnn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'left'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'time_id'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2804\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2805\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2806\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2807\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2808\u001b[0m         \u001b[1;31m# take() does not accept boolean indexers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[1;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[0;32m   1551\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1552\u001b[0m         self._validate_read_indexer(\n\u001b[1;32m-> 1553\u001b[1;33m             \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis_number\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mraise_missing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1554\u001b[0m         )\n\u001b[0;32m   1555\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[1;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[0;32m   1644\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"loc\"\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1645\u001b[0m                 \u001b[0mnot_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1646\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{not_found} not in index\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1647\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1648\u001b[0m             \u001b[1;31m# we skip the warning on Categorical/Interval\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['bid_spread_sum_6c1', 'ask_spread_sum_6c1', 'trade_order_count_sum_6c1', 'trade_order_count_sum_3c1', 'trade_order_count_sum_0c1', 'ask_spread_sum_4c1', 'size_tau2_3c1', 'log_return1_realized_volatility_1c1', 'size_tau2_1c1', 'volume_imbalance_sum_1c1', 'ask_spread_sum_1c1', 'bid_spread_sum_3c1', 'size_tau2_0c1', 'bid_ask_spread_sum_0c1', 'total_volume_sum_0c1', 'trade_size_sum_4c1', 'bid_spread_sum_4c1', 'trade_size_sum_3c1', 'trade_size_sum_6c1', 'bid_spread_sum_0c1', 'bid_spread_sum_1c1', 'price_spread_sum_3c1', 'price_spread_sum_4c1', 'log_return1_realized_volatility_4c1', 'bid_ask_spread_sum_4c1', 'total_volume_sum_6c1', 'trade_order_count_sum_1c1', 'total_volume_sum_3c1', 'log_return1_realized_volatility_0c1', 'volume_imbalance_sum_3c1', 'log_return1_realized_volatility_6c1', 'trade_size_sum_0c1', 'ask_spread_sum_0c1', 'price_spread_sum_6c1', 'trade_order_count_sum_4c1', 'volume_imbalance_sum_6c1', 'size_tau2_4c1', 'bid_ask_spread_sum_6c1', 'price_spread_sum_1c1', 'size_tau2_6c1', 'volume_imbalance_sum_0c1', 'bid_ask_spread_sum_1c1', 'bid_ask_spread_sum_3c1', 'log_return1_realized_volatility_3c1', 'price_spread_sum_0c1', 'total_volume_sum_1c1', 'volume_imbalance_sum_4c1', 'total_volume_sum_4c1', 'ask_spread_sum_3c1', 'trade_size_sum_1c1'] not in index\""
     ]
    }
   ],
   "source": [
    "# nnn = ['time_id',\n",
    "#      'log_return1_realized_volatility_0c1',\n",
    "#  'log_return1_realized_volatility_1c1',\n",
    "#  'log_return1_realized_volatility_2c1',\n",
    "#  'total_volume_sum_0c1',\n",
    "#  'total_volume_sum_1c1',\n",
    "#  'total_volume_sum_2c1',\n",
    "#  'trade_size_sum_0c1',\n",
    "#  'trade_size_sum_1c1',\n",
    "#  'trade_size_sum_2c1',\n",
    "#  'trade_order_count_sum_0c1',\n",
    "#  'trade_order_count_sum_1c1',\n",
    "#  'trade_order_count_sum_2c1',\n",
    "#  'price_spread_sum_0c1',\n",
    "#  'price_spread_sum_1c1',\n",
    "#  'price_spread_sum_2c1',\n",
    "#  'bid_spread_sum_0c1',\n",
    "#  'bid_spread_sum_1c1',\n",
    "#  'bid_spread_sum_2c1',\n",
    "#  'ask_spread_sum_0c1',\n",
    "#  'ask_spread_sum_1c1',\n",
    "#  'ask_spread_sum_2c1',\n",
    "#  'volume_imbalance_sum_0c1',\n",
    "#  'volume_imbalance_sum_1c1',\n",
    "#  'volume_imbalance_sum_2c1',\n",
    "#  'bid_ask_spread_sum_0c1',\n",
    "#  'bid_ask_spread_sum_1c1',\n",
    "#  'bid_ask_spread_sum_2c1',\n",
    "#  'size_tau2_0c1',\n",
    "#  'size_tau2_1c1',\n",
    "#  'size_tau2_2c1'] \n",
    "# nnn = mat1.columns.tolist()\n",
    "nnn = ['time_id',\n",
    "     'log_return1_realized_volatility_0c1',\n",
    "     'log_return1_realized_volatility_1c1',     \n",
    "     'log_return1_realized_volatility_3c1',\n",
    "     'log_return1_realized_volatility_4c1',     \n",
    "     'log_return1_realized_volatility_6c1',\n",
    "     'total_volume_sum_0c1',\n",
    "     'total_volume_sum_1c1', \n",
    "     'total_volume_sum_3c1',\n",
    "     'total_volume_sum_4c1', \n",
    "     'total_volume_sum_6c1',\n",
    "     'trade_size_sum_0c1',\n",
    "     'trade_size_sum_1c1', \n",
    "     'trade_size_sum_3c1',\n",
    "     'trade_size_sum_4c1', \n",
    "     'trade_size_sum_6c1',\n",
    "     'trade_order_count_sum_0c1',\n",
    "     'trade_order_count_sum_1c1',\n",
    "     'trade_order_count_sum_3c1',\n",
    "     'trade_order_count_sum_4c1',\n",
    "     'trade_order_count_sum_6c1',      \n",
    "     'price_spread_sum_0c1',\n",
    "     'price_spread_sum_1c1',\n",
    "     'price_spread_sum_3c1',\n",
    "     'price_spread_sum_4c1',\n",
    "     'price_spread_sum_6c1',   \n",
    "     'bid_spread_sum_0c1',\n",
    "     'bid_spread_sum_1c1',\n",
    "     'bid_spread_sum_3c1',\n",
    "     'bid_spread_sum_4c1',\n",
    "     'bid_spread_sum_6c1',       \n",
    "     'ask_spread_sum_0c1',\n",
    "     'ask_spread_sum_1c1',\n",
    "     'ask_spread_sum_3c1',\n",
    "     'ask_spread_sum_4c1',\n",
    "     'ask_spread_sum_6c1',   \n",
    "     'volume_imbalance_sum_0c1',\n",
    "     'volume_imbalance_sum_1c1',\n",
    "     'volume_imbalance_sum_3c1',\n",
    "     'volume_imbalance_sum_4c1',\n",
    "     'volume_imbalance_sum_6c1',       \n",
    "     'bid_ask_spread_sum_0c1',\n",
    "     'bid_ask_spread_sum_1c1',\n",
    "     'bid_ask_spread_sum_3c1',\n",
    "     'bid_ask_spread_sum_4c1',\n",
    "     'bid_ask_spread_sum_6c1',\n",
    "     'size_tau2_0c1',\n",
    "     'size_tau2_1c1',\n",
    "     'size_tau2_3c1',\n",
    "     'size_tau2_4c1',\n",
    "     'size_tau2_6c1']\n",
    "train = pd.merge(train,mat1[nnn],how='left',on='time_id')\n",
    "test = pd.merge(test,mat2[nnn],how='left',on='time_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 779)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16380"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "del mat1,mat2\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test['size_tau2_2c1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 1\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 1.97 GiB for an array with shape (772, 343145) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-125-ca86db2f1fd3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtest_predictions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;31m# Traing and evaluate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m \u001b[0mpredictions_lgb_1\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mtrain_and_evaluate_lgb\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mparams0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m \u001b[1;31m# predictions_lgb_2= train_and_evaluate_lgb(train, test,params1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;31m# test['target'] = predictions_lgb_1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-125-ca86db2f1fd3>\u001b[0m in \u001b[0;36mtrain_and_evaluate_lgb\u001b[1;34m(train, test, params)\u001b[0m\n\u001b[0;32m     82\u001b[0m                           \u001b[0mverbose_eval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m250\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m                           \u001b[0mearly_stopping_rounds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m                           feval = feval_rmspe)\n\u001b[0m\u001b[0;32m     85\u001b[0m         \u001b[1;31m# Add predictions to the out of folds array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[0moof_predictions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mval_ind\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[0;32m    226\u001b[0m     \u001b[1;31m# construct booster\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 228\u001b[1;33m         \u001b[0mbooster\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBooster\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    229\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_valid_contain_train\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m             \u001b[0mbooster\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_train_data_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, params, train_set, model_file, model_str, silent)\u001b[0m\n\u001b[0;32m   1712\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1713\u001b[0m             _safe_call(_LIB.LGBM_BoosterCreate(\n\u001b[1;32m-> 1714\u001b[1;33m                 \u001b[0mtrain_set\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstruct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1715\u001b[0m                 \u001b[0mc_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1716\u001b[0m                 ctypes.byref(self.handle)))\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36mconstruct\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1083\u001b[0m                                 \u001b[0minit_score\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_predictor\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1084\u001b[0m                                 \u001b[0msilent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msilent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1085\u001b[1;33m                                 categorical_feature=self.categorical_feature, params=self.params)\n\u001b[0m\u001b[0;32m   1086\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfree_raw_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1087\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[1;34m(self, data, label, reference, weight, group, init_score, predictor, silent, feature_name, categorical_feature, params)\u001b[0m\n\u001b[0;32m    828\u001b[0m                                                                                              \u001b[0mfeature_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    829\u001b[0m                                                                                              \u001b[0mcategorical_feature\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 830\u001b[1;33m                                                                                              self.pandas_categorical)\n\u001b[0m\u001b[0;32m    831\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_label_from_pandas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    832\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36m_data_from_pandas\u001b[1;34m(data, feature_name, categorical_feature, pandas_categorical)\u001b[0m\n\u001b[0;32m    315\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Input data must be 2 dimensional and non empty.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfeature_name\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'auto'\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfeature_name\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 317\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    318\u001b[0m         \u001b[0mcat_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect_dtypes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minclude\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'category'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m         \u001b[0mcat_cols_not_ordered\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcol\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcat_cols\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mordered\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    225\u001b[0m         \u001b[1;33m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mCallable\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 227\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m         \u001b[0mkind\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mParameter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPOSITIONAL_OR_KEYWORD\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mrename\u001b[1;34m(self, mapper, index, columns, axis, copy, inplace, level, errors)\u001b[0m\n\u001b[0;32m   4131\u001b[0m             \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4132\u001b[0m             \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4133\u001b[1;33m             \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4134\u001b[0m         )\n\u001b[0;32m   4135\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mrename\u001b[1;34m(self, mapper, index, columns, axis, copy, inplace, level, errors)\u001b[0m\n\u001b[0;32m   1073\u001b[0m                 \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1074\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1075\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0minplace\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1076\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1077\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0maxis_no\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreplacements\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mcopy\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m   5809\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5810\u001b[0m         \"\"\"\n\u001b[1;32m-> 5811\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdeep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5812\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5813\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mcopy\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m    792\u001b[0m             \u001b[0mnew_axes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    793\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 794\u001b[1;33m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"copy\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdeep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    795\u001b[0m         \u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_axes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    796\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, f, filter, **kwargs)\u001b[0m\n\u001b[0;32m    440\u001b[0m                 \u001b[0mapplied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 442\u001b[1;33m                 \u001b[0mapplied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    443\u001b[0m             \u001b[0mresult_blocks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_extend_blocks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mapplied\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult_blocks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\blocks.py\u001b[0m in \u001b[0;36mcopy\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m    696\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    697\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdeep\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 698\u001b[1;33m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    699\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_block_same_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    700\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 1.97 GiB for an array with shape (772, 343145) and data type float64"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import lightgbm as lgb\n",
    "\n",
    "seed0=2021\n",
    "params0 = {\n",
    "    'objective': 'rmse',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'max_depth': -1,\n",
    "    'max_bin':100,\n",
    "    'min_data_in_leaf':500,\n",
    "    'learning_rate': 0.05,\n",
    "    'subsample': 0.72,\n",
    "    'subsample_freq': 4,\n",
    "    'feature_fraction': 0.5,\n",
    "    'lambda_l1': 0.5,\n",
    "    'lambda_l2': 1.0,\n",
    "    'categorical_column':[0],\n",
    "    'seed':seed0,\n",
    "    'feature_fraction_seed': seed0,\n",
    "    'bagging_seed': seed0,\n",
    "    'drop_seed': seed0,\n",
    "    'data_random_seed': seed0,\n",
    "    'n_jobs':-1,\n",
    "    'verbose': -1}\n",
    "seed1=42\n",
    "params1 = {\n",
    "        'learning_rate': 0.1,        \n",
    "        'lambda_l1': 2,\n",
    "        'lambda_l2': 7,\n",
    "        'num_leaves': 800,\n",
    "        'min_sum_hessian_in_leaf': 20,\n",
    "        'feature_fraction': 0.8,\n",
    "        'feature_fraction_bynode': 0.8,\n",
    "        'bagging_fraction': 0.9,\n",
    "        'bagging_freq': 42,\n",
    "        'min_data_in_leaf': 700,\n",
    "        'max_depth': 4,\n",
    "        'categorical_column':[0],\n",
    "        'seed': seed1,\n",
    "        'feature_fraction_seed': seed1,\n",
    "        'bagging_seed': seed1,\n",
    "        'drop_seed': seed1,\n",
    "        'data_random_seed': seed1,\n",
    "        'objective': 'rmse',\n",
    "        'boosting': 'gbdt',\n",
    "        'verbosity': -1,\n",
    "        'n_jobs':-1,\n",
    "    }\n",
    "# Function to early stop with root mean squared percentage error\n",
    "def rmspe(y_true, y_pred):\n",
    "    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
    "\n",
    "def feval_rmspe(y_pred, lgb_train):\n",
    "    y_true = lgb_train.get_label()\n",
    "    return 'RMSPE', rmspe(y_true, y_pred), False\n",
    "\n",
    "def train_and_evaluate_lgb(train, test, params):\n",
    "    # Hyperparammeters (just basic)\n",
    "    \n",
    "    features = [col for col in train.columns if col not in {\"time_id\", \"target\", \"row_id\"}]\n",
    "    # Create out of folds array\n",
    "    y = train['target']\n",
    "    oof_predictions = np.zeros(train.shape[0])\n",
    "    # Create test array to store predictions\n",
    "    test_predictions = np.zeros(test.shape[0])\n",
    "    # Create a KFold object\n",
    "    kfold = KFold(n_splits = 5, random_state = 2021, shuffle = True)\n",
    "    # Iterate through each fold\n",
    "    for fold, (trn_ind, val_ind) in enumerate(kfold.split(train)):\n",
    "        print(f'Training fold {fold + 1}')\n",
    "        x_train, x_val = train.iloc[trn_ind], train.iloc[val_ind]\n",
    "        y_train, y_val = y.iloc[trn_ind], y.iloc[val_ind]\n",
    "        # Root mean squared percentage error weights\n",
    "        train_weights = 1 / np.square(y_train)\n",
    "        val_weights = 1 / np.square(y_val)\n",
    "        train_dataset = lgb.Dataset(x_train[features], y_train, weight = train_weights)\n",
    "        val_dataset = lgb.Dataset(x_val[features], y_val, weight = val_weights)\n",
    "        model = lgb.train(params = params,\n",
    "                          num_boost_round=1300,\n",
    "                          train_set = train_dataset, \n",
    "                          valid_sets = [train_dataset, val_dataset], \n",
    "                          verbose_eval = 250,\n",
    "                          early_stopping_rounds=50,\n",
    "                          feval = feval_rmspe)\n",
    "        # Add predictions to the out of folds array\n",
    "        oof_predictions[val_ind] = model.predict(x_val[features])\n",
    "        # Predict the test set\n",
    "        test_predictions += model.predict(test[features]) / 5\n",
    "    rmspe_score = rmspe(y, oof_predictions)\n",
    "    print(f'Our out of folds RMSPE is {rmspe_score}')\n",
    "    lgb.plot_importance(model,max_num_features=20)\n",
    "    # Return test predictions\n",
    "    return test_predictions\n",
    "# Traing and evaluate\n",
    "predictions_lgb_1= train_and_evaluate_lgb(train, test,params0)\n",
    "# predictions_lgb_2= train_and_evaluate_lgb(train, test,params1)\n",
    "# test['target'] = predictions_lgb_1\n",
    "# test[['row_id', 'target']].to_csv('submission.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "247"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from numpy.random import seed\n",
    "seed(42)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "def root_mean_squared_per_error(y_true, y_pred):\n",
    "         return K.sqrt(K.mean(K.square( (y_true - y_pred)/ y_true )))\n",
    "    \n",
    "es = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', patience=20, verbose=0,\n",
    "    mode='min',restore_best_weights=True)\n",
    "\n",
    "plateau = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.2, patience=7, verbose=0,\n",
    "    mode='min')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kfold based on the knn++ algorithm\n",
    "\n",
    "out_train = pd.read_csv(data_dir+'train.csv')\n",
    "out_train = out_train.pivot(index='time_id', columns='stock_id', values='target')\n",
    "\n",
    "#out_train[out_train.isna().any(axis=1)]\n",
    "out_train = out_train.fillna(out_train.mean())\n",
    "out_train.head()\n",
    "\n",
    "# code to add the just the read data after first execution\n",
    "\n",
    "# data separation based on knn ++\n",
    "nfolds = 5 # number of folds\n",
    "index = []\n",
    "totDist = []\n",
    "values = []\n",
    "# generates a matriz with the values of \n",
    "mat = out_train.values\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "mat = scaler.fit_transform(mat)\n",
    "\n",
    "nind = int(mat.shape[0]/nfolds) # number of individuals\n",
    "\n",
    "# adds index in the last column\n",
    "mat = np.c_[mat,np.arange(mat.shape[0])]\n",
    "\n",
    "\n",
    "lineNumber = np.random.choice(np.array(mat.shape[0]), size=nfolds, replace=False)\n",
    "\n",
    "lineNumber = np.sort(lineNumber)[::-1]\n",
    "\n",
    "for n in range(nfolds):\n",
    "    totDist.append(np.zeros(mat.shape[0]-nfolds))\n",
    "\n",
    "# saves index\n",
    "for n in range(nfolds):\n",
    "    \n",
    "    values.append([lineNumber[n]])    \n",
    "\n",
    "\n",
    "s=[]\n",
    "for n in range(nfolds):\n",
    "    s.append(mat[lineNumber[n],:])\n",
    "    \n",
    "    mat = np.delete(mat, obj=lineNumber[n], axis=0)\n",
    "\n",
    "for n in range(nind-1):    \n",
    "\n",
    "    luck = np.random.uniform(0,1,nfolds)\n",
    "    \n",
    "    for cycle in range(nfolds):\n",
    "         # saves the values of index           \n",
    "\n",
    "        s[cycle] = np.matlib.repmat(s[cycle], mat.shape[0], 1)\n",
    "\n",
    "        sumDist = np.sum( (mat[:,:-1] - s[cycle][:,:-1])**2 , axis=1)   \n",
    "        totDist[cycle] += sumDist        \n",
    "                \n",
    "        # probabilities\n",
    "        f = totDist[cycle]/np.sum(totDist[cycle]) # normalizing the totdist\n",
    "        j = 0\n",
    "        kn = 0\n",
    "        for val in f:\n",
    "            j += val        \n",
    "            if (j > luck[cycle]): # the column was selected\n",
    "                break\n",
    "            kn +=1\n",
    "        lineNumber[cycle] = kn\n",
    "        \n",
    "        # delete line of the value added    \n",
    "        for n_iter in range(nfolds):\n",
    "            \n",
    "            totDist[n_iter] = np.delete(totDist[n_iter],obj=lineNumber[cycle], axis=0)\n",
    "            j= 0\n",
    "        \n",
    "        s[cycle] = mat[lineNumber[cycle],:]\n",
    "        values[cycle].append(int(mat[lineNumber[cycle],-1]))\n",
    "        mat = np.delete(mat, obj=lineNumber[cycle], axis=0)\n",
    "\n",
    "\n",
    "for n_mod in range(nfolds):\n",
    "    values[n_mod] = out_train.index[values[n_mod]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colNames.remove('row_id')\n",
    "train.replace([np.inf, -np.inf], np.nan,inplace=True)\n",
    "test.replace([np.inf, -np.inf], np.nan,inplace=True)\n",
    "qt_train = []\n",
    "train_nn=train[colNames].copy()\n",
    "test_nn=test[colNames].copy()\n",
    "for col in colNames:\n",
    "    #print(col)\n",
    "    qt = QuantileTransformer(random_state=21,n_quantiles=2000, output_distribution='normal')\n",
    "    train_nn[col] = qt.fit_transform(train_nn[[col]])\n",
    "    test_nn[col] = qt.transform(test_nn[[col]])    \n",
    "    qt_train.append(qt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_nn[['stock_id','time_id','target']]=train[['stock_id','time_id','target']]\n",
    "test_nn[['stock_id','time_id']]=test[['stock_id','time_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making agg features\n",
    "from sklearn.cluster import KMeans\n",
    "train_p = pd.read_csv(data_dir+'train.csv')\n",
    "train_p = train_p.pivot(index='time_id', columns='stock_id', values='target')\n",
    "\n",
    "corr = train_p.corr()\n",
    "\n",
    "ids = corr.index\n",
    "\n",
    "kmeans = KMeans(n_clusters=7, random_state=0).fit(corr.values)\n",
    "print(kmeans.labels_)\n",
    "\n",
    "l = []\n",
    "for n in range(7):\n",
    "    l.append ( [ (x-1) for x in ( (ids+1)*(kmeans.labels_ == n)) if x > 0] )\n",
    "    \n",
    "\n",
    "mat = []\n",
    "matTest = []\n",
    "\n",
    "n = 0\n",
    "for ind in l:\n",
    "    print(ind)\n",
    "    newDf = train_nn.loc[train_nn['stock_id'].isin(ind) ]\n",
    "    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n",
    "    newDf.loc[:,'stock_id'] = str(n)+'c1'\n",
    "    mat.append ( newDf )\n",
    "    \n",
    "    newDf = test_nn.loc[test_nn['stock_id'].isin(ind) ]    \n",
    "    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n",
    "    newDf.loc[:,'stock_id'] = str(n)+'c1'\n",
    "    matTest.append ( newDf )\n",
    "    \n",
    "    n+=1\n",
    "    \n",
    "mat1 = pd.concat(mat).reset_index()\n",
    "mat1.drop(columns=['target'],inplace=True)\n",
    "\n",
    "mat2 = pd.concat(matTest).reset_index()\n",
    "mat2 = pd.concat([mat2,mat1.loc[mat1.time_id==5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnn = ['time_id',\n",
    "     'log_return1_realized_volatility_0c1',\n",
    "     'log_return1_realized_volatility_1c1',     \n",
    "     'log_return1_realized_volatility_3c1',\n",
    "     'log_return1_realized_volatility_4c1',     \n",
    "     'log_return1_realized_volatility_6c1',\n",
    "     'total_volume_sum_0c1',\n",
    "     'total_volume_sum_1c1', \n",
    "     'total_volume_sum_3c1',\n",
    "     'total_volume_sum_4c1', \n",
    "     'total_volume_sum_6c1',\n",
    "     'trade_size_sum_0c1',\n",
    "     'trade_size_sum_1c1', \n",
    "     'trade_size_sum_3c1',\n",
    "     'trade_size_sum_4c1', \n",
    "     'trade_size_sum_6c1',\n",
    "     'trade_order_count_sum_0c1',\n",
    "     'trade_order_count_sum_1c1',\n",
    "     'trade_order_count_sum_3c1',\n",
    "     'trade_order_count_sum_4c1',\n",
    "     'trade_order_count_sum_6c1',      \n",
    "     'price_spread_sum_0c1',\n",
    "     'price_spread_sum_1c1',\n",
    "     'price_spread_sum_3c1',\n",
    "     'price_spread_sum_4c1',\n",
    "     'price_spread_sum_6c1',   \n",
    "     'bid_spread_sum_0c1',\n",
    "     'bid_spread_sum_1c1',\n",
    "     'bid_spread_sum_3c1',\n",
    "     'bid_spread_sum_4c1',\n",
    "     'bid_spread_sum_6c1',       \n",
    "     'ask_spread_sum_0c1',\n",
    "     'ask_spread_sum_1c1',\n",
    "     'ask_spread_sum_3c1',\n",
    "     'ask_spread_sum_4c1',\n",
    "     'ask_spread_sum_6c1',   \n",
    "     'volume_imbalance_sum_0c1',\n",
    "     'volume_imbalance_sum_1c1',\n",
    "     'volume_imbalance_sum_3c1',\n",
    "     'volume_imbalance_sum_4c1',\n",
    "     'volume_imbalance_sum_6c1',       \n",
    "     'bid_ask_spread_sum_0c1',\n",
    "     'bid_ask_spread_sum_1c1',\n",
    "     'bid_ask_spread_sum_3c1',\n",
    "     'bid_ask_spread_sum_4c1',\n",
    "     'bid_ask_spread_sum_6c1',\n",
    "     'size_tau2_0c1',\n",
    "     'size_tau2_1c1',\n",
    "     'size_tau2_3c1',\n",
    "     'size_tau2_4c1',\n",
    "     'size_tau2_6c1'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat1 = mat1.pivot(index='time_id', columns='stock_id')\n",
    "mat1.columns = [\"_\".join(x) for x in mat1.columns.ravel()]\n",
    "mat1.reset_index(inplace=True)\n",
    "\n",
    "mat2 = mat2.pivot(index='time_id', columns='stock_id')\n",
    "mat2.columns = [\"_\".join(x) for x in mat2.columns.ravel()]\n",
    "mat2.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "train_nn = pd.merge(train_nn,mat1[nnn],how='left',on='time_id')\n",
    "test_nn = pd.merge(test_nn,mat2[nnn],how='left',on='time_id')\n",
    "del mat1,mat2\n",
    "del train,test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FFNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://bignerdranch.com/blog/implementing-swish-activation-function-in-keras/\n",
    "from keras.backend import sigmoid\n",
    "def swish(x, beta = 1):\n",
    "    return (x * sigmoid(beta * x))\n",
    "\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "from keras.layers import Activation\n",
    "get_custom_objects().update({'swish': Activation(swish)})\n",
    "\n",
    "hidden_units = (128,64,32)\n",
    "stock_embedding_size = 24\n",
    "\n",
    "cat_data = train_nn['stock_id']\n",
    "\n",
    "def base_model():\n",
    "    \n",
    "    # Each instance will consist of two inputs: a single user id, and a single movie id\n",
    "    stock_id_input = keras.Input(shape=(1,), name='stock_id')\n",
    "    num_input = keras.Input(shape=(244,), name='num_data')\n",
    "\n",
    "\n",
    "    #embedding, flatenning and concatenating\n",
    "    stock_embedded = keras.layers.Embedding(max(cat_data)+1, stock_embedding_size, \n",
    "                                           input_length=1, name='stock_embedding')(stock_id_input)\n",
    "    stock_flattened = keras.layers.Flatten()(stock_embedded)\n",
    "    out = keras.layers.Concatenate()([stock_flattened, num_input])\n",
    "    \n",
    "    # Add one or more hidden layers\n",
    "    for n_hidden in hidden_units:\n",
    "\n",
    "        out = keras.layers.Dense(n_hidden, activation='swish')(out)\n",
    "        \n",
    "\n",
    "    #out = keras.layers.Concatenate()([out, num_input])\n",
    "\n",
    "    # A single output: our predicted rating\n",
    "    out = keras.layers.Dense(1, activation='linear', name='prediction')(out)\n",
    "    \n",
    "    model = keras.Model(\n",
    "    inputs = [stock_id_input, num_input],\n",
    "    outputs = out,\n",
    "    )\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the root mean squared percentage error\n",
    "def rmspe(y_true, y_pred):\n",
    "    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
    "\n",
    "# Function to early stop with root mean squared percentage error\n",
    "def feval_rmspe(y_pred, lgb_train):\n",
    "    y_true = lgb_train.get_label()\n",
    "    return 'RMSPE', rmspe(y_true, y_pred), False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_name='target'\n",
    "scores_folds = {}\n",
    "model_name = 'NN'\n",
    "pred_name = 'pred_{}'.format(model_name)\n",
    "\n",
    "n_folds = 5\n",
    "kf = model_selection.KFold(n_splits=n_folds, shuffle=True, random_state=2020)\n",
    "scores_folds[model_name] = []\n",
    "counter = 1\n",
    "\n",
    "features_to_consider = list(train_nn)\n",
    "\n",
    "features_to_consider.remove('time_id')\n",
    "features_to_consider.remove('target')\n",
    "try:\n",
    "    features_to_consider.remove('pred_NN')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "train_nn[features_to_consider] = train_nn[features_to_consider].fillna(train_nn[features_to_consider].mean())\n",
    "test_nn[features_to_consider] = test_nn[features_to_consider].fillna(train_nn[features_to_consider].mean())\n",
    "\n",
    "train_nn[pred_name] = 0\n",
    "test_nn[target_name] = 0\n",
    "test_predictions_nn = np.zeros(test_nn.shape[0])\n",
    "\n",
    "for n_count in range(n_folds):\n",
    "    print('CV {}/{}'.format(counter, n_folds))\n",
    "    \n",
    "    indexes = np.arange(nfolds).astype(int)    \n",
    "    indexes = np.delete(indexes,obj=n_count, axis=0) \n",
    "    \n",
    "    indexes = np.r_[values[indexes[0]],values[indexes[1]],values[indexes[2]],values[indexes[3]]]\n",
    "    \n",
    "    X_train = train_nn.loc[train_nn.time_id.isin(indexes), features_to_consider]\n",
    "    y_train = train_nn.loc[train_nn.time_id.isin(indexes), target_name]\n",
    "    X_test = train_nn.loc[train_nn.time_id.isin(values[n_count]), features_to_consider]\n",
    "    y_test = train_nn.loc[train_nn.time_id.isin(values[n_count]), target_name]\n",
    "    \n",
    "    #############################################################################################\n",
    "    # NN\n",
    "    #############################################################################################\n",
    "    \n",
    "    model = base_model()\n",
    "    \n",
    "    model.compile(\n",
    "        keras.optimizers.Adam(learning_rate=0.006),\n",
    "        loss=root_mean_squared_per_error\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        features_to_consider.remove('stock_id')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    num_data = X_train[features_to_consider]\n",
    "    \n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))         \n",
    "    num_data = scaler.fit_transform(num_data.values)    \n",
    "    \n",
    "    cat_data = X_train['stock_id']    \n",
    "    target =  y_train\n",
    "    \n",
    "    num_data_test = X_test[features_to_consider]\n",
    "    num_data_test = scaler.transform(num_data_test.values)\n",
    "    cat_data_test = X_test['stock_id']\n",
    "\n",
    "    model.fit([cat_data, num_data], \n",
    "              target,               \n",
    "              batch_size=2048,\n",
    "              epochs=1000,\n",
    "              validation_data=([cat_data_test, num_data_test], y_test),\n",
    "              callbacks=[es, plateau],\n",
    "              validation_batch_size=len(y_test),\n",
    "              shuffle=True,\n",
    "             verbose = 1)\n",
    "\n",
    "    preds = model.predict([cat_data_test, num_data_test]).reshape(1,-1)[0]\n",
    "    \n",
    "    score = round(rmspe(y_true = y_test, y_pred = preds),5)\n",
    "    print('Fold {} {}: {}'.format(counter, model_name, score))\n",
    "    scores_folds[model_name].append(score)\n",
    "    \n",
    "    tt =scaler.transform(test_nn[features_to_consider].values)\n",
    "    #test_nn[target_name] += model.predict([test_nn['stock_id'], tt]).reshape(1,-1)[0].clip(0,1e10)\n",
    "    test_predictions_nn += model.predict([test_nn['stock_id'], tt]).reshape(1,-1)[0].clip(0,1e10)/n_folds\n",
    "    #test[target_name] += model.predict([test['stock_id'], test[features_to_consider]]).reshape(1,-1)[0].clip(0,1e10)\n",
    "       \n",
    "    counter += 1\n",
    "    features_to_consider.append('stock_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_nn[\"row_id\"] = test_nn[\"stock_id\"].astype(str) + \"-\" + test_nn[\"time_id\"].astype(str) \n",
    "test_nn[target_name] = (test_predictions_nn*0.585+predictions_lgb_1*0.415)\n",
    "# test_nn[target_name] = (test_predictions_nn*0.5+predictions_lgb_1*0.285+test_predictions_TabNet)\n",
    "\n",
    "score = round(rmspe(y_true = train_nn[target_name].values, y_pred = train_nn[pred_name].values),5)\n",
    "print('RMSPE {}: {} - Folds: {}'.format(model_name, score, scores_folds[model_name]))\n",
    "\n",
    "display(test_nn[['row_id', target_name]].head(3))\n",
    "test_nn[['row_id', target_name]].to_csv('submission.csv',index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a fork from https://www.kaggle.com/alexioslyon/lgbm-baseline and other great kaggles.thanks a lot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3830, 248)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.loc[train.stock_id == 1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
